U.S. Department of Homeland Security
FY 2021 Capacity Assessment
DEPARTMEN
US
OF
With honor and integrity, we will safeguard the American people, our homeland, and our values.
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
About this Report
The Foundations for Evidence-Based Policymaking Act of 2018 ("Evidence Act") requires that the
U.S. Department of Homeland Security conduct and report a capacity assessment describing the
coverage, quality, methods, effectiveness, and independence of evaluation, research, analysis,
and statistics efforts of the Department.
The DHS FY 2021 Capacity Assessment describes the Department's initial effort to assess its
capacity to build and use evidence from evaluation, statistics, research, and analysis, consistent
with implementing guidance from the Office of Management and Budget (OMB). The capacity
assessment provides a baseline understanding of the Department's strengths and weaknesses
and lays the foundation to improve the infrastructure and culture for evaluation, evidence
building, and organizational learning within DHS.
As required, the DHS FY 2021 Capacity Assessment is published at the DHS public website and
at Evaluation.gov with the Department's other Evidence Act plans and reports.
Contact Information
For more information, contact:
Michael Stough, Evaluation Officer
Department of Homeland Security
Office of the Chief Financial Officer
Division of Program Analysis and Evaluation
245 Murray Lane SW
Mailstop 200
Washington, DC 20528
i
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Contents
About this Report
i
DHS Capacity Assessment Highlights
1
Summary of Findings
1
Opportunities for Improvement
3
How Findings Will Be Used
3
Overview
4
Introduction
4
Guiding Principles of Design
4
Evidence Building
5
Capacity Assessment Approach
6
Component-Level Capabilities
6
Maturity Model
7
Sample
7
Assessment Methods
9
Group Discussions
9
Structured Plan Review
9
Individual Survey
10
Inventory of Studies
11
Structured Study Review
13
Maturity Ratings and Synthesis of Findings
15
Challenges and Limitations
16
Findings
17
Maturity of DHS Evidence Building Activities
17
Coverage
18
Overview of Coverage Findings
18
Quality
24
Overview of Quality Findings
25
Methods
29
Overview of Methods Findings
29
Using the Methods Inventory: Appropriateness of Evaluation Methods
35
ii
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Independence
38
Overview of Independence Findings
38
Effectiveness
40
Overview of Effectiveness Findings
40
Opportunities for Improvement
46
Appendix A. Abbreviations and Acronyms
47
Appendix B: Addressing Evidence Act and OMB Requirements
48
Evidence Act Overview
48
Evidence Act and OMB Requirements
48
Evidence-Building Activities
51
Appendix C. Evidence-Building Activities and Operations Evaluated or Analyzed
52
Appendix D. Summary of Individual Survey Data
60
Expertise
60
Supports
62
Agency
63
iii
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
DHS Capacity Assessment Highlights
The U.S. Department of Homeland Security (DHS) Evaluation Officer in the Division of Program
Analysis & Evaluation (PA&E) oversees periodic assessments of DHS capacity to build and use
evidence, with the aim of helping the Department and its Components¹ identify concrete steps
for continuous improvement. For the fiscal year (FY) 2021 assessment, PA&E developed technical
requirements for a task order awarded to the Homeland Security Operations Analysis Center
(assessment team)2 to design and conduct an actionable assessment.
This assessment included 14 DHS Components in the Department's strategic plan. It reflects data
collected through discussion groups and surveys with selected DHS staff and reviews of evidence
documents. The assessment was designed to advance unity of effort in the Department's
implementation of the Evidence Act and embody a continuous improvement mindset for
assessing and expanding the Department's capacity to produce and use evidence.
PA&E and the assessment team collaborated to develop this summary report of the most
actionable findings. The findings were determined to a great extent by whether personnel whose
input was vital to the capacity assessment were prepared and able to participate in the
assessment, not just as informed respondents but as collaborators on sampling. While the data
were incomplete, the findings from multiple methods and data sources offer insights on baseline
patterns and opportunities to improve capacity for evidence building and use going forward.
Summary of Findings
Conducting the capacity assessment provided an opportunity to discover, describe, and assess
the Department's evidence-building capacity and activities, determining where current evidence
capacity is and is not sufficient to meet future needs and where capacity-building efforts should
be targeted.
Overall DHS lacked shared understanding - common terminology and concepts - related
to
evidence-building as defined by the Evidence Act and OMB. Most Components did not have
coordinated Component-level perspective on staff, funding, and other infrastructure for
evidence building or for evidence-building activities (studies) underway in FY 2021. All four
evidence building activities (i.e., evaluation, research, analysis, and statistics) were underway
across the Department with varying levels of maturity on the assessment dimensions (coverage,
quality, methods, independence, and effectiveness), as summarized in Exhibit 1. Capacity was
uneven across Components, nascent in some places, but developing.
1 This report uses "DHS" or "Department" when referring to the overarching entity of the Department of
Homeland Security (DHS) and "Component" when referring to individual components and offices.
2 This third-party assessment was funded by the U.S. Department of Homeland Security and conducted by the
Homeland Security Operational Analysis Center (HSOAC), a Federally Funded Research and Development Center
(FFRDC) operated by the RAND Corporation under contract with the Department of Homeland Security under
Contract #HSHQDC-16-00007 Task Order 7ORDAD21FR0000014. The contents of this publication provide a
summary of HSOAC's findings and do not necessarily reflect the views or policies of the Department of Homeland
Security, nor does the mention of trade names, commercial products, or organizations imply endorsement of same
by the U.S. Government. The assessment is led by the Project Director, Dr. Brodi Kotila (HSOAC).
1
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 1. Maturity of DHS Evidence Building on Assessed Dimensions, by
Activity
Activity
Maturity Model
Dimension
Evaluation
Research
Analysis
Statistics
5 = Optimizing
Coverage
Implementing
In progress
Implementing
Implementing
4 = Completed
Quality
Implementing
In progress
Implementing
In progress
3 = Implementing
Independence
Completed
Completed
Not rated
Implementing
2 = In progress
Effectiveness
In progress
In progress
Implementing
In progress
1= Not initiated
Notes: Maturity for the Methods dimension is incorporated in the quality
dimension maturity score. Specifically, the quality dimension assesses: "do these
0 = No basis for judgement
methods incorporate the necessary level of rigor?"
Coverage Summary Finding: DHS had some staff, funding, and policies in place, and evidence-
building (studies) underway in FY 2021, but they were distributed unevenly across the
Department and considered by many Component representatives to be insufficient to fully
achieve all the objectives defined in the Evidence Act.
Quality Summary Finding: DHS-wide policies and guidance outlined quality standards for some
activities, though compliance with these standards varied across evidence building activities.
Department-level evidence plans were relatively mature. Components did not have equivalent
policies and plans for their evidence building activities. FY 2021 studies met some quality
standards but the percentage of studies meeting standards varied by activity and by standard.
Methods Summary Finding: DHS evidence-building addressed a balance of formative and
summative aspects, and studies used a wide range of qualitative and quantitative data, data
collection, and analysis methods. The levels in which certain data and methods were used may
limit the Department's ability to make credible inferences and fulfill summative purposes,
especially for quantitative, customer-centric determinations of effectiveness (outcomes and
impacts), cost-effectiveness, or equity that result from DHS mission delivery.
Independence Summary Finding: Most FY 2021 studies were performed by internal staff.
Information provided for studies suggests that the majority of evidence-building activities were
conducted free from inappropriate influence.
Effectiveness Summary Finding: While DHS lacked policies that outlined requirements for
stakeholder engagement and dissemination, some Department-wide and Component
mechanisms existed to gather stakeholder needs, input, and feedback for evidence building, to
disseminate findings of evidence building, and to use evidence for decision making and
improvement. Some organizational contexts, including Component culture, practices, and
technology resources, exist that support evidence building and its use in day-to-day operations.
2
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Opportunities for Improvement
The findings suggest the following areas for improvement.
Establish a strong foundation for evidence building and use. DHS capacity building efforts should
continue to (1) foster shared understanding of evidence-building through common terminology
and concepts consistent with the Evidence Act and OMB guidance to which DHS is accountable;
(2) deepen Component-level understanding of personnel, funding, and other infrastructure
available for evidence-building as well as evidence-building activities that are underway within
the Department; and (3) promote the importance of evidence building and use.
Assess the Department's evidence needs and align evidence-building efforts and resources
with those needs. Beyond the DHS learning agenda, developing Component-specific learning
agendas and evaluation plans would enable DHS to comprehensively assess Components' short
and long-term evidence needs, measure Component capacity against those needs, and
appropriately allocate resources to build evidence while targeting gaps in capacity.
Build and sustain evidence building capabilities at the Component level and DHS-wide. To
ensure that evidence building adheres to principles of scientific integrity and rigor, DHS needs to
hire personnel with specialized expertise for evidence building, provide training and time to
develop staff, and establish sufficient budgets to support evidence building activities and procure
external researchers for evidence building. DHS should empower certain evidence-building staff
(or external researchers they manage) with sufficient independence and autonomy to design,
conduct, and appropriately disseminate findings, methods, and data from evidence building.
Aim for continuous improvement of the evidence-building enterprise. DHS should implement
and monitor compliance with existing plans and policies for evidence building and use, and
establish new plans, policies, and mechanisms where needed. Department-wide, evidence
building teams should make plans to improve their efforts, targeting areas identified for
improvement in this assessment.
Promote the dissemination and use of evidence. DHS should consult evidence users and a broad
range of stakeholders to inform evidence-building policies, plans, and activities so they are
relevant and useful. Existing continuous improvement processes should leverage all types of
evidence. DHS and Components should establish new or formalize existing practices to promote
more consistent use of evidence in programs, policymaking, and business processes. Improving
existing dissemination mechanisms to enable timely release of products and findings to the
broadest audiences possible, including publicly releasing evidence, is a necessary precursor for
ensuring its use in learning, improvement, and accountability to the American public.
How Findings Will Be Used
The assessment team and PA&E disseminated findings to, and obtained feedback from,
Components and used findings as the basis of facilitated Component-specific capacity planning
activities. The assessment provided opportunity for a collective and concerted effort to improve
Component and DHS capacity to use and produce evidence, and for building on the foundation
the assessment team has documented in the DHS FY 2021 Capacity Assessment.
3
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Overview
Introduction
The U.S. Department of Homeland Security (DHS) has a diverse and complex mission to prevent
attacks and mitigate threats against the United States and our allies, respond to natural and man-
made disasters, and advance American prosperity and economic security. Since DHS was
established from its predecessor agencies in 2003, the Department has continued to expand and
mature capabilities to use data and analysis in shaping strategy and operations.
DHS developed this DHS FY 2021 Capacity Assessment to discover, describe, and assess the
Department's evidence-building, determining where current evidence capacity is and is not
sufficient to meet current and future needs and where capacity building efforts should be
targeted. The capacity assessment supports the Department's implementation of the
Foundations for Evidence Based Policymaking Act of 2018³ (Evidence Act) by providing a baseline
understanding of Department's capacity to build and use evidence. This baseline can guide
decisions and next steps for maturing capacity.
The DHS Evaluation Officer, in PA&E, oversees the periodic assessments of DHS capacity to build
and use evidence, with the aim of helping the Department identify concrete steps for continuous
improvement. For the FY 2021 assessment, PA&E developed technical requirements for a task
order awarded to Homeland Security Operations Analysis Center (HSOAC, herein assessment team)
to design and conduct this assessment.
Guiding Principles of Design
The DHS FY 2021 Capacity Assessment fulfills the specific requirement to assess the coverage,
quality, methods, effectiveness, and independence of the Department's evaluation, research,
analysis, and statistics efforts. The assessment team worked closely with PA&E to design an
assessment that would allow the Department to understand the maturity of its evidence-building
enterprise and lay a foundation for strengthening these capabilities over time, while balancing
complex statutory requirements, diverse missions and capabilities across the Components, and
utility. The assessment team used the following principles to guide the design of the assessment
activities:
Support unity of effort. The assessment team designed assessment activities to expand
awareness of the Evidence Act and develop a shared understanding of evidence-building
activities and their utility and importance across the Department.
Plan for iteration and improvement over time. The assessment focused on Components'
most significant evidence-building activities and a limited set of foundational capabilities
that can feasibly be addressed now to mature evidence building. The assessment
identifies opportunities for improving capacity and future assessments of capacity.
3
Pub. L. No. 115-435, 132 Stat. 5529 (2019)
4
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Demonstrate usefulness to the DHS Components. The assessment team collected and
analyzed DHS- and Component-level data, generating findings at DHS and Component
levels, to support DHS- and Component-specific planning activities aimed at maturing
capacity for evidence building and use.
Tailor Evidence Act requirements to DHS context. The assessment team organized and
addressed Evidence Act and Office of Management and Budget (OMB) requirements to
(1) allow for systematic assessment, across organizations and time, and generation of
useful and actionable findings; (2) gather information from staff to understand capacity
at organizational, individual, and study levels; and 3) minimize the burden on participating
staff while building capacity to sustain this effort long term.
Evidence Building
Informed by a review of the Evidence Act, all applicable OMB guidance, the interim capacity
assessment conducted by DHS in FY 2020, and other relevant materials, PA&E developed a list of
evaluation, research, analysis, and statistical activities (activity subtypes) that should be included
in or excluded from this assessment. Exhibit 2 lists and describes activity subtypes.
Exhibit 2. Assessed Evidence-Building Activities and Activity Subtypes
Evidence-
Building
Activity Subtypes
Activity
Evaluation of programs, policies, regulations, or organization evaluation (also termed program
evaluation), including formative, process and implementation, outcome, impact, and economic
Evaluation
evaluation
Project evaluation of ongoing or completed grantee projects required by DHS grantmaking programs
Basic research: experimental or theoretical work undertaken primarily to acquire new knowledge of the
underlying foundations of phenomena and observable facts
Applied research: original investigation undertaken in order to acquire new knowledge, directed
primarily toward a specific practical aim or objective
Research
Foundational research: describes and documents programs, policies, services, or interventions
currently implemented in the field or eligible and impacted populations and their characteristics
Exploratory research: examines correlational relationships between program- or policy-relevant
constructs to identify logical connections that could form the basis for future programs, policies,
services, or interventions or frameworks to measure their results
Policy and regulatory analysis, typically using cost-benefit and cost-effectiveness analysis
Operations research: the development of mathematical models, statistical analyses, simulations, and
analytical reasoning to understand and improve real-world operations
Analysis
Performance measurement and monitoring: ongoing and systematic tracking of data and information
relevant to policies, strategies, programs, projects, or activities, which can include indicators for context,
inputs, process, efficiency, outputs, intermediate outcomes, and outcomes
Statistical activities: The use of data to describe outcomes and descriptors of interest, such as through
Statistics
estimates of population characteristics, summaries of test results, indices of economic activity,
measures of environmental conditions, and incidence rates for a wide variety of events
The team designed the assessment to gather information and report of assessment findings by
activity and, in some instances, activity subtype. Appendix B outlines the Evidence Act and OMB
requirements and how they were operationalized in the DHS capacity assessment.
5
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Capacity Assessment Approach
Informed by a review of the Evidence Act, OMB guidance, the interim capacity assessment
conducted by DHS in FY 2020, and other relevant materials, the assessment team designed:
a comprehensive set of 49 capabilities (called Component-Level Capabilities, or CLCs herein)
that unpack various elements of each of the Evidence Act and OMB requirements and that
can be flexibly applied across evidence building activities.
a structured maturity scoring system tailored to each CLC that is based on a general maturity
model; and
a multimethod approach (e.g., uses discussion groups, web-based surveys, evidence
document reviews) focused on understanding CLC maturity and triangulating across the most
appropriate data sources to develop a complete picture of the maturity of DHS capacity to
build and use evidence.
Exhibit 3 provides a summary of the process used to establish the capacity assessment plan and
to conduct the FY 2021 Capacity Assessment.
Exhibit 3. Overview of DHS FY 2021 Capacity Assessment Approach
Component-Level Capabilities
Methods to
(CLCs) for FY2021 (25 total)
Assess CLCs
Coverage
Group
Identify CLCs to address all
6 of 18 CLCs
Discussion
activity types and dimensions
Requirements
Activities
Quality
Structured
Review
Dimensions
8 of 9 CLCs
Plan Review
Apply
Evidence Act
Research
Coverage
relevant
OMB Guidance
Quality
method(s)
Evaluation
Methods
Individual
DHS PA&E FY20
to
Analysis
Methods
1 of 2 CLCs
Survey
Interim Capacity
assess
Assessment
Statistics
Effectiveness
each CLC
Other relevant
Independence
Effectiveness
Inventory of
materials
7 of 11 CLCs
Studies
Independence
Structured
3 of 7 CLCs
Study Review
Component-Leve Capabilities
For each of the five statutory dimensions (coverage, quality, methods, independence,
effectiveness), the assessment team developed multiple Component-Level Capabilities (CLCs) to
unpack various elements of the dimension. The FY 2021 Capacity Assessment focused on a subset
of CLCs developed (25 of 49) that could feasibly provide
"...an objective accounting of an agency's capacity (the sufficiency of, e.g., the agency's staffing,
funding, infrastructure, and processes) to carry out the evidence-building activities needed to
meet its agency functions and its capacity to disseminate and use evidence."
4 Evidence-Based Policymaking: Learning Agendas and Annual Evaluation Plans, M-21-27 (OMB, 2021)
6
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Subsequent sections provide short descriptions of the CLCs in relation to the methods used to
gather data about them.
Maturity Model
To guide the systematic rating of these capabilities, the assessment team used a maturity model
approach. Informed by the general capability maturity model shown in Exhibit 4, the assessment
team customized the data analysis method(s) and the maturity scoring rubric for each CLC and
the data collected for it. Generally, for each CLC, the maturity ratings roughly translate in
accordance with the general capability maturity model.
Exhibit 4. Overview of the General Capability Maturity Model
Score
Maturity Level
Description
The capability was fully embedded into the Component's operational structure and
5
Optimizing
culture at the time of the FY 2021 capacity assessment and the Component was
focused on continuous improvement in this area
4
Completed
The capability was fully implemented at the time of the capacity assessment and
long-term resources for that capability had been identified
3
Implementing
The plans for that capability were finalized and approved as of the FY 2021 capacity
assessment, initial resources were identified, and relevant activities were underway
2
In progress
Some progress was underway to create that capability at the time of the capacity
assessment
1
Not initiated
Initial planning for that capability was not initiated at the time of the FY 2021
capacity assessment, or ad hoc activities were performed
When Component representatives provided no information about that capability for
a
No basis for
particular evidence-building activity they identified as a most significant activity
0
(e.g., in the context of group discussions), when a Component did not submit
judgement
relevant plans or documents for assessment, or when ten or fewer responses were
received to the two surveys described below.
Sample
The DHS Components and Offices listed below participated in some or all capacity assessment
activities:
U.S. Customs and Border Protection
U.S. Coast Guard (USCG)
(CBP)
U.S. Secret Service (USSS)
Cybersecurity and Infrastructure
Countering Weapons of Mass
Security Agency (CISA)
Destruction Office (CWMD)
Federal Emergency Management
Federal Law Enforcement Training
Agency (FEMA)
Center (FLETC)
U.S. Immigration and Customs
Office of Intelligence and Analysis (I&A)
Enforcement (ICE)
Management Directorate (MGMT)
Transportation Security Administration
Office of Strategy, Policy, and Plans
(TSA)
(PLCY)
U.S. Citizenship and Immigration
Science & Technology Directorate (S&T)
Services (USCIS)
7
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
To help prepare Component representatives to participate in the capacity assessment, the
assessment team and PA&E developed guidance defining the evidence-building activities,
structured requests to gather sampling information from Components, a list of frequently asked
questions, and materials for webinars. The assessment team and PA&E hosted webinars, office
hours, and monitored a dedicated email alias to provide assistance to capacity assessment
participants before and throughout the data collection period.
The assessment team convened preparatory meetings (pre-meetings) May 3-14, 2021 with
Component representatives to help them identify the most significant evidence-building
activities conducted in the Component, the DHS federal personnel to participate in capacity
assessment data collection activities and evidence building activities (studies) to inventory. For
an activity deemed most significant, the Component would generally be expected to have a
portfolio of evidence products and activities that are conducted on a recurrent basis. The
information gathered following the pre-meetings is summarized below.
All participating Components reported that they conducted analysis. Most Components
conducted evaluation and research. Fewer than half of Components identified statistics as a most
significant activity. The number of participating Components conducting each evidence-building
activity is summarized in Exhibit 5.
Exhibit 5. Number of Participating DHS Components Conducting Evidence-
Building Activities, Total and by Activity
Category
Total
Evaluation
Research
Analysis
Statistics
Participating DHS
14
12
11
14
6
Components
Exhibit 6 summarizes the number of DHS evidence-building personnel and studies identified by
Components. Components identified 143 federal personnel whose primary roles and
responsibilities included supervising or overseeing evidence-building activities or budget,
governance, and infrastructure for evidence-building, or any combination of these (indicated in
Exhibit 6 as Leaders). Components identified 340 federal personnel whose primary roles and
responsibilities include conducting evidence-building, disseminating evidence, and/or
supporting the use of evidence (indicated in Exhibit 6 as Staff). Components identified 368
studies to inventory (indicated in Exhibit 6 as Studies). For 76 individuals, the Component
indicated they conducted more than one evidence-building activity. For 103 individuals and 65
studies, the Component did not initially specify an evidence-building activity.
Exhibit 6. DHS Evidence-Building Personnel and Studies, Total and by Activity
Category
Total
Evaluation
Research
Analysis
Statistics
More Than 1
Unspecified
Leader
143
8
4
50
6
75
N/A
Staff
340
30
38
142
26
1
103
FY 2021 Study
368
54
72
150
27
N/A
65
Note: "N/A" indicates that this was not applicable.
8
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Assessment Methods
The assessment team used a mix of qualitative and quantitative methods and multiple data
sources to inform maturity ratings for CLCs. This section describes the focus, timing, and sample
for each of the study methods: group discussions, structured plan review, individual survey,
study inventory survey, and structured study review.
Group Discussions
The capacity assessment draws on group discussions with DHS Headquarters and Component
evidence-building leaders, supplemented with a review of relevant documents collected from
Components by PA&E and the assessment team. The assessment team conducted group
discussions with nine Components and one crosscutting discussion with representatives from
multiple organizations in DHS HQ, between May 24 and June 11, 2021. Additionally, two
Components provided written input instead of joining a group discussion.
The assessment team gathered information using a group discussion protocol and used this
information to develop capability maturity ratings and explanatory rationales for the ratings.
Group discussions and supplementary document review explored the following:
budget for and investment in evidence building;
strategic and evidence plans;
policies and guidance for planning and conducting evidence building;
policies and mechanisms for stakeholder engagement to support evidence building, including
mechanisms to gather evidence users' needs and feedback;
processes to integrate evidence in decision making;
continuous improvement or learning cycle process that use evidence; and
policies and mechanisms for disseminating findings to external stakeholders and the public.
Structured Plan Review
The assessment team assessed the presence and quality of Department and Component-wide
evidence plans, including evidence-building plans (or learning agendas), evaluation plans, and
performance plans, against statutory and OMB requirements for each type of plan.5
The Department was in the process of developing its FY2022-2026 learning agenda and FY2023
annual evaluation plan, but the assessment team identified three relevant DHS-wide documents:
interim learning agenda (internal document)
DHS Agency Evaluation Plan6 for FY2022
Annual Performance Report for Fiscal Years 2020-2022 7
5 Phase 1 Implementation of the Foundations for Evidence-Based Policymaking Act of 2018: Learning Agendas,
Personnel, and Planning Guidance, M-19-23 (OMB, 2019); Preparation, Submission, and Execution of the Budget,
Circular A-11 (OMB, 2021)
6 FY 2020-2022 Annual Performance Report Appendix C: DHS Agency Evaluation Plan (DHS, 2021)
7
U.S. Department of Homeland Security FY 2020-2022 Annual Performance Report (DHS, 2021)
9
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Components had no formal plans in place at the time of this assessment. One Component
strategic plan that contained performance measures with annual targets was reviewed, but the
results are not reported here.
Individual Survey
The assessment team administered a web-based survey to Components' identified evidence-
building staff (EBS). The individual survey was fielded between May 14 and June 1, 2021 to a total
of 483 individuals, including all individuals identified by Components as evidence-building
Leaders or Staff. The survey explored respondents' perspectives of the following:
personal knowledge of core evidence-building competencies, including procurement and
management of external organizations for evidence building;
relevance of core evidence building tasks to their job;
training and other organizational supports for evidence building and use; and
Component context.
Evidence-building staff completed 191 surveys (40 percent of the 483 fielded surveys). Findings
from the survey must be interpreted keeping these limited response rates and the potential for
bias in mind; findings do not generalize beyond the respondents. Exhibit 7 presents the survey
sample total distribution (counts) across evidence-building activities.
Exhibit 7. Individual Surveys Fielded and Completed, Total and by Activity
Category
Total
Evaluation
Research
Analysis
Statistics
More Than 1
Unspecified
Fielded
483
38
42
192
32
76
103
Completed
191
30
20
95
26
N/A
N/A
Notes: Total includes 20 respondents that selected "I do not conduct any of these activities as primary functions of
my job." "N/A" indicates that this was not applicable.
Exhibit 8 summarizes the distribution (percentage) of respondents across occupational groups.
Data from categories with fewer than 10 responses have been suppressed to protect
confidentiality. Individual survey respondents were distributed across 13 (of 24 possible)
occupational groups and 'Other;' however, '0300 General Administrative, Clerical, and Office
Services' was the largest group (42 percent).
10
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 8. Percentage of Respondents Reporting Occupational Groups
42%
24%
15%
8%
6%
5%
0300
1500
1800
0100
0500
Suppressed
General
Mathematical
Inspection,
Social Science,
Accounting and
Occupational
Administrative,
Sciences
Investigation,
Psychology, and
Budget
Series and Other
Clerical, and
Enforcement, and
Welfare
Office Services
Compliance
Note: n=190
Source: Individual Survey Q52.
Exhibit 9 summarizes the distribution (percentage) of respondents across staff levels and years
of experience. Respondents were distributed across multiple staff levels; however, 'GS14-15' was
the largest group (72 percent). Respondent experience ranges from 0-16+ years; however, '0-5
years' was the largest group (39 percent).
Exhibit 9. Percentage of Respondents Reporting Staff Levels and Years of
Experience
Staff Levels
Years of Experience
Suppressed
Staff Levels
5%
SES
11-15
8%
years
16%
GS12-13
0-5
16%
6-10
years
years
39%
20%
GS14-15
72%
16+
years
26%
Note: n=191
Source: Individual Survey Q53 and Q54.
Inventory of Studies
The capacity assessment included a survey-based inventory of FY 2021 evaluation, research,
analysis, and statistics studies. The survey was fielded May 20 - June 11, 2021 to federal
personnel identified by Components as managing FY 2021 studies.
11
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
The assessment team used the study inventory survey to collect factual information from study
managers about FY 2021 studies, including but not limited to the following:
evidence building activity and subtype;
DHS strategic objectives and Component missions addressed by the study;
types of study purposes and foci of the evidence building;
types of data sources, data collection methods, and analysis used in evidence building;
types of claims or judgements made;
use of input from and dissemination to stakeholders external to the research team;
protections for the safety and privacy of participants; and
study plans and reports, to allow for the independent structured study review.
The study inventory survey and structured study review rating guides (described below) were
developed based on applicable OMB guidance that outlined the quality standards and best
practices for federal evidence building. 8 The study inventory survey was also designed to collect
data relating to multiple Evidence Act and OMB requirements, including a list of Department
operations and activities evaluated and analyzed (see Appendix C).
The assessment team fielded surveys for 368 studies that Components identified as underway in
FY 2021. Study managers responded to the survey for 244 studies (66 percent). This total included
86 total surveys where the respondent indicated that a study was not conducting in-scope
evidence-building activities (34 surveys) or was conducting performance measurement (52
analysis surveys); in these instances, the survey concluded without collecting more information.
Respondents completed surveys for 158 studies (43 percent of the 368 studies). Findings from
the survey must be interpreted keeping these limited response rates and the potential for bias in
mind; findings do not generalize beyond the reported studies. Exhibit 10 summarizes the total
number of surveys fielded and completed surveys received across DHS as a whole, as well as by
evidence-building activity.
Exhibit 10. Study Inventory Surveys Fielded and Completed, Total and by Activity
No in-scope
Number of
evidence
Total
Evaluation
Research
Statistics
Surveys
Analysis
Unspecified
building
activities
Fielded
368
54
72
150
27
65
N/A
Completed
244
34
57
92
27
N/A
34
Notes: "N/A" indicates that this was not applicable. Counts in the Fielded row represent Component-assigned
evidence building activity. Completed row represent evidence-building activity of studies as reported by respondents.
8For example, Regulatory Analysis, Circular A-4 (OMB, 2003); Regulatory Impact Analysis: A Primer, Circular A-4
(OMB, 2011); Professional Work in the Mathematical Sciences Group (OPM, 2005); Statistical Policy Directive No.
1: Fundamental Responsibilities of Federal Statistical Agencies and Recognized Statistical Units (OMB, 2014);
Guidance for Providing and Using Administrative Data for Statistical Purposes (OMB, 2014); Phase 4
Implementation of the Foundations for Evidence-Based Policymaking Act of 2018: Program Evaluation Standards
and Practices, M-20-12 (OMB, 2020)
12
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Structured Study Review
The assessment team planned to draw a sample of FY 2021 evaluation, research, analysis, and
statistics activities to independently rate the quality and independence of submitted study plans
or reports using the study inventory survey questions and study review rating guides. Only four
of the 51 documents submitted by respondents contained information that was sufficient for the
assessment team to conduct an independent review. Although the independent review could not
be completed as planned, the assessment team was able to analyze self-reported survey
responses to arrive at maturity ratings for seven CLCs. Exhibit 11 summarizes the criteria used for
rating study quality and independence.
Exhibit 11. Criteria Used for Rating Study Quality and Independence
Quality and Independence Criteria
Evaluation and Research
Rigor: Did the evaluation or research (1) make claims that do not go beyond the data; (2) use data collection
methods that specifically cover content that has direct bearing on the question being addressed; (3) acknowledge
relevant limitations; and (4) attempt to mitigate at least some of the limitations
Relevance and utility: Extent of connection between the evaluation or research and the missions of the Component
Transparency: How findings and methods are being disseminated (or planned to be disseminated)
Independence and objectivity: Extent to which the evaluation or research team did not modify the study based on
input from individuals outside of the research team in which there was conflict of interest present
Ethics: Extent to which the evaluation or research team protects the safety and privacy of the participants and other
affected entities
Improvements: Whether the evaluation or research team is looking to improve its work going forward
Analysis
Statement of need: Whether the analysis has a statement of need specifying a problem to be solved
Monitors goals: Whether the analysis facilitates a judgment about whether the goals of the activity were met
Examines costs and benefits: Whether the analysis makes judgments about the advantages and disadvantages of
the activity being analyzed
Examines alternative approaches: Whether the analysis considers other possible approaches besides the activity
being assessed
Utility: How findings and methods are being disseminated (or planned to be disseminated)
Improvements: Whether the analysis team is looking to improve its analysis going forward
Statistics
Relevance to policy: Extent to which input is collected on statistical methods and outputs from data users and
experts
Credibility among data users: Extent to which statistical information is disseminated through multiple channels, in
various forms that are appropriate to lay persons and experts, reflects limitations, and is made available in a way that
maintains confidentiality
Trust among data providers: Extent to which data are protected, such that they are analyzed under a pledge of
confidentiality and cannot be used for administrative, regulatory, law enforcement, or any other nonstatistical purpose
Independence from political and other undue external influence: Extent to which statistical personnel are authorized
to refuse to disseminate identifiable data, choose when to release data and how; and choose how to maintain and
store data
Improvements: Whether the statistical team is looking to improve its data and statistics going forward
13
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 12 maps the assessment dimensions and CLCs to the relevant methods used for data
collection and analysis.
Exhibit 12. Assessment Dimensions and CLCs Mapped to Methods
Dimensions and CLC Brief Descriptions
Coverage
Internal staff with sufficient training and specialized expertise to conduct
evidence-building activities
Internal staff with sufficient training and specialized expertise to procure
and manage external organizations, groups, or experts to support
evidence-building activities
Evidence-building staff (EBS) receive the supports necessary (e.g., time,
support staff, training) to conduct evidence-building activities
Budget for and investment in evidence-building activities in this fiscal year
Policies and guidance that specify how to plan and conduct evidence-
building activities
Inventory of studies with key information and progress of evidence-
building activities planned, in progress, and completing in this fiscal year
Quality
Policy outlines quality standards for evidence-building activities, and
compliance is regularly monitored
Presence and quality of Department-wide and Component-wide learning
agendas (LAs) for strategic priorities for statistics, evaluation, research,
and analysis
Presence and quality of Department- and Component-wide annual
evaluation plans (EPs) that specify significant evaluations for subsequent
FY
Presence and quality of Department- and Component-wide annual
Performance Plans (PPs) that specify annual performance measures and
targets that support the strategic plan
Research quality: Rigor, relevance and utility, transparency,
independence and objectivity, and ethics of research studies
Evaluation quality: Rigor, relevance and utility, transparency,
independence and objectivity, and ethics of evaluation studies
Analysis quality: Included statement of need, monitoring of goals,
examination of costs and benefits, examination of alternative approaches,
and utility
Statistical quality: Relevance to policy issues, credibility among data
users, trust among data providers, and independence from political and
other undue influence
Methods
Inventory of methods used for evidence building activities
Independence
Research independence and objectivity
Evaluation independence and objectivity
Statistical independence from political and other undue influence
14
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Dimensions and CLC Brief Descriptions
Effectiveness
Policy outlines requirements for evidence-related stakeholder
engagement plans and compliance is regularly monitored
Mechanisms have been implemented to assess evidence user needs,
input, and feedback; input and feedback informs evidence-building
Processes have been implemented to integrate evidence into business
processes, program activities, and organizational decision-making
Continuous improvement processes or learning cycle processes have
been implemented to use evidence for improvement
Policy outlines requirements for evidence dissemination plans and
compliance is regularly monitored
Mechanisms exist for effectively disseminating evidence products to
internal and external stakeholders
Mechanisms have been implemented for timely public release of
evidence products
Maturity Ratings and Synthesis of Findings
The assessment team applied the CLC-specific maturity scoring rubric and analysis methods to
the data collected for each CLC to produce a maturity rating for the CLC by evidence-building
activity and/or crosscutting (across all rated evidence-building activities). For CLC ratings based
on information gathered during discussion groups, the assessment team generated preliminary
maturity ratings and an accompanying explanatory rationale for each rating. Components
reviewed and could provide additional documentation to inform the assessment team's review
and, potentially, adjustments to the preliminary ratings.
Components were provided an opportunity to review and provide feedback on their draft
Component profile. Once Component profiles were finalized, the assessment team synthesized
findings across Components to document baseline capacities within each of the assessment
dimensions for DHS as a whole.
The assessment team produced a comprehensive two-volume report containing their detailed
analysis, findings, and recommendations from which this public summary report, a set of internal
Component profiles, and DHS- and Component-level briefing materials were drawn. From
November 2021 to February 2022 the assessment team conducted twelve briefings on assessment
findings with DHS HQ and Component representatives. Participation in a DHS-wide briefing and
eleven Component briefings, delivered in the context of their capacity planning activities, was
coordinated by Components. These reports and briefings provided the information needed to
initiate informed discussions with Component leaders about a desired future state that is most
appropriate for each Component's mission, operations, resources, and needs, and the appropriate
milestones to achieve that future state from the baseline capacity documented in this assessment.
15
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Challenges and Limitations
The assessment team's use of qualitative and quantitative methods and multiple data sources
were complementary, and while incomplete, converged toward common findings. Several
limitations should be considered when reviewing the findings.
Complexity. The dimensions, evidence-building activities, and CLCs assessed were complex
and overlapping, requiring understanding and judgements around the terms, definitions, and
uses that were difficult even for technical experts. Variation of evidence-building capabilities
and organizational characteristics (for example, mission, structure, and philosophy on
centralization) between and within Components contributed additional complexity.
Complexity added to the overall challenge of fostering unity in how evidence-building and
maturity are defined across the four broad activities and how to assess and continuously
improve capacity for evidence.
Sampling and response. The data reflect perspectives and factual information provided by
DHS federal personnel who contribute to Components' most significant, in-scope evidence-
building activities.9 The resulting findings were determined to a great extent by whether
personnel whose input was vital to the capacity assessment were prepared and able to
participate in the assessment, not just as informed respondents but as collaborators on
sampling. Despite efforts to prepare, support, and provide alternatives to optimize
participation, issues persisted through the data-gathering and analysis phases that led to
sampling and response limitations. Findings from the survey must be interpreted keeping
limited response rates and the potential for bias in mind.
Operations research as an analysis activity. The inclusion of operations research as an
analysis activity, given there is no existing federal or DHS guidance for operations research,
introduced limitations in the analysis findings and how they should be interpreted.
Specifically, inclusion of operations research may have had the effect of lowering some
analysis maturity ratings, because many of the CLCs tied to guidance or standards.
Objectivity of Quality and Independence dimension measures. The assessment team could
not complete an independent review of studies due to the insufficient documentation (study
plans and reports) provided by respondents in connection to FY 2021 studies. Instead, they
used information reported by respondents in the study inventory survey to assess studies'
quality and independence and to establish respective maturity ratings. Although respondents
had access to significantly more information related to the quality and independence of each
effort, these data may also be similarly affected by issues related to complexity, sampling,
and response or other bias.
9 The assessment intentionally did not seek perspectives of a broader group of DHS federal personnel as users of
evidence. The Government Accountability Office's 2020 Survey of Federal Managers summary of DHS managers'
responses provides this additional perspective. See 2020 Federal Managers Survey: Results on Government
Performance Management issues, Department of Homeland Security (DHS) (GAO, 2021)
16
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Findings
A high-level snapshot, or scorecard, of maturity across DHS evidence-building activities and
statutory dimensions is shown in Exhibit 13. The remainder of the section presents an overview
of CLC maturity ratings and findings, organized by the five assessment dimensions (i.e., coverage,
quality, methods, effectiveness, independence).
Maturity of DHS Evidence Building Activities
Conducting the capacity assessment provided an opportunity to discover, describe, and assess
the Department's evidence-building, determining where current evidence capacity is and may
not be sufficient to meet current and future needs and where capacity building efforts should be
targeted.
Overall DHS lacked shared understanding - common terminology and concepts - related to
evidence-building as defined by the Evidence Act and OMB. Most Components did not have
coordinated Component-level perspective on staff, funding, and other infrastructure for
evidence building or for evidence-building activities (studies) underway in fiscal year (FY) 2021.
Despite this, the assessment team determined that all four evidence building activities (i.e.,
evaluation, research, analysis, and statistics) were underway across the Department with varying
levels of maturity on the assessment dimensions of coverage, quality, methods, independence,
and effectiveness, as summarized in Exhibit 13. DHS capacity for evidence-building and use was
uneven across Components, nascent in some places, but developing. Given the participants were
restricted to individuals for which evidence building was relevant, these findings suggested
opportunities to improve in the future.
Exhibit 13. Maturity of DHS Evidence Building on Assessed Dimensions, by
Evidence-Building Activity
Activity
Maturity Model
Dimension
Evaluation
Research
Analysis
Statistics
5 = Optimizing
Coverage
Implementing
In progress
Implementing
Implementing
4 = Completed
Quality
Implementing
In progress
Implementing
In progress
3 = Implementing
Independence
Completed
Completed
Not rated
Implementing
2 = In progress
Effectiveness
In progress
In progress
Implementing
In progress
1= Not initiated
Notes: Maturity for the Methods dimension is incorporated in the quality
dimension maturity score. Specifically, the quality dimension assesses: "do these
0 = No basis for judgement
methods incorporate the necessary level of rigor?"
17
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Coverage
COVERAGE:
The Coverage dimension addresses the question: What
Staff with training and expertise
evaluation, research, analysis, and statistical activities
for evidence building
are happening, and where are these activities
Staff with training and expertise
happening?
for procuring and managing
The assessment team defined Coverage in terms of
external groups for evidence
organizational and individual capacity to plan and
building
conduct evidence building and the evidence-building
Organizational support for
activities (studies) underway in FY 2021. The Coverage
evidence building
CLCs captured the extent to which Component staff had
Budget and expenditures for
that conduct evidence building as primary functions of
evidence building
their jobs, whether staff have expertise and training
Policies and procedures for
around evidence-building and were provided with
planning and conducting
appropriate supports to conduct their activities, levels of
evidence building
funding budgeted for and invested in evidence-building,
Inventory of FY 2021 evidence-
and whether DHS Components had Component-wide
building activities (studies)
policies and guidance in place to plan and conduct their
primary evidence-building activities. One CLC collected
information on all FY 2021 evidence-building activities (studies). This section presents findings
from the discussion group and accompanying document review, individual survey, and study
inventory survey.
Overview of Coverage Findings
DHS received a range of maturity scores between '1' (not initiated) and '3' (implementing) across
the Coverage CLCs. Exhibit 14 presents DHS maturity ratings for these CLCs by evidence-building
activity (per CLC and overall) and crosscutting (across all evidence-building activities). Coverage
CLCs (average) ratings reflect the average maturity ratings (scores) for the rated activity specific
CLCs.
The assessment team did not assign a CLC maturity rating for the inventory CLC; instead, they
provided DHS a structured inventory of FY 2021 studies and also summarized study
characteristics in dimension-specific findings sections. Appendix C includes a list of selected
evaluation, research, analysis, and statistics studies from the assessment team's inventory of FY
2021 studies that were or expect to be disseminated to the public. PA&E supplemented the list
with studies from the Department's annual evaluation plans (FY 2022 and FY 2023) and the
unified regulatory agendas (FY 2021 and FY 2022) that were not included in the inventory of the
FY 2021 studies. To ensure the list was useful as an inventory of studies, PA&E also incorporated
studies identified through a search of DHS, Component, and DHS-sponsored entities' websites.
18
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 14. Coverage Maturity Ratings, by Activity and Crosscutting
Activity
CLC Brief Description
Evaluation
Research
Analysis
Statistics
Crosscutting
Internal staff with sufficient training and
specialized expertise to conduct evidence- 3
3
3
3
3
building activities
Internal staff with sufficient training and
specialized expertise to procure and
manage external organizations, groups, or 2
3
2
2
2
experts to support evidence-building
activities.
Evidence-building staff (EBS) receive the
supports necessary (e.g., time, support
3
3
3
3
3
staff, training) to conduct evidence-
building activities
Budget for and investment in evidence-
2
2
2
3
2
building activities in this fiscal year
Policies and guidance that specify how to
plan and conduct evidence-building
3
1
3
2
2
activities
Inventory of studies with key information
and progress of current and planned
Not rated
Not rated
Not rated
Not rated
Not rated
evidence-building activities
Coverage CLCs (average)
3
2
3
3
2
Coverage Summary Finding: DHS had some staff, funding, and policies in place, and evidence-
building (studies) underway in FY 2021, but they were distributed unevenly across the
Department and considered by many Component representatives to be insufficient to fully
achieve all the objectives defined in the Evidence Act.
DHS had staff with primary responsibilities for evidence building, but the size of the
evidence-building staff varied across Components and activities. Components identified 483
individuals who support Components' most significant evidence-building as a primary
function of their job. Personnel were distributed unevenly, from eight in one Component to
128 in another. Component-level survey response rates varied from 16 percent to 77 percent.
Respondents reported some knowledge of core evidence-building competencies though
they deemed many competencies only somewhat relevant to their jobs. The percentages
of respondents that self-reported higher levels (rating of '4' or (5') of knowledge and
relevance to their job varied widely (33-83 percent for knowledge and 20-84 percent for
relevance). For 16 of the 20, core evidence-building competencies assessed, fewer than 70
percent of respondents reported higher levels of knowledge and relevance. For nine
competencies, fewer than 50 percent of respondents considered them relevant to their jobs.
Exhibit 15 summarizes a subset of the assessed competencies, including those with the six
highest and six lowest percentages of respondents reporting higher levels of both knowledge
and relevance to their job. Appendix D provides the full set of items and responses.
19
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 15. Percentage of Respondents Reporting Higher Levels ('4' or '5') of
Knowledge and Relevance for Competencies
Knowledge
Relevance
Present results concisely using language accessible to the
83%
audience
84%
Engage stakeholders to understand relevant context
76%
relating to data
79%
Disseminate recommended actions to decisionmakers that
74%
are based on results from evidence-building activities
72%
Engage with appropriate stakeholders to identify
73%
researchable questions, hypotheses, or issues to study
68%
Consider and critically assess the strengths and limitations
67%
of different data sources
68%
63%
Identify contextual factors that affect implementation
65%
Analyze qualitative data using appropriate methods such
52%
as content analysis or thematic analysis
47%
Choose the study design and methods that are best suited
51%
for a research question
42%
Assess the reliability of a specific study by identifying
48%
related studies and comparing methods and results
41%
Use logic or system models that convey how implemented
47%
activities are expected to produce outcomes
43%
45%
Procure and manage external organizations
38%
Follow Federal regulations on the ethical treatment of
33%
human subjects in research and evaluation, including
seeking informed consent and protecting confidentiality
20%
Notes: n=186-190. Rating scales were 1 (No knowledge) to 5 (Extensive knowledge/expertise) and 1 (Not relevant)
to 5 (Central to my job or position).
Source: Individual Survey Q2-Q21 (a subset are shown here).
Respondents reported higher levels of knowledge and relevance for presenting results concisely
(83 percent), engaging stakeholders to understand relevant context (76 percent) or to identify
researchable questions (73 percent), and disseminating recommended actions to decisionmakers
(74 percent). Fewer than half of respondents reported higher levels of knowledge for assessing
the reliability of a specific study (48 percent), using logic or system models (47 percent), procuring
and managing external organizations (45 percent), and following federal regulations on the
ethical treatment of human subjects in research and evaluation (33 percent).
20
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Majority of respondents reported they did not receive adequate professional training for
how to design or conduct evidence building or procure external organizations for evidence
building. Fewer than 70 percent of individual survey respondents agreed or strongly agreed
they received adequate professional training on how to use data effectively (65 percent), how
to design and conduct evidence-building activities (49 percent), and how to procure and
manage external organizations, groups, or experts for evidence building (42 percent). Exhibit
16 summarizes responses to adequacy of training.
Exhibit 16. Percentage of Respondents Reporting 'Agree' or "Strongly' Agree to
Receiving Adequate Professional Training
Received Adequate Professional Training
How to use data effectively to inform what I do on a daily
65%
basis
How to design and conduct evidence-building activities
49%
How to procure external organizations, groups, or experts
42%
Notes: n=190. Rating scale was Strongly Disagree, Disagree, Agree, Strongly Agree.
Source: Individual Survey Q24, Q27, and Q28.
Respondents received some of the necessary supports to conduct evidence-building
activities. Fewer than 70 percent of respondents agreed or strongly agreed they had support
for evidence building, including training to find, gather and assess data (60 percent); access
to support staff (59 percent); and time to conduct evidence building activities (55 percent) or
participate in a professional learning community (45 percent). Exhibit 17 (next page)
summarizes responses to support for evidence building. Appendix D provides the full set of
items and responses used in Exhibits 16 and 17.
DHS may not have enough dedicated staff, or enough staff and contractors with specialized
expertise, to effectively conduct evidence-building activities. One hundred fifty-eight (158)
respondents described what resources they need to effectively conduct evidence building
activities. Of those, many respondents reported the need for more staff (44 percent), and
specifically, more professional staff, contractors, and subject matter experts with specialized
expertise in evidence building. Respondents also reported needing training for existing staff
(24 percent), access to data and tools for analysis (22 percent), collaboration (15 percent),
time to conduct evidence-building activities (14 percent), and funding (11 percent). Twelve
percent of respondents indicated no resources were needed. Exhibit 18 (next page)
summarizes responses to what resources are needed.
21
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 17. Percentage of Respondents Reporting 'Agree' or 'Strongly' Agree to
Receiving Support for Evidence Building
Received Support for Evidence Building
Adequate professional learning to find, gather, and
60%
critically assess data from different sources
Access to the support staff I need to conduct evidence-
59%
building activities.
Adequate time to conduct evidence-building activities
55%
Dedicated time to participate in a data or evidence-related
45%
professional learning community
Notes: n=190. Rating scale was Strongly Disagree, Disagree, Agree, Strongly Agree.
Source: Individual Survey Q22, Q23, Q25, and Q26.
Exhibit 18. Percentage of Respondents Describing Additional Resources Needed
44%
24%
22%
15%
14%
11%
N
Staff and
Training
Data and
Collaboration
Time
Funding
Expertise
Analysis
Tools
Notes: n=158. Does not show 4% that responded with 'resources' or 12% that reported 'none' or 'no resources
needed'. Respondents could state all that apply, so percentages do not add to 100%.
Source: Individual Survey Q29.
The size of budgets to support evidence building varied by Components and by activity and
many Component representatives reported their budgets were insufficient to meet all the
Evidence Act objectives. One Component reported a Component-wide funding structure for
evidence-building with funds both budgeted and invested for evaluation, research, and
analysis activities. Some Components had substantial funding for one or more evidence-
building activities, often statistics. Representatives of six Components reported in discussion
groups that they had insufficient funds to support evidence-building activities. They noted
that as a result, the Component could not fully achieve all the objectives defined in the
Evidence Act.
22
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
DHS-wide policies provided relevant information to plan and conduct evidence-building
activities, but Components did not have equivalent policies and guidance for various
evidence-building activities. DHS-wide policies and guidance for evaluation and some
analysis activities (e.g., regulatory analysis, and performance measurement) provided
relevant information on how to plan and conduct those evidence-building activities. Six
Components reported they did not have any policies or guidance in place specifying how to
plan and conduct any evidence-building activity. Five Components had policies or guidance
in place for at least one activity. While some Components were in the process of developing
such policies and guidance, others highlighted challenges, including culture, associated with
creating additional, centralized guidance to inform evidence-building at the Component level.
DHS had a portfolio of evidence-building and the size of the evidence-building portfolio
varied across activities and Components. Components identified 368 studies underway in FY
2021. Studies were distributed unevenly, from zero studies in one Component to 74 in
another. Component-level survey response rates for the study inventory varied from eight
percent to 71 percent, excluding the 86 surveys where the respondent indicated no in-scope
evidence building activities or performance measurement.
Trends in the characteristics of FY 2021 studies suggested other areas in which DHS
coverage may not be sufficient to meet key objectives of the Evidence Act.
- DHS evidence-building activities should support the Department's strategic plan, address
all divisions' needs, and consider equity, a key Administration priority. Evaluation and
research studies supported the full range of DHS strategic objectives (data not shown),
with the largest percentages of studies aligned to "responding during incidents" (27
percent); "secure and manage air, land, and maritime borders" (26 percent) and
"maintain U.S. waterways and maritime resources" (24 percent). Studies were submitted
by more than 70 DHS divisions. Most evaluation and research studies (86 percent) directly
related to Component missions. Very few studies (13 percent) addressed equity issues.
- DHS should meaningfully engage and collaborate with external stakeholders (such as
public, state, and local agencies, and non-governmental researchers) throughout the
evidence lifecycle. Yet only one in four reported studies (26 percent) was conducted by
external organizations and about one in three (31 percent) reported studies disseminated
information to the public. The Independence and Effectiveness sections report these data
in greater detail.
- DHS should use evidence to improve programs, policies, regulations, strategies, and
operations. Greater percentages of studies focused on operations (35 percent), programs
(29 percent), and activities (19 percent) compared to policies (eight percent), strategies
(two percent), and regulations (one percent).
These study characteristics are summarized in Exhibit 19.
23
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 19. Selected Characteristics of FY 2021 Studies
Most evaluation, research,
1 in 4 studies was
and analysis studies focused
conducted by
on operations, programs,
external researchers
and activities
Operations
35%
Programs
29%
External
Activity
19%
26%
Policy
8%
Internal
Phenomenon
Evaluation and research
6%
studies covered
Strategy
2%
all DHS Strategic Objectives
Regulation
1%
Most evaluation and
research studies (86%)
directly related to
Public
Component missions
31%
Dissemination
No Public
Dissemination
Directly
1 in 3 evaluation, research,
86%
and analysis studies were
Few studies (13%)
disseminated to the public
addressed equity
Directly
Moderately
Very little
Notes: n=157-158 for all studies; n=90-91 for evaluation and research studies; and n=130-131 for evaluation,
research, and analysis studies.
Source: Study Inventory Survey Q4, Q13, Q15, Q16, Q17, Q27a (research); Q29, Q37, Q40, Q41, Q42, Q52a
(evaluation); Q54, Q62, Q70, Q75a (analysis); Q85, Q92a (statistics).
Quality
QUALITY:
The Quality dimension addresses the question: Are the
Policies and guidance outlining
data used of high quality with respect to utility,
quality standards
objectivity, and integrity?
Presence and quality of evidence
The assessment team focused on what capacity existed
plans
for and whether evidence building activities were
Quality of evidence-building
conducted in such a way as to deliver high quality
activities and improvement
information. The Quality CLCs focused on the extent to
actions underway, including rigor
which DHS and Components had established, activity-
specific quality standards consistent with federal guidance and adhered to those standards when
planning and conducting evidence-building. Rigor of study methods is assessed in study quality
CLCs, and so maturity of the Methods dimension is subsumed in the Quality dimension maturity
rating. This section presents findings from the discussion groups, accompanying document
review, the study inventory survey, and structured study review.
24
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Overview of Quality Findings
DHS received a range of maturity ratings between '1" (not initiated) and '5' (optimizing) across
the Quality CLCs. Exhibit 20 presents DHS maturity ratings for eight CLCs.
Exhibit 20. Quality Maturity Ratings, by Activity and Crosscutting
Activity
CLC Brief Description
Evaluation
Research
Analysis
Statistics
Crosscutting
Policy outlines quality standards for
evidence-building activities, and
2
1
3
2
2
compliance is regularly monitored
Presence and quality of Department- and
Component-wide learning agendas (LAs)
Not rated
Not rated
Not rated
Not rated
3
for strategic priorities for statistics,
evaluation, research, and analysis
Presence and quality of Department- and
Component-wide annual evaluation plans
Not rated
Not rated
Not rated
Not rated
5
(EPs) that specify significant evaluations
planned for subsequent FY
Presence and quality of Department- and
Component-wide annual Performance
Plans (PPs) that specify annual
Not rated
Not rated
Not rated
Not rated
3
performance measures and targets that
support the strategic plan
Evaluation quality: Rigor, relevance and
utility, transparency, independence and
3
Not rated
Not rated
Not rated
Not rated
objectivity, and ethics of evaluation
studies; actions underway to improve
Research quality: Rigor, relevance and
utility, transparency, independence and
Not rated
3
Not rated
Not rated
Not rated
objectivity, and ethics of research studies;
actions underway to improve
Analysis quality: Included statement of
need, monitoring of goals, examination of
costs and benefits, examination of
Not rated
Not rated
2
Not rated
Not rated
alternative approaches, and utility; actions
underway to improve
Statistical quality: Relevance to policy
issues, credibility among data users, trust
among data providers, and independence
Not rated
Not rated
Not rated
1
Not rated
from political and other undue influence;
actions underway to improve
Quality CLCs (average)
3
2
3
2
Not rated
Maturity ratings for the first CLC (policy outlines quality standards) represent an average of DHS
Components' maturity scores for the evidence-building activities and crosscutting scores. 'Not
rated' and gray shading indicate that a CLC was assessed only on an activity-specific or
crosscutting basis. Quality CLCs (average) ratings reflected the average maturity scores for the
rated activity specific CLCs, adjusted to account for additional information provided by DHS HQ
as appropriate.
25
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Quality Summary Finding: DHS-wide policies and guidance outlined quality standards for some
activities, though compliance with these standards varied across evidence-building activities.
Department-level evidence plans were relatively mature. Components did not have equivalent
policies and plans for their evidence-building activities. FY 2021 studies met some quality
standards but the percentage of studies meeting standards varied by activity and by standard.
DHS-wide policies and guidance specified quality standards for some activities, but
Components did not have equivalent policies and guidance. DHS had department-wide
policies and guidance for evaluation and certain analysis activities (i.e., regulatory analysis
and performance measurement) that provided relevant guidance on quality standards. The
Department also had broad directives for scientific integrity and quality of scientific,
statistical, and financial information and was developing policies to improve the quality of
certain statistical activities. No DHS Component reported a Component-wide policy that
outlined quality standards for any evidence-building activity at the time of the assessment,
though five Components were in the process of developing such policies.
DHS monitored compliance with quality standards for certain activities. Across the
Department, representatives reported that robust processes were in place to facilitate quality
and consistency in planning and building evidence for two analysis activities (i.e., regulatory
analysis and performance measurement.) These processes included internal and external
review of evidence-building against established quality standards and federal best practices.
The assessment team could not confirm the extent to which DHS-wide evaluation, research,
and information quality standards were implemented and monitored for compliance in
Components.
Department-level evidence plans were relatively mature, but Components did not have
equivalent plans. Department-wide evidence-building plans met the majority of assessment
criteria (data not shown). The interim learning agenda (LA) and evaluation plan (EP) provided
clear, concrete, and actionable guidance for developing and using evidence to inform DHS
activities and consultation with internal stakeholders, but lacked consultation with external
stakeholders. 10 The annual performance report included target and actual performance
measures from FY 2016 to present and planned targets for FY 2021 and FY 2022 but was not
sufficiently aligned with the strategic plan. DHS Components had no formal evidence plans in
place at the time of this assessment,11 which limits the Department's ability to assess current
capacity against evidence needs and appropriately allocate resources to address needs and
capacity gaps.
10
Consultation with external stakeholders occurred during the course of this capacity assessment for development
of the Department's FY2022-2026 learning agenda, but information about these efforts were not available to the
assessment team.
11 The assessment team reviewed a Component strategic plan that included annual performance targets. The
results of the plan's review are not provided in this report.
26
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study inventory data suggested reported FY 2021 studies met some quality standards, but
the percentage of studies meeting standards varied by activity and by standard. Study
contacts' responses to questions related to quality (six multi-part questions for each of
evaluation, research, and analysis, and five for statistics) suggested higher percentages of
evaluation and research studies met quality standards than did analysis and statistical
studies. The percentage of studies meeting quality standards are summarized in Exhibit 21.
Strengths and weaknesses are also described in the three findings that follow.
Exhibit 21. Percentage of FY 2021 Studies that Met Quality Standards, By
Evidence-Building Activity
EVALUATION STUDY QUALITY
RESEARCH STUDY QUALITY
Met
Met
Standard
Standard
Ethics
91%
Relevance and utility
98%
Relevance and utility
82%
Ethics
95%
Independence and objectivity
76%
Independence and objectivity
93%
Rigor
41%
Rigor
44%
Transparency
32%
Transparency
30%
Met four of five elements
47%
Met four of five elements
60%
Looking to improve
68%
Looking to improve
58%
ANALYSIS STUDY QUALITY
STATISTICS STUDY QUALITY
Met
Met
Standard
Standard
Statement of need
60%
Independence from political or undue
external influence
37%
Examines costs and benefits
48%
Examines alternative
Credibility among data users
33%
approaches
48%
Trust among data providers
15%
Monitors goals
43%
Relevance to policy
11%
Utility
30%
Met four of five elements
30%
Met four of four elements
8%
Looking to improve
60%
Looking to improve
70%
Notes: evaluation n=34; research n=57; analysis n=40; and statistics n=27. Source: Study Inventory Survey.
27
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Most reported FY 2021 evaluation and research studies met standards for relevance and
utility, independence and objectivity, and ethics; however, fewer studies met standards for
rigor and transparency. The percentage of evaluation and research studies that met
relevance and utility, independence and objectivity, and ethics standards ranged from 76
percent to 98 percent. The comparatively low percentages for transparency (about 30
percent for both evaluation and research) indicated that study findings, methods, and data
(either in full or summary form) were not routinely released to the public or other key
stakeholders. The low percentages for rigor (about 40 percent for both evaluation and
research) indicated that studies did not attempt to mitigate at least some identified
limitations (for research) or made claims that go beyond the data (for evaluation).
Majority of reported FY 2021 analysis studies, which were predominantly operations
research, 12 did not meet criteria for quality. Respondents self-reported that fewer than half
of the 40 analysis studies examined costs and benefits, examined alternative approaches,
assessed whether goals were met, and had utility. 13 Analysis studies scored lowest on criteria
related to utility, indicating that DHS did not regularly release findings and methods, including
deidentified analysis data, so that laypeople and experts can understand the analysis.
Majority of reported FY 2021 statistical studies¹ did not meet criteria for quality. Of the
four criteria used to determine the quality of statistics, relevance to policy and trust among
data providers received the lowest scores. Of the three elements examined for relevance to
policy, the most common weakness was collecting input on statistical methods and outputs
from both data users and experts. Of the two elements assessed for building trust among
data providers, the most common weakness was analyzing data under a pledge of
confidentiality. Respondents for 70 percent of the reported statistics products indicated
teams were looking to improve their data and statistics in the future. This suggests that many
DHS evidence-building staff who conduct statistics recognize the need to improve on their
work and address gaps.
12 The study inventory survey was not completed for performance measurement activities and few policy or
regulatory analysis studies were reported; thus, the quality of studies reflected primarily operations research.
13 OMB guidance for analysis activities consistently referenced 'utility' as a criterion of quality, described as data's
utility for intended users and for its intended purpose, including downstream use and reproducibility.
14 DHS does not have any designated statistical agencies or activities that are required to comply with the
Confidential Information Protection and Statistical Efficiency Act of 2018 (CIPSEA).
28
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Methods
METHODS:
The methods dimension addresses the questions: What
Evidence building activity and
are the methods being used for these activities, do these
subtype
methods incorporate the necessary level of rigor, and
Focus and purpose of the
are those methods appropriate for the activities to which
evidence building
they are being applied?
Data sources, data collection
The assessment team incorporated rigor of methods
methods, and analysis methods
(or equivalent concept) in the Quality dimension CLC
used in evidence building
maturity ratings, so it is not rated separately here. This
Nature of claims and
section presents a partial summary of the methods
judgements made
inventory, as reported by 158 study inventory survey
responses across 14 Components, and findings. The responses and documents provided by
respondents were not sufficient to examine the appropriateness of the methods to specific
Component divisions, or to questions, purposes, or claims made for a specific study. However,
this section illustrates, with evaluation, how summary information about study purposes, claims,
and methods can still be useful in considering appropriateness of methods for an evidence-
building activity.
Two important concepts in this section are study purpose and study claims. A formative purpose
is one that informs or improves the design or implementation of an activity. A summative
purpose is one that determines what goals, outcomes, or impacts have been achieved as a result
of implementing an activity. Some studies only describe people, events, or incidents. Other
studies make claims such as inferences, such attempting to show a relationship between
activities and measurable outcomes. Studies may make other claims, often derived from the
inferences made. The credibility of inferences is dependent on the specific data, methods, and
study designs used to address the questions.
15
Overview of Methods Findings
Methods Summary Finding: DHS evidence-building addressed a balance of formative and
summative aspects, and studies used a wide range of qualitative and quantitative data, data
collection, and analysis methods. The frequencies with which certain data sources and methods
were used may limit the Department's ability to make credible inferences and fulfill summative
purposes, especially for quantifying effectiveness (outcomes and impacts), cost-effectiveness, or
equity that relates to or results from DHS mission delivery.
15 OMB guidance indicates that questions of effectiveness or efficiency should be answered by evaluation and
claims of effectiveness and impact must be supported by study designs that can credibly generate causal evidence
when well executed. Impact evaluations include experimental (i.e., randomized control trials) and quasi-
experimental designs and may necessitate direct data collection and long-term or multi-year participant follow-up.
See Evidence-Based Policymaking: Learning Agendas and Annual Evaluation Plans, M-21-27 (OMB, 2021)
29
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
The sample of reported FY 2021 studies reflected a range of evidence-building
activities and subtypes. The survey responses mostly reflected studies conducting
applied research (17%), operations research/analysis (15%16 evaluation (14%), and
statistics (11%). Exhibit 22 summarizes how respondents categorized the studies they
reported in the study inventory.
Exhibit 22. Percentage of Studies Reporting Evidence-Building Activity and
Subtype
Statistics
11%
Performance
measurement
and monitoring
21%
Statistics
No in-
11%
scope
evidence
building
Other
activities
14%
Analysis
Project
14%
37%
evaluation
0%
Operations
Evaluation
research
Evaluation of
14%
15%
programs,
policies, or
regulations
14%
Research
24%
Policy and
Foundational
Applied
regulatory
research
research
analysis
2%
17%
1%
Basic
research
2%
Exploratory
research
3%
Notes: evaluation n=34; research n=57; analysis n=92; and statistics n=27. Project evaluation is evaluation of
ongoing or completed grantee projects required by DHS grantmaking programs.
Source: Study Inventory Survey Q1.
16 The study inventory survey was not completed for performance measurement activities and few policy or
regulatory analysis studies were reported; thus, the methods of studies reflected primarily operations research.
30
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study purposes varied by evidence-building activity but were balanced in addressing
formative and summative purposes overall. The percentage of studies that addressed some
summative purposes varied across research (62 percent), evaluation (75 percent), analysis
(80 percent), and statistics (93 percent). Overall, they were balanced in addressing formative
and summative purposes: 75 percent of studies had some summative aspects and 77 percent
of studies had some formative aspects. Exhibit 23 summarizes percentages of studies
reporting purpose, by activity and total.
Exhibit 23. Percentage of Studies Reporting Purpose, Total and by Activity
Formative
Summative
Both formative and summative
Evaluation
25%
28%
47%
Research
38%
14%
48%
Analysis
20%
12%
68%
Statistics
7%
52%
41%
Total
25%
23%
52%
Notes: evaluation n=32; research n=56; analysis n=40; and statistics n=27.
Source: Study Inventory Survey Q11a (research); Q36a (evaluation); Q61a (analysis); and Q83a (statistics).
Most reported evaluation and research studies attempted to make an inference, though
most analysis studies did not make judgements about activities studied. Exhibit 24 indicates
that most research (81 percent) and evaluation (64 percent) studies attempted show a
relationship between implemented activities and measurable outcomes (make an inference).
Fewer than 50 percent of analysis studies made statements about the advantages and
disadvantages of an activity (48%), other possible approaches to the activity (48%), or
whether the goals of an activity were met (43%).
Exhibit 24. Percentage of Studies Reporting Claims, Total and by Activity
Evaluation and Research
Analysis
Describe only
Make an inference
Statements Made
Advantages and
Evaluation
36%
64%
48%
disadvantages
Other possible
Research
19%
81%
48%
approaches
Total
Whether goals
26%
74%
43%
were met
Notes: research n=57; evaluation n=33; and analysis n=40.
Source: Study Inventory Survey Q13 (research); Q38 (evaluation); and Q65, Q66, Q69 (analysis).
31
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Studies used a variety of methods to collect data directly from individuals, and many used
qualitative methods only or mixed qualitative and quantitative methods of data collection.
Across reported evaluation, research, and analysis studies, 34 percent used qualitative data
collection methods, such as interviews or focus groups, observations, and questionnaires.
Twenty-nine percent of studies combined qualitative methods with other quantitative
methods, such as surveys (of a census, statistical sample, or non-statistical sample) and tests
of cognition, attitudes, or skills. Fewer studies (19 percent) relied on quantitative methods
only and 18 percent did not collect data directly. Exhibit 25 reports the most common data
collection methods across studies, by evidence-building activity.
Exhibit 25. Percentage of Studies Reporting Direct Data Collection Methods, by
Activity
Evaluation
Research
Analysis
53%
50%
44%
43%
39%
32%
23%
23%
26%
25%
23%
21%
9%
3%
3%
Quantitative survey
Qualitative interview
Observation
Qualitative
Test (e.g., cognitive,
or focus group
questionnaire
affective, skill)
Notes: evaluation n=34; research n=57; and analysis n=92. The most common methods across all reported studies
are shown. Quantitative survey combines three categories: survey of census (program's population), survey of
statistical sample, and survey of nonstatistical sample. The following methods were less common across all studies
though percentages varied by activity: 'log, journal, or diary, journey/process mapping,' 'non-written artifact,' and
'other. Respondents could select all that apply, so percentages do not add to 100%.
Source: Study Inventory Survey Q27b (research); Q52b (evaluation); and Q75b (analysis).
Most FY 2021 studies collected data directly from individuals within DHS Components,
while fewer studies collected data from DHS customers and external partners. Nearly 80
percent of reported evaluation, research, and analysis studies collected information directly
from individuals within DHS. Comparatively fewer collected data directly from external
stakeholders, such as individuals of other federal agencies, program customers, non-
government researchers, or state, local, tribal, or territorial (SLTT) agencies. Exhibit 26 reports
the most common direct data sources across studies, by evidence-building activity.
32
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 26. Percentage of Studies Reporting Direct Data Sources (Individuals from
Organizations or Groups), by Activity
Evaluation
Research
Analysis
84%
83%
68%
35%
30%
26%
15%
18% 18%
18%
11%
13%
12%
8%
3%
DHS Component
Federal agency
Program customer
Researcher
State, local, tribal,
or beneficiary
or territorial agency
Notes: evaluation n=34; research n=57; and analysis n=92. The most common sources across all reported studies
are shown. DHS Component(s) combines two categories: Submitting Component and Other DHS Component. The
following were less common across all studies though percentages varied by activity: "federal grant recipient,
"industry or trade group,' "nongovernmental or nonprofit organization,' 'professional association, interest or advocacy
group,' American public,' and "other.' Respondents could select all that apply, so percentages do not add to 100%.
Source: Study Inventory Survey Q27c (research); Q52c (evaluation); and Q75c (analysis).
Studies used a wide range of existing federal data sources to support evidence building.
Greater than 60 percent of reported evaluation, research, and analysis studies used existing
federal data and information to support evidence building. Performance measures,
administrative and operational data, federal statistical data, policies, plans, and reports from
reviews, assessments, or evaluations were most common. Exhibit 27 reports common
existing federal data sources by evidence-building activity.
Exhibit 27. Percentage of Studies Reporting Existing Federal Data Sources, by
Activity
Evaluation
Research
Analysis
44%
32% 32%
35%
28%
32%
30%
30%
25%
23%
21%
16%
12%
9%
12%
Performance
Policies and plans
Reports
Administrative/
Statistical data
measures
operational data
Notes: research n=57; evaluation n=34; and analysis n=92. The most common sources across all reported studies
are shown. The following data were less common across all studies though percentages varied by activity: 'physical
and technological infrastructure data, 'financial data,' 'sensor or telemetry data,' "personnel data,' and 'other."
Respondents could select all that apply, so percentages do not add to 100%.
Source: Study Inventory Survey Q27d (research); Q52d (evaluation); and Q75d (analysis).
33
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Qualitative analysis, in particular content and case study analysis, was common across
evidence building; however, fewer studies used advanced qualitative analysis methods.
More than 60 percent of reported evaluation, research, and analysis studies used qualitative
analysis to support evidence building. Content analysis and case study analysis were common
across evidence building activities. Comparatively, lower percentages of studies reported
using advanced methods such as thematic framework analysis, grounded theory, interpretive
phenomenological analysis, and ethnographic analysis. 17 Exhibit 28 reports common
qualitative analysis by evidence-building activity.
Exhibit 28. Percentage of Studies Reporting Qualitative Analysis Methods, by
Evidence-Building Activity
Evaluation
Research
Analysis
41%
38%
30%
21%
18%
23%
23%
18%
15%
18%
9%
9%
11%
13%
6%
Content analysis
Narrative analysis
Case study
Expert panel review
Literature/
systematic review
Notes: research n=57; evaluation n=34; and analysis n=92. The most common methods across all reported studies
are shown. The following methods were less common across all studies though percentages varied by activity:
"thematic or thematic framework analysis,' "interpretive phenomenological analysis,' 'grounded theory,' "ethnographic
analysis,' and 'other.' Respondents could select all that apply, so percentages do not add to 100%.
Source: Study Inventory Survey Q27e (research); Q52e (evaluation); and Q75e (analysis).
Quantitative analysis, in particular descriptive statistics, was common across DHS evidence
building. Fewer evaluation, research, and statistics studies used advanced quantitative
analysis methods compared to analysis studies. Nearly 80 percent of reported evaluation,
research, and analysis studies used quantitative analysis to support evidence building.
Compared with analysis studies, lower percentages of evaluation, research, and statistics
studies reported using advanced methods such as system modeling, inferential statistics,
time series modeling, and economic analysis. Exhibits 29 and 30 report common quantitative
analysis methods by evidence-building activity.
17 OMB expects agencies to draw on a full range of methodological approaches, including qualitative research,
ethnography, and other inclusive methodologies such as participatory, emancipatory, and community-based
research that are informed by social and behavioral sciences. Such methodologies use many of the advanced
analysis methods. See Evidence-Based Policymaking: Learning Agendas and Annual Evaluation Plans, M-21-27
(OMB, 2021)
34
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 29. Percentage of Studies Reporting Quantitative Analysis, by Evidence-
Building Activity
Evaluation
Research
Analysis
71%
50%
45%
35%
35%
25%
23%
16%
15% 14%
15%
12%
12%
6%
4%
Descriptive statistics
System modeling/
Inferential statistics
Visualization
Time series
simulation
Notes: research n=57; evaluation n=34; and analysis n=92. The most common methods across all reported studies
are shown. The following methods were less common across all studies though percentages varied by activity:
'economic analysis,' "network analysis,' data mining and analytics, 'multiple criteria decision analysis, text mining
and analytics,' "meta-analysis,' and "bibliometric or scientometric analysis,' and 'other.' Respondents could select all
that apply, so percentages do not add to 100%.
Study Inventory Survey. Q27e (research); Q52e (evaluation); and Q75e (analysis).
Exhibit 30. Percentage of Statistics Studies Reporting Statistical Methods
Statistical Methods Used
Descriptive statistics
59%
Quality control
30%
Method comparison
26%
Survey data
22%
Proportions
22%
Note: statistics n=27. The most common methods across reported studies are shown. Methods used in lower
percentages were 'operations research,' "correlation,' "two-way tables, 'regression, 'multivariate analysis,'
"forecasting,' 'design of experiments, "time series,' "meta-analysis,' 'distribution fitting,' "appraisal, and 'other'.
Respondents could select all that apply, so percentages do not add to 100%.
Source: Study Inventory Survey Q93 (statistics).
Using the Methods Inventory: Appropriateness of Evaluation Methods
The section illustrates, for evaluation, how summary information about study purpose, claims,
and methods may still be useful to the Department for considering limitations of current methods
used and identifying opportunities for improving future evidence-building.
35
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 31 provides a partial activity-specific snapshot of the evaluation methods inventory. The
exhibit summarizes characteristics of the FY 2021 reported evaluation studies, including focus,
purpose, and claims, as well as data sources, data collection methods, and data analysis that
respondents reported in the highest percentages.
Exhibit 31. Evaluation Methods Inventory
Focus, n=34
Purpose, n=32
Claim, n=33
Describe without
Program
68%
Both formative
making an
47%
and summative
inference
Policy
15%
36%
Summative
28%
Organization
15%
64%
Formative
25%
Make an
Regulation
3%
inference
Direct Data Collection
Direct Data Sources,
Existing Data Sources,
Methods, n=34
n=34
n=34
Policies
Quantitative
DHS
53%
68%
44%
survey
Component(s)
and plans
Qualitative
44%
Program customer
26%
Reports
35%
interview
or beneficiary
Qualitative
Administrative/
32%
SLTT agency
18%
32%
questionnaire
operational data
Performance
Observation
12%
Federal agency
15%
32%
measures
Log or journal
9%
Grant recipient
15%
Statistical data
21%
Qualitative Analysis
Quantitative Analysis
Overall Analysis
Methods, n=34
Methods, n=34
Methods, n=32
Content
41%
analysis
Descriptive
71%
statistics
Mixed methods
53%
Narrative
38%
Inferential
analysis
15%
statistics
Quantitative only
21%
Case study
21%
Economic analysis
15%
Qualitative only
21%
Thematic
15%
analysis
Time series
12%
"Other"
6%
Panel review
9%
Visualization
12%
Unspecified
Notes: evaluation n=34. For data collection, data sources, and analysis, the most common responses are shown.
Focus and Overall Methods do not add to 100% due to rounding. For data collection methods, data sources, and
analysis methods, respondents could select all that apply, so percentages do not add to 100%.
Source: Study Inventory Survey Q29, Q36a, Q38, and Q52b-e (evaluation).
36
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
The low percentages with which certain data and methods were used in evaluations may
limit the Department's ability to make credible inferences and fulfill summative purposes
at the levels reported, including quantifying effectiveness (outcomes and impacts), cost-
effectiveness, or equity that relates to or results from DHS mission delivery.
- Most reported evaluation studies were focused on programs (68 percent), had some
summative purposes (75 percent), and attempted to make an inference (64 percent). This
means most reported evaluations aimed to determine what goals, outcomes, or impacts
were achieved as a result of implementing a program and attempted to show
relationships between program activities and measurable outcomes.
-
Low percentages of reported evaluations collected data directly from program customers
or beneficiaries (26 percent) and other external stakeholders (18 percent SLTT agency, 15
percent each for federal agency and grant recipients), as compared with personnel from
DHS Components. Direct data collection from customers and external stakeholders
provides evaluation-specific data that is not available in administrative data, such as
experiences, barriers, unmet needs, outcomes, and adverse consequences of interacting
with DHS programs, to inform customer- and results-centric design and delivery.
- Low percentages of reported evaluations used advanced qualitative data analysis, such
as thematic analysis (15 percent), grounded theory, interpretative phenomenology, and
ethnography (all 0 percent, data not shown), despite 74 percent conducting some form
of qualitative analysis. Such methods enhance the rigor of interpretations of qualitative
data in relation to the theoretical frameworks guiding program designs (such as logic or
system models) and in relation to customers' lived experiences with and without the
program or the sociocultural contexts in which they receive program services.
- Although half of reported evaluations conducted some form of quantitative data
collection, low percentages of evaluations used advanced quantitative analysis methods,
such as inferential statistics (15 percent), time series modeling (12 percent), and
economic analysis (12 percent). These methods, combined with certain study designs,
enable determinations under what circumstances a sample of individuals represents the
broader program population or society; if and how much differences between two or
more groups, changes over time, or associations between variables can be attributed to
(or considered caused by) an implemented program when compared to its absence or
alternative programs; and whether it is the most cost-effective alternative.
Federal evaluation standards indicate evaluations should use the most advanced methods and
study designs appropriate to address evaluation questions, while balancing goals, scale, timeline,
feasibility, and available resources, to provide greater confidence in the results. Since the
Evidence Act Title 1 elevates evaluation as an essential mission function, the issues related to the
rigor and appropriateness of data and methods used in evaluation studies are important for
future capacity building and assessment activities.
37
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Independence
INDEPENDENCE:
Independence dimension addresses the question: To
Who conducted the study
what extent are the activities being carried out free from
Extent of modifications of the
bias and inappropriate influence?
study based on input from
individuals for whom there was a
To assess the independence of evidence-building
conflict of interest (COI) present
activities across DHS, the assessment team used the
Authorization to refuse to
structured study review protocols to analyze data
provided by Component representatives through the
disseminate identifiable data,
study inventory survey. The study inventory survey
choose when to release data and
asked respondents questions related to addressing
how, and choose how to
conflicts of interests and inappropriate influence in
maintain and store data
conducting and disseminating studies, consistent with
activity-specific principles for independence in federal guidance. The assessment team checked
whether the studies were conducted by researchers independent from the Department but did
not use these data in the maturity rating. Independence standards allow evidence building by
internal staff who operate with an appropriate level of independence from program,
regulatory, and policymaking activities, 18 so these findings are reported for context only.
Overview of Independence Findings
Independence Summary Finding: Most FY 2021 studies were performed by internal staff.
Information provided for studies suggests that the majority of evidence-building activities were
conducted free from inappropriate influence.
DHS received two maturity ratings of '4' (completed) and one maturity rating of
'3' (implementing) across the CLCs. DHS scores for three CLCs, each of which represents only
one evidence-building activity, appear in Exhibit 32. The 'Not rated' and gray shading indicates
that a CLC was assessed only on an activity-specific basis from an average of all respondents'
data who reported their study was conducting the given activity, so there are no crosscutting
ratings.
Activity
CLC Brief Description
Evaluation
Research
Analysis
Statistics
Crosscutting
Evaluation independence and objectivity
4
Not rated
Not rated
Not rated
Not rated
Research independence and objectivity
Not rated
4
Not rated
Not rated
Not rated
Statistical independence from political and
Not rated
Not rated
Not rated
3
Not rated
other undue influence
18 For example, see Phase 4 Implementation of the Foundations for Evidence-Based Policymaking Act of 2018:
Program Evaluation Standards and Practices, M-20-12 (OMB, 2020)
38
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
About one in four reported studies were conducted by researchers independent from DHS.
The majority of reported studies (74 percent) were conducted by internal staff in the
Component or in another DHS Component and 26 percent by independent researchers, or an
external firm, organization, or group. Exhibit 33 summarizes the percentage of studies
reporting who is conducting the study, whether internal or external researchers.
Exhibit 33. Percentage Studies Reporting Who Is Conducting Study, By Evidence-
Building Activity and Total
Internal
External
Evaluation
68%
32%
Research
86%
14%
Analysis
55%
45%
Statistics
85%
15%
Total
74%
26%
Notes: evaluation n=34; research n=56; analysis n=40; statistics n=27; and total n=157.
Source: Study Inventory Survey Q27 (research); Q52 (evaluation); Q75 (analysis); and Q92 (statistics).
Study inventory data suggested the majority of FY 2021 studies were conducted
independently and objectively, free from inappropriate influence, and the study teams
were looking to improve the independence and objectivity of evidence-building studies
moving forward. The extent to which evidence-building activities were conducted free from
inappropriate influence varied by activity type, with greater percentages of evaluation and
research reporting indicators of independence. Summarized previously in Exhibit 21, the
assessment team reported the following:
- A majority reported evaluation (76 percent) and research (93 percent) did not modify the
study based on input from individuals outside of the research team, or made
modifications based on input from individuals with no conflicts of interest.
-
Only 40 percent of statistical studies indicated at least two of the three criteria for
independence of the study team: authorized to refuse to disseminate identifiable data;
choose when to release data and how; and choose how to maintain and store data.
Respondents who reported modifying the design or results of studies (typically, evaluations)
based on outside input reported having done so to coordinate findings among other agencies
and stakeholders, to promote inclusivity and collaboration, or to solicit the feedback of subject-
matter experts to improve the rigor of the work. Such rationale are consistent with federal
standards for evaluation.
39
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Effectiveness
EFFECTIVENESS:
The Effectiveness dimension addresses the question:
Policies for stakeholder
Are the activities meeting their intended outcomes,
engagement in evidence building
including serving the needs of stakeholders and being
Mechanisms for assessing user
disseminated?
and stakeholder needs to inform
Effectiveness CLCs included whether policies for
evidence building
evidence-building activities outlined requirements for
Processes for integrating
stakeholder engagement plans and reporting, and
evidence
mechanisms were established to assess evidence
Continuous improvement or
consumer and user needs, input, and feedback to
learning cycle that uses evidence
inform evidence building. Effectiveness CLCs also
Policies and guidance for
included whether processes were implemented to
dissemination of evidence
integrate evidence in business processes, program
Mechanisms for dissemination to
activities, and organizational decision making. One CLC
internal and external
examined whether continuous improvement or
stakeholders, including the
learning cycle processes have been implemented to use
public
evidence for improvement. Effectiveness CLCs also
examined whether policies for evidence-building
activities outlined requirements for dissemination plans and authorities, and mechanisms were
established to disseminate evidence products to internal and external stakeholders, including the
public. This section presents findings from the discussion groups and accompanying document
review.
Although the assessment team checked whether the study findings were made available and to
whom, these data were not used in the maturity rating and are reported for context only.
Finally, the individual survey gathered respondents' perceptions of their Component context.
Component context, including Component culture, practices, and technology resources, can
affect capacity to effectively use evidence. These data were not used for any CLC maturity ratings
in the FY 2021 capacity assessment; however, these data may provide an important baseline
measure of the extent to which supportive organizational context exists for evidence use in day-
to-day operations.
Overview of Effectiveness Findings
Effectiveness Summary Finding: While DHS lacked policies that outlined requirements for
stakeholder engagement and dissemination, some Department-wide and Component
mechanisms existed to gather stakeholder needs, input, and feedback for evidence building, to
disseminate findings of evidence building, and to use evidence for decision making and
improvement. Some organizational contexts, including Component culture, practices, and
technology resources, exist that support evidence building and use in day-to-day operations.
40
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
DHS received a range of maturity scores between '1' (not initiated) and '3' (implementing)
across the Effectiveness CLCs. Analysis was more mature than other activities across most
CLCs. Ratings for the seven CLCs, by evidence-building activity (per CLC and overall) and
crosscutting (across all evidence-building activities), appear in Exhibit 34. Effectiveness CLCs
(average) ratings reflected the average maturity scores for the rated activity specific CLCs.
Exhibit 34. Effectiveness Maturity Ratings, by Evidence-Building Activity
Activity
CLC Brief Description
Evaluation
Research
Analysis
Statistics
Crosscutting
Policy outlines requirements for evidence-
related stakeholder engagement and
1
1
1
1
1
compliance is regularly monitored
Mechanisms have been implemented to
assess evidence-user needs, input, and
2
2
2
2
2
feedback; input and feedback informs
evidence-building
Processes have been implemented to
integrate evidence into business
2
2
3
3
3
processes, program activities, and
organizational decision-making
Continuous improvement processes or
learning cycle processes have been
2
1
3
2
2
implemented to use evidence for
improvement
Policy outlines requirements for evidence
dissemination and compliance is regularly
1
1
3
1
2
monitored
Mechanisms exist for effectively
disseminating evidence products to
2
2
3
3
3
internal and external stakeholders
Mechanisms have been implemented for
1
2
3
2
2
timely public release of evidence products
Effectiveness CLCs (average)
2
2
3
2
2
Overall, DHS lacked policies or procedures that outlined requirements for engaging a broad
array of individual and external stakeholders in evidence building activities. DHS
implemented and monitored compliance with Department-wide guidance for regulatory
analysis (one of multiple analysis subtypes) stakeholder engagement and reporting. While
the Department-wide instruction for evaluation also provided guidance relating to
stakeholder engagement, compliance was not yet monitored across the Department. No
Component-wide policies or procedures were in place that outlined requirements for
stakeholder engagement plans and reporting in evidence-building.
41
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
DHS has implemented some mechanisms to assess evidence user needs, but these
mechanisms were not used consistently to plan evidence-building activities or to inform
programs, policies, regulations, or organizations. Nine Components reported some
mechanisms were in place to assess evidence users' or consumers' needs and seek input and
feedback from users or consumers (or both) for at least one type of evidence-building activity;
however, these mechanisms were used irregularly to plan evidence-building activities or to
inform programs, policies, regulations, and organizations. Of the nine Components that
reported having some mechanisms in place to assess evidence consumers' needs, eight had
mechanisms pertaining to analysis and four Components had mechanisms in place specifically
related to one or more of their evaluation, research, and statistics activities.
Leaders and staff across the Department somewhat consistently used evidence in business
processes, program activities, and organizational decision making, with strengths in the use
of regulatory analysis and performance measurement information. Seven Components
provided information that informed a maturity score of 3 (Implementing) or higher. Other
Components noted that products derived from evidence-building activities were occasionally
used to inform ongoing processes, program activities, and organizational policies, practices,
and decision making but that the use of such data was inconsistent. In some of these cases,
the process for integrating evidence into their activities was in a "nascent stage."
Components reported having a more robust process for integrating some evidence but not
for all four activities. According to Component representatives who participated in discussion
groups, evidence from analysis and statistics was used more regularly than other evidence
types.
DHS established performance measurement processes that serve as continuous
improvement or learning cycle processes; although, most Components had not
implemented their own continuous improvement processes to focus on Component-
specific improvements. DHS reported well-established strategic review process through
which the Department synthesized available evidence to assess program progress in
achieving DHS's priority goals. The strategic review process used performance measurement
information; however, the assessment team could not determine whether and how other
types of evidence, including DHS-sponsored evaluation, research, statistics, or other forms of
analysis, informed the strategic review process. Operational and some support Components
participated in these efforts, but some support Components did not participate. Most
Components did not have equivalent formal continuous learning processes to identify
promising practices, problem areas, causal factors, or areas for improvement specific to their
missions. However, several Components reported informal processes or activities used for
other purposes that also helped to support this objective.
42
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
DHS had Department-wide policies and guidance that outline requirements and authorities
for dissemination of regulatory analysis and performance measurement products, as well
as some policy and guidance in place for evaluation and research for which compliance was
not monitored. Components generally did not have equivalent policies. DHS had
department-wide policies and guidance for research (scientific integrity), evaluation, and
certain analysis activities (e.g., regulatory analysis and performance measurement), certain
aspects of which are relevant to dissemination. Components were required to adhere to
these policies and either participate in or feed equivalent activities into HQ-coordinated
activities. No DHS Component reported having in place Component-wide policies or
procedures that outlined requirements and authorities for evidence dissemination plans.
However, two Components reported that they were in the process of developing such policies
and procedures for one or more evidence-building activities.
DHS had some mechanisms in place to disseminate evidence products and findings to
internal and external stakeholders, but mechanisms were not in place across all
Components for all evidence-building activities. DHS representatives consistently reported
that DHS follows all applicable statutory and federal guidance pertaining to regulatory
analysis, which requires certain regulatory analyses, including methods, findings, and data,
be disseminated to the public so that interested parties can replicate analyses. Similarly, DHS
followed federal guidance relating to performance measurement, annually publishing
performance and accountability reports, including the DHS annual financial report, annual
performance report, and the DHS summary of performance and financial information on
DHS's public website. All Components participating in discussion groups reported either that
they had some mechanisms in place for at least one activity or that they were in the process
of developing such mechanisms for at least one activity. Components provided examples of
internal and external mechanisms for dissemination, including issuing information on internal
and public websites, blogs, social media, white papers, and dashboards.
Although DHS had some mechanisms in place to publicly release completed products from
certain evidence-building activities, mechanisms were not in place across all Components
and for all types of evidence-building activities. DHS had established Department-wide
practices for public dissemination of regulatory analysis and performance measurement
products and findings. Half of the DHS Components that participated in the FY 2021 capacity
assessment did not have any mechanisms in place for publicly releasing completed products
from evidence-building activities in a timely manner. For the seven Components that
reported having mechanisms in place, those mechanisms were somewhat mature though
maturity varied by evidence-building activity.
43
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
About one in three reported evaluation, research, and analysis studies were disseminated
to the public. Across all four evidence-building activities, most reported studies (69 percent)
were made available to restricted stakeholder groups that include one or more members of
the analysis team, internal (DHS) stakeholders, and external stakeholders. External
stakeholders often included federal and SLTT agency partners, relevant members of private
industry, and those conducting oversight (e.g., Congress, OMB, Government Accountability
Office). Only about one third of all reported studies (31 percent) released all or some portion
of study findings, methods, and deidentified data to the public. Exhibit 35 presents a
summary of the reported release of studies.
Exhibit 35. Percentage Studies Reporting Release of Study Information, By
Evidence-Building Activity and Total
Restricted Release
Public Release
Evaluation
68%
32%
Research
69%
31%
Analysis
70%
30%
Total
69%
31%
Notes: evaluation n=34; research n=55; analysis n=40; and total n=129. 'Public Release' combines three response
options. 'Restricted Release' combines three responses.
Source: Study Inventory Survey. Q17 (research); Q42 (evaluation); and Q70 (analysis).
Some organizational contexts, including Component culture, practices, and technology
resources, already existed to support evidence production and use in day-to-day
operations, but most respondents reported their Component should not be considered a
role model for evidence use. For 12 of the 14 items used to assess Component context, fewer
than 70% of respondents agreed or strongly agreed their Component had these
characteristics. Most respondents reported that Components valued and rewarded
continuous improvement (76 percent) and encouraged information exchange across the
organization (72 percent). Fewer respondents perceived that Components made it clear what
staff should be doing with evidence (43 percent), had IT systems to efficiently examine data
(46 percent), allowed enough time to integrate evidence in major decision making (45
percent), or prioritized evidence in decision making (57 percent). Exhibit 36 summarizes the
levels of agreement among individual survey respondents with statements about their
Component context. Appendix D provides the full set of items and responses.
44
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 36. Percentage of Respondents Reporting 'Agree' or 'Strongly Agree' on
Component Context
My operational or support Component
Values and rewards flexibility and continuous quality
76%
improvement
Modifies its course of action based on evidence
64%
Has prioritized using evidence for decisionmaking
57%
Encourages internal communication to ensure there is
72%
information exchanged across the entire organization
Appropriately disseminates products and findings to
66%
internal and external stakeholders
Uses evidence to support decisionmaking on program
65%
and policy options, operations, and strategies
Allows enough time to create/obtain, analyze, and
consider research results and other evidence when
45%
making major decisions
Has IT systems and tools to generate displays (e.g.,
63%
reports, tables, charts) that are useful to my work
Has IT systems in place to efficiently examine data
46%
Involves staff responsible for providing evidence in
62%
decision-making discussions
Acts on personnel suggestions for using or improving
61%
the use of evidence
Has articulated how using evidence fits with agency
60%
goals
Makes it clear to personnel what they should be doing
43%
with evidence
Should be considered a role model for evidence use by
40%
other Federal agencies
Note: n=187-191.
Source: Individual Survey Q37-Q50.
While most of the capacity assessment focuses on infrastructure for evidence building, these
data provide important perspective on aspects of organizational culture and practices that can
be targeted in future capacity building efforts to facilitate evidence building and use.
45
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Opportunities for Improvement
Across all four activities, the assessment team found that capacity for evidence-building and use
is uneven across DHS Components and nascent but developing in some places. Drawing on these
findings, the assessment team presented recommendations that DHS could adopt to strengthen
its evidence-building enterprise.
Establish a strong foundation for evidence building and use. DHS capacity building efforts should
continue to (1) foster shared understanding of evidence-building through common terminology
and concepts consistent with the Evidence Act and OMB guidance to which DHS is accountable;
(2) deepen Component-level understanding of personnel, funding, and other infrastructure
available for evidence-building as well as evidence-building activities that are underway within
the Department; and (3) promote the importance of evidence building and use.
Assess the Department's evidence needs and align evidence-building efforts and resources
with those needs. Beyond a Department-wide learning agenda, developing Component-specific
learning agendas and evaluation plans would enable DHS to comprehensively assess
Components' short and long-term evidence needs, measure Component capacity against those
needs, and appropriately allocate resources to build evidence while targeting gaps in capacity.
Build and sustain evidence building capabilities at the Component level and DHS-wide. To
ensure that evidence building adheres to principles of scientific integrity and rigor, DHS needs to
hire personnel with specialized expertise for evidence building, provide training and time to
develop staff, and establish sufficient budgets to support evidence building activities and procure
external researchers for evidence building. DHS should empower certain evidence-building staff
(or external researchers they manage) with sufficient independence and autonomy to design,
conduct, and appropriately disseminate findings, methods, and data from evidence building.
Aim for continuous improvement of the evidence-building enterprise. DHS should implement
and monitor compliance with existing plans and policies for evidence building and use, and
establish new plans, policies, and mechanisms where needed. Department-wide, evidence
building teams should make plans to improve their efforts, targeting areas identified for
improvement in this assessment.
Promote the dissemination and use of evidence. DHS should consult evidence users and a broad
range of stakeholders to inform evidence-building policies, plans, and activities so they are
relevant and useful. Existing continuous improvement processes should leverage all types of
evidence. DHS and Components should establish new or formalize existing practices to promote
more-consistent use of evidence in programs, policymaking, and business processes. Improving
existing dissemination mechanisms to enable timely release of products and findings to the
broadest audiences possible, including publicly releasing evidence, is a necessary precursor for
ensuring its use in learning, improvement, and accountability to the American public.
46
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Appendix A. Abbreviations and Acronyms
CBP
U.S. Customs and Border Protection
LA
learning agenda
CISA
Cybersecurity and Infrastructure
MGMT Management Directorate
Security Agency
n
number of respondents (for individual
CLC
component-level capability
survey) or studies (for study inventory
survey)
COI
conflict of interest
N/A
not applicable
CWMD Countering Weapons of Mass
Destruction Office
OMB
Office of Management and Budget
DHS
U.S. Department of Homeland Security
OPM
Office of Personnel Management
EBS
evidence-building staff
PA&E
Program Analysis and Evaluation
Division
EP
evaluation plan
PLCY
Office of Strategy, Policy, and Plans
FEMA Federal Emergency Management
Agency
PP
performance plan
FFRDC federally funded research and
SLTT
state, local, tribal, and territorial
development center
SOP
standard operating procedure
FLETC Federal Law Enforcement Training
S&T
Science and Technology Directorate
Centers
TSA
Transportation Security Administration
FY
fiscal year
USCG U.S. Coast Guard
HQ
headquarters
USCIS U.S. Citizenship and Immigration
HSOAC Homeland Security Operational Analysis
Services
Center
USSS
U.S. Secret Service
I&A
Office of Intelligence and Analysis
ICE
U.S. Immigration and Customs
Enforcement
47
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Appendix B: Addressing Evidence Act and OMB Requirements
Evidence Act Overview
The Foundations for Evidence-Based Policymaking Act of 2018 ("Evidence Act"), 19 signed into
law on January 14, 2019, aims to improve the availability and use of evidence to make critical
decisions about government strategy and operations. The Evidence Act emphasizes
collaboration and coordination in developing and using evidence, better use of existing federal
data in evidence-building activities, and open government data. It calls upon the Department to
integrate evidence building into routine practices and policies and to institutionalize using
evidence for organizational learning, continuous improvement, and decision-making. Coupling
better use of evidence building activities with the Administration and Department priorities,
through the strategic plan and other transformative initiatives, will further advance the
Department as a learning organization.
The Evidence Act requires agencies to conduct assessments of their capacity to build and use
evidence, concurrent with the renewal of the strategic plan. The assessment team designed the
capacity assessment to help the Department meet statutory requirements and OMB's intent
that the assessment
"...provide agencies with a baseline against which agencies can measure
improvements to the coverage, quality, methods, effectiveness, and
independence of statistics, evaluation, research, and analyses The Capacity
Assessment will provide senior officials with information needed to fulfill the
Evidence Act's intent to improve the agency's ability to support the
development and use of evaluation, coordinate and increase technical expertise
available for evaluation and related research activities within the agency, and
improve the quality of evaluations and knowledge of evaluation methodology
and standards."20
Collectively, these efforts can improve how the DHS builds and uses evidence, and can better
align management functions, including strategic planning, program and performance
management, resource management, policymaking, data, and evidence-building activities
across the enterprise.
Evidence Act and OMB Requirements
The DHS FY 2021 Capacity Assessment provides information about the extent to which the
Department has the capacity to undertake the activities outlined in the learning agenda and
other evidence-building activities. Exhibit 37 lists Evidence Act and OMB requirements for the
capacity assessment and demonstrates how they have been operationalized for the DHS
capacity assessment.
19
Pub. L. No. 115-435, 132 Stat. 5529 (2019)
20 Phase 1 Implementation of the Foundations for Evidence-Based Policymaking Act of 2018: Learning Agendas,
Personnel, and Planning Guidance, M-19-23 (OMB, 2019)
48
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Exhibit 37. Crosswalk of Requirements and DHS Capacity Assessment
Evidence Act
Evidence Act
OMB Circular
Operationalization in the
Dimensions
Elements
A-11
DHS FY2021 Capacity Assessment
Coverage
(A) a list of the activities and operations of What is
Organizational and individual capacity to
the agency that are currently being
happening
plan and conduct evidence building:
evaluated and analyzed
and where is
staff with expertise and training to
it happening?
plan and conduct evidence building
(B) the extent to which the evaluations,
staff with expertise and training to
research, and analysis efforts and related
procure and manage external
activities of the agency support the needs
organizations for evidence building
of various divisions within the agency
staff with supports necessary to
(C) the extent to which the evaluation
conduct evidence building
research and analysis efforts and related
funding and investment in evidence
activities of the agency address an
building
appropriate balance between needs
policies that outline procedures for
related to organizational learning, ongoing
planning and conducting evidence
program management, performance
building
management, strategic management,
interagency and private sector
Data collected but not rated for maturity:
coordination, internal and external
oversight, and accountability
Inventory of FY 2021 evidence-building
activities (studies) attached as Appendix C
Extent to which FY 2021 studies address
(E) the extent to which evaluation and
research capacity is present within the
DHS strategic objectives
agency to include personnel and agency
Component mission objectives
processes for planning and implementing
Range of foci (e.g., operations,
evaluation activities
programs, activities, strategies,
policies, regulations, phenomena)
equity
Quality
Are the data
Organizational capacity to adhere to activity
used of high
specific quality standards and best practices
quality with
in planning and conduct of evidence building
respect to
to ensure information quality:
utility,
policy with quality standards and
objectivity,
compliance monitored
and integrity?
Presence of evidence plans and adherence
to statutory and OMB requirements and
guidance:
Learning Agenda (LA) in OMB M-
19-23
Annual Evaluation Plan (EP) in
OMB M-19-23
Annual Performance Plan (PP) in
OMB Circular A-11
Extent to which FY 2021 studies adhere to
activity-specific quality standards and best
practices:
evaluation and research in OMB M-
20-12
analysis in OMB Circular A-4
statistics in OMB Statistical Policy
Directive #1 and OMB 14-06
49
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Evidence Act
Evidence Act
OMB Circular
Operationalization in
Dimensions
Elements
A-11
DHS Capacity Assessment
Methods
(D)the extent to which the agency uses
What are the
Extent to which FY 2021 studies
methods and combinations of methods
methods being
address formative and/or
that are appropriate to agency divisions
used for these
summative purposes
and the corresponding research questions activities? Do
collect data from direct data
being addressed, including an appropriate these methods
combination of formative and summative
incorporate the
sources (individuals)
evaluation research and analysis
necessary
use existing data sources
approaches
level of rigor?
use data collection methods
Are those
methods
use qualitative data analysis
methods
appropriate for
the activities to
use quantitative data analysis or
which they are
statistical methods
being applied?
make certain claims and
judgements
Independence
Independence:
Extent to which FY 2021 studies adhere to
to what extent
activity-specific independence standards
are the
and best practices:
activities being
evaluation and research in OMB
carried out free
M-20-12
from bias and
inappropriate
statistics in OMB Statistical Policy
influence?
Directive #1
Data collected but not rated for maturity:
Extent to which FY 2021 studies are
conducted by external researchers.
Effectiveness
(E) the extent to which evaluation and
Are the
Organizational capacity to address
research capacity is present within the
activities
stakeholder needs, disseminate and use
agency to include disseminating best
meeting their
evidence building:
practices and findings, and incorporating
intended
policies and guidance for
employee views and feedback
outcomes,
stakeholder engagement plans
including
serving the
mechanisms for assessing user
(F) the extent to which the agency has the
needs of
and stakeholder needs, input, and
capacity to assist agency staff and
feedback to inform evidence
stakeholders
program offices to develop the capacity to
use evaluation research and analysis
and being
building
disseminated?
processes for integrating
approaches and data in the day-to-day
evidence in decision making
operations
continuous improvement or
learning cycle process that use
evidence
policies and guidance for
dissemination plans
mechanisms for disseminating
evidence to internal and external
stakeholders, including the public
Data collected but not rated for maturity:
Extent to which FY 2021 study information
is disseminated to the public
Organizational context that affects capacity
to effectively use evidence (e.g., vision,
collaboration, systems, communication,
decision making, and improvement)
50
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Evidence-Building Activities
The Evidence Act states that evidence comes from evaluation, research, analysis, and statistics.
OMB provided Agencies with guidance and information to support their interpretation these
broad terms, to include program evaluation, performance measurement, policy analysis, and
research or statistics conducted for foundational fact finding as well as other "activity subtypes"
suggested by OMB's descriptions of those activities. Drawing from the Evidence Act's definition
of evidence and all available OMB guidance, PA&E developed a list of evaluation, research,
analysis, and statistical activities subtypes that should be included or excluded from this
assessment. The in-scope activity subtypes are listed and described in Exhibit 38.
Exhibit 38. Assessed Evidence-Building Activities and Activity Subtypes
Evidence-
Building
Activity Subtypes
Activity
Evaluation of programs, policies, regulations, or organization evaluation (also termed program
evaluation), including formative, process and implementation, outcome, impact, and economic
Evaluation
evaluation
Project evaluation of ongoing or completed grantee projects required by DHS grantmaking programs
Basic research: experimental or theoretical work undertaken primarily to acquire new knowledge of the
underlying foundations of phenomena and observable facts
Applied research: original investigation undertaken in order to acquire new knowledge, directed
primarily toward a specific practical aim or objective
Research
Foundational research: describes and documents programs, policies, services, or interventions
currently implemented in the field or eligible and impacted populations and their characteristics
Exploratory research: examines correlational relationships between program- or policy-relevant
constructs to identify logical connections that could form the basis for future programs, policies,
services, or interventions or frameworks to measure their results
Policy and regulatory analysis, typically using cost-benefit and cost-effectiveness analysis
Operations research: the development of mathematical models, statistical analyses, simulations, and
analytical reasoning to understand and improve real-world operations
Analysis
Performance measurement and monitoring: ongoing and systematic tracking of data and information
relevant to policies, strategies, programs, projects, or activities, which can include indicators for context,
inputs, process, efficiency, outputs, intermediate outcomes, and outcomes
Statistical activities: The use of data to describe outcomes and descriptors of interest, such as through
Statistics21
estimates of population characteristics, summaries of test results, indices of economic activity,
measures of environmental conditions, and incidence rates for a wide variety of events
Notes: The Evidence Act and OMB guidance do not consider use of data for non-statistical purposes (any
administrative, regulatory, law enforcement adjudicatory, or other purpose that affects the rights, privileges, or
benefits of an identifiable respondent) evidence. Notable other activities that were excluded from this capacity
assessment, informed by OMB guidance, were (1) Audits, inspections, and evaluations conducted by the DHS
Inspector General, Government Accountability Office and other external auditors; and (2) Experimental development
that is directed at the production or improvement of materials, devices, and systems or methods, including the
design, construction and testing of experimental prototypes and technology demonstrations.
21 DHS does not have any designated statistical agencies or activities that are required to comply with the
Confidential Information Protection and Statistical Efficiency Act of 2018 (CIPSEA).
51
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Appendix C. Evidence-Building Activities and Operations Evaluated or
Analyzed
This list includes selected evaluation, research, analysis, and statistics activities that were
conducted, commissioned, or sponsored by DHS since the passage of the Evidence Act that serve
as recent evidence assets for decision making. This list includes a selection of studies identified
in the assessment team's inventory of FY 2021 studies, the DHS annual evaluation plans (FY 2022
and FY 2023), and the Unified Regulatory Agenda. The list also includes studies and analysis
identified by PA&E through a search of DHS, Component, and DHS-sponsored entities' websites.
The list is not intended as an exhaustive list; rather it serves to illustrate a variety of evidence
products (formal reports, analytic documents, statistical summaries, and interactive data
products) that provide decisionmakers and the public with evidence about the Department's
activities, operations, and the contexts in which they occur. In addition, nearly 3,000 DHS data
sets are also available at data.gov.
'In progress' studies included in this list were reported in the study inventory survey-
accompanied by a scoping document or study plan and indicating that public release was
expected- - or were identified as "significant"22 evaluations or analyses from the DHS annual
evaluation plans and the Unified Regulatory Agenda.
Exhibit 39. List of Selected DHS Evidence-Building Activities
Study/Report Title
Year
Evidence
Organization
Strategic Objective 1.2 Detect and Disrupt Threats
DHS Targeted Violence and Terrorism Prevention
In Progress
Evaluation
S&T
Grant Program Evaluation
Office for Targeted Violence and Terrorism Prevention
2021
Evaluation
S&T
(OTVTP) FY 2016 Grant Evaluations
Evaluation of a Targeted Violence Prevention Program
2021
Evaluation
S&T
in Los Angeles County, California
Multiagency Programs with Police as a Partner for
2021
Research
S&T
Reducing Radicalisation to Violence
The Use of a Scenario-Based Nominal Group
Technique to Assess P/CVE Programs: Development
2021
Research
S&T
and Pilot Testing of a Toolkit
Cognitive and behavioral radicalization: A systematic
2021
Research
S&T
review of the putative risk and protective factors
Counter-narratives for the prevention of violent
radicalization: a systematic review of targeted
2021
Research
S&T
interventions
Police programs that seek to increase community
connectedness for reducing violent extremism
2020
Research
S&T
behavior, attitudes and beliefs
22 For the annual evaluation plans, criteria for "significant" evaluation include addressing learning agenda priorities
or responding to mandates for evaluation from Congress, OMB, Government Accountability Office, or Inspector
General, and other criteria. At the recommendation of the DHS Chief Economist, "significant" analysis includes
economically significant rulemaking for which an economic impact analysis is required.
52
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study/Report Title
Year
Evidence
Organization
Practical Terrorism Prevention: Reexamining U.S.
National Approaches to Addressing the Threat of
2019
Research
PLCY
Ideologically Motivated Violence
Leveraging a Targeted Violence Prevention Program
to Prevent Violent Extremism: A Formative Evaluation
2018
Evaluation
S&T
in Los Angeles
Countering Violent Extremism: The Application of Risk
Assessment Tools in the Criminal Justice and
2018
Research
S&T
Rehabilitation Process (Literature Review)
Strategic Objective 1.4 Counter Weapons of Mass Destruction and Emerging Threats
Jack Rabbit (JR): Large Scale Ammonia Release
In progress
Research
S&T
Response
Master Question List for African Swine Fever Virus
2021
Research
S&T
Monthly Report
Master Question List for COVID-19 (caused by SARS-
2021
Research
S&T
CoV-2) Monthly Report
Supplemental Reference for SARS-CoV-2 Delta
2021
Research
S&T
Variant
Estimated Surface Decay of SARS-CoV-2 (virus that
2021
Research
S&T
causes COVID-19) Calculator
Estimated Airborne Decay of SARS-CoV-2 Calculator
2021
Research
S&T
COVID-19 Vulnerability by Immigration Status: Status-
2021
Statistics
PLCY
Specific Risk Factors and Demographic Profiles
COVID-19 Vaccine Early Skepticism: Misinformation
and Informational Needs among Essentials Works in
2021
Statistics
S&T
the USA
COVID-19 Vaccine Concerns about Safety,
Effectiveness and Policies in the United States,
2021
Statistics
S&T
Canada, Sweden, and Italy among Unvaccinated
Individuals
Disinfection and Reuse of Personal Protective
2020
Research
S&T
Equipment
Evaluation of Disinfectant Efficacy Against SARS-
2020
Research
S&T
CoV-2
Applying Lessons Learned from COVID-19 Response
to a Future High-Consequence Food or Agriculture
2020
Research
CWMD
Incident
Small Unmanned Aerial System Adversary
2020
Research
S&T
Capabilities
Regulatory Assessment and Initial Regulatory
Flexibility Analysis for the Interim Final Rule:
2018
Analysis
CBP
Air Cargo Advance Screening (ACAS) Rule
Strategic Objective 2.1 Secure and Manage Air, Land, and Maritime Borders
Tactical Mapping of Border Security Impacts: El Paso
S&T
2021
Research
Sector
Mexican and Northern Triangle Perspectives on Mass
S&T
Migration: Identifying and Assessing Strategic
2021
Research
Narrative Alignment
Southwest Land Border Encounters
2021
Statistics
CBP
Nationwide Encounters
2021
Statistics
CBP
Assaults and Use of Force Statistics Fiscal Year 2021
2021
Statistics
CBP
to Date
53
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study/Report Title
Year
Evidence
Organization
2021
CBP Enforcement Statistics
2020
Statistics
CBP
2019
2018
2021
Custody and Transfer Statistics
Statistics
CBP
2020
Migrant Protection Protocols
2021
Statistics
CBP
2020
Modeling the Impact of Border-Enforcement Measures
2020
Research
S&T
Collection of Biometric Data From Aliens Upon entry
To and Exit From the United States Initial Regulatory
2020
Analysis
CBP
Impact Analysis
2020
Border Security Metrics Report
2019
Statistics
PLCY
2018
2020
Nonimmigrant Admissions to the United States
2019
Statistics
PLCY
2018
July 2020
June 2020
May 2020
April 2020
March 2020
Family Unit Actions Reports
Statistics
PLCY
Feb 2020
Jan 2020
Dec 2019
Nov 2019
Oct 2019
Characterization of the Synthetic Opioid Threat Profile
2019
Research
S&T
to Inform Inspection and Detection Solutions
Strategic Objective 2.2 Extend the Reach of U.S. Border Security
Modeling Push-and-Pull Factors in Cross-Border
S&T
In progress
Research
Migration with Deep Learning
The Road Less Traveled: Bolstering the Absorptive
Capacity of Southern Central American States to
S&T
2021
Research
Facilitate the Southern Flow of Northern Triangle
Immigrants
Laying the Foundation for Regional Cooperation:
Developing a Regional Approach to Managing
S&T
2021
Research
Migration Flows from the Northern Triangle through
Mexico to the United States
Human Smuggling and Associated Revenues: What
Do or Can We Know About Routes from Central
2019
Research
S&T
America to the United States
Strategic Objective 2.3 Enforce U.S. Immigration Laws
Alternatives to Detention Program Evaluation
In progress
Evaluation
ICE
Estimates of the Unauthorized Immigrant Population
2021
Statistics
PLCY
Residing in the United States
2018
2020
U.S. Immigration and Customs Enforcement Fiscal
2019
Statistics
ICE
Year Enforcement and Removal Operations Report
2018
54
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study/Report Title
Year
Evidence
Organization
2020
Immigration Enforcement Actions: Annual Flow Report 2019
Statistics
PLCY
2018
Student and Exchange Visitor Information System
2020
(SEVIS) by the Numbers: Annual Report on
2019
Statistics
ICE
International Student Trends
2018
Strategic Objective 2.4 Administer Immigration Benefits to Advance the Security and Prosperity of the Nation
Refugee, Asylum, and International Operations (RAIO)
Foundations Training Program Pilot Evaluation
In progress
Evaluation
USCIS
Rescission of "Asylum Application, Interview, &
Employment Authorization" Rule and Change to
"Removal of 30-Day Processing Provision for Asylum
In progress
Analysis
USCIS
Applicant Related Form I-765 Employment
Authorization" Regulatory Impact Analysis
Modernizing H-1B Requirements and Oversight and
Providing Flexibility in the F-1 Program Regulatory
In progress
Analysis
USCIS
Impact Analysis
Deferred Action for Childhood Arrivals Final Rule
In progress
Regulatory Impact Analysis
Analysis
USCIS
Procedures for Credible Fear Screening and
Consideration of Asylum, Withholding of Removal and
Cat Protection Claims by Asylum Officers Final
In progress
Analysis
USCIS
Regulatory Impact Analysis
Deferred Action for Childhood Arrivals NPRM
2021
Analysis
USCIS
Regulatory Impact Analysis
Procedures for Credible Fear Screening and
Consideration of Asylum, Withholding of Removal and
2021
Analysis
USCIS
Cat Protection Claims by Asylum Officers Regulatory
Impact Analysis
Trends in Naturalization Rates: FY2018 Update
2021
Statistics
USCIS
Characteristics of People Who Naturalized Between
2021
Statistics
USCIS
FY2015 and FY2019
Monitoring EB-5 Program Changes on Form I-526
2021
Statistics
USCIS
Receipts
Historical National Median Processing Time (in
Months) for All USCIS Offices for Select Forms by
2021
Statistics
USCIS
Fiscal Year
H-2A Nonimmigrant Temporary Agricultural Worker
2021
Statistics
USCIS
Trends Report: FY 2017 - 2020
2021
Report on H-1B Petitions: Fiscal Year Annual Report
2019
Statistics
USCIS
to Congress
2018
2021
Characteristics of H-1B Specialty Occupation Workers: 2020
Statistics
USCIS
Fiscal year Annual Report to Congress
2019
2018
2021
2020
Characteristics of H-2B Nonagricultural Temporary
2019
Statistics
USCIS
Workers: Fiscal Year Report to Congress
2018
Impact of the Homeland Security Act on Immigration
2021
Functions Transferred to the Department of Homeland
2020
Statistics
USCIS
Security
2019
55
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study/Report Title
Year
Evidence
Organization
2018
Fiscal Year Appropriations Reporting Requirement
2021
Statistics
USCIS
Refugee Data
2020
Estimates of the Lawful Permanent Resident
2021
Statistics
PLCY
Population Residing in the United States
2019
Temporary Protected Status: Calendar Year Annual
2021
Statistics
USCIS
Report
2020
Annual Report on Immigration Applications and
2021
Statistics
USCIS
Petitions Made by Victims of Abuse
2020
U VISA Report: U VISA Demographics
2020
Research
USCIS
Arrest Histories of U Visa Petitioners
2020
Research
USCIS
Trends in U Visa Law Enforcement Certifications,
2020
Research
USCIS
Qualifying Crimes, and Evidence of Helpfulness
U VISA Report: U Visa Filing Trends Report
2020
Research
USCIS
U VISA Technical Appendix
2020
Research
USCIS
E-Verify Customer Satisfaction Survey
2020
Statistics
USCIS
Annual Report on the Use of Special Immigrant Status
2020
Statistics
USCIS
for Citizens or Nationals of Afghanistan or Irag
2020
U.S. Lawful Permanent Residents: Annual Flow
2019
Statistics
PLCY
Report
2018
2020
Refugees and Asylees: Annual Flow Report
2019
Statistics
PLCY
2018
2020
U.S. Naturalization: Annual Flow Report
2019
Statistics
PLCY
2018
2020
USCIS Statistical Annual Report Final
Statistics
USCIS
2018
H-2B Nonagricultural Temporary Worker Visa and
2020
Statistics
USCIS
Status: Semiannual Report to Congress
2018
Registration Requirement for Petitioners Seeking to
File H-1B Petitions on Behalf of Cap-Subject Aliens
2019
Analysis
USCIS
Final Regulatory Impact Analysis
H-1B Authorized to Work Population Estimate Report
2019
Statistics
USCIS
F-1 Students obtaining Another Nonimmigrant
2019
Statistics
USCIS
Classification: FY 2008-2018 Approvals
DACA Requestors with an IDENT Response:
2019
Statistics
USCIS
November 2019 Update
2019
Yearbook of Immigration Statistics
Statistics
USICS
2018
Strategic Objective 3.1 Secure Federal Civilian Networks
High Value Asset (HVA) Program Evaluation
In progress
Evaluation
CISA
56
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study/Report Title
Year
Evidence
Organization
High Value Asset (HVA) Program Evaluation
Strategic Objective 3.2 Strengthen the Security and
Resilience of Critical Infrastructure
In progress
Evaluation
CISA
All-Hazards Communications Unit Position-Specific
Training and Stakeholder Communication Unit
In progress
Evaluation
CISA
Program
CISA Exercises Program Evaluation
In progress
Evaluation
CISA
CISA Stakeholder Engagement Division Critical
Infrastructure Partnership Advisory Council (CIPAC)
In progress
Evaluation
CISA
National Convening Activities Evaluation
Chemical Facility Anti-Terrorism Standards (CFATS)
2021
Statistics
CISA
Monthly Statistics
Building a More Resilient ICT Supply Chain: Lessons
2020
Research
CISA
Learned During the COVID-19 Pandemic
Retrospective Analysis of the 2007 Chemical Facility
2020
Analysis
CISA
Anti-Terrorism Standards
Strategic Objective 3.3 Assess and Counter Evolving Cybersecurity Risks
Analyzing a More Resilient National Positioning,
2021
Analysis
CISA
Navigation, and Timing Capacity
Cost of a Cyber Incident: Systematic Review and
2020
Research
CISA
Cross-Validation
Assessment of the Cyber Insurance Market
2018
Research
CISA
Maritime Cybersecurity Project
2018
Research
S&T
Strategic Objective 4.1 Enforce U.S. Trade Laws and Facilitate Lawful Trade and Travel
Assessment of Customs-Trade Partnership Against
2021
Evaluation
S&T
Terrorism (CTPAT) Program
Addressing Cross Border E-Commerce Challenges
2021
Research
S&T
with Emerging Technology
Mandatory Advance Electronic Data (AED) for
International Mail Shipment IFR Regulatory Impact
2021
Analysis
CPB
Analysis
Mandatory Advance Electronic Data (AED) for
International Mail Shipment Final Rule Regulatory
In progress
Analysis
CPB
Impact Analysis
Trade Statistics
2021
Statistics
CBP
2020
CBP Trade and Travel Fiscal Year Report
2019
Statistics
CBP
2018
Strategic Objective 4.2 Safeguard the U.S. Transportation System
TSA Checkpoint Travel Numbers (Current Year
2021
Statistics
TSA
Versus Prior Year(s)/Same Weekday)
The Risk Mitigation Value of the Transportation
2019
Research
USCG
Worker Identification Credential (TWIC)
Behavior Detection Visual Search Task Analysis
2018
Research
TSA
Project Visual Search Battery Report
Strategic Objective 4.3 Maintain U.S. Waterways and Maritime Resources
2021
2020
Accident Statistics
Statistic
USCG
2019
2018
57
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study/Report Title
Year
Evidence
Organization
2020
National Life Jacket Wear Rate Observational Study
2019
Statistics
USCG
2018
National Recreational Boating Safety Participation
2020
Statistics
USCG
Survey
National Recreational Boating Safety Exposure Survey 2020
Statistics
USCG
Predictive Port Resilience Tool to Assess Regional
2019
Research
S&T
Impacts of Hurricanes
Strategic Objective 5.1 Build a National Culture of Preparedness
Homeland Security Grant Program Evaluation and
Case Studies
In progress
Evaluation
FEMA
National Flood Insurance Program: Standard Flood
Insurance Policy, Homeowner Flood Form Regulatory
In progress
Analysis
FEMA
Impact Analysis
USFA Topical Fire Report Series
Recurring
Statistics
FEMA
Protective Actions Research
Recurring
Research
FEMA
Developing Metrics and Scoring Procedures to
2021
Research
FEMA
Support Mitigation Grant Program Decisionmaking
A Comparative Analysis of Hazard-Prone Housing
Acquisition Programs in US and New Zealand
2021
Research
S&T
Communities
Fire Incidents for States and Counties
2021
Statistics
FEMA
U.S. Fire Statistics
2021
Statistics
FEMA
2021
National Household Survey
2020
Statistics
FEMA
2019
2021
Flood Mapping Monthly Notice to Congress
2020
Statistics
FEMA
2019
2021
Data Sources and Methodology Documentation for the
2019
Statistics
FEMA
USFA Topical Fire Report Series
2018
Cost of Assistance Estimates in the Disaster
Declaration Process for the Public Assistance
2020
Analysis
FEMA
Program Regulatory Impact Analysis
A National Evaluation of State and Territory Roles in
Hazard Mitigation: Building Local Capacity to
2020
Research
S&T
Implement FEMA Hazard Mitigation Assistance Grants
Hurricane Floyd/ Hurricane Matthew Empirical
2019
Research
S&T
Disaster Resilience Study
Developing Consequence Thresholds for Storm
Models through Participatory Processes: Case Study
2019
Research
S&T
of Westerly Rhode Island
Plan integration for resilience scorecard: evaluating
2019
Research
S&T
networks of plans in six US coastal cities
Resilient Design Education in the United States:
Current and Emerging Curricula in Colleges and
2018
Research
S&T
Universities
Homeowner Acceptance of Voluntary Property
2018
Research
S&T
Acquisition Offers
58
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Study/Report Title
Year
Evidence
Organization
Strategic Objective 5.2 Respond During Incidents
Tribal Declarations Pilot Guidance Regulatory Impact
In progress
Analysis
FEMA
Analysis
Disaster Declarations for States and Counties
2021
Statistics
FEMA
Disaster Declarations for Tribal Nations
2021
Statistics
FEMA
Strategic Objective 5.3 Support Outcome-Driven Community Recovery
Disaster Housing Assistance (historical data)
2021
Statistics
FEMA
Public Assistance Program Summary of Obligations
2021
Statistics
FEMA
Historical Flood Risk and Costs
2021
Statistics
FEMA
Measuring Successful Disaster Recovery
2018
Research
S&T
Homeowner Acceptance of Voluntary Property
2018
Research
S&T
Acquisition Offers
Plans that Disrupt Development: Equity Policies and
2019
Research
S&T
Social Vulnerability in Six Coastal Cities
Strategic Objective 5.4 Train and Exercise First Responders
Situational Awareness Enhanced through Social
2019
Research
S&T
Media Analytics: A Survey of First Responders
Strategic Objective 6.1 Strengthen Departmental Governance and Management
Implementation of DHS Directive 026-06, Rev 2, Test
In progress
Evaluation
S&T
and Evaluation, 01 October 2020 Evaluation
2021
2020
DHS Annual Performance Reports
Analysis
DHS HQ
2019
2018
2021
DHS Annual Performance Reports: Appendix A
2020
(Methodology)
Analysis
DHS HQ
2019
2018
Strategic Objective 6.2 Develop and Maintain a High Performing Workforce
Virtual Bomb Factory Scenario Evaluation
In progress
Evaluation
FLETC
Human Performance: Evaluation of Operational
Readiness
In progress
Research
FLETC
De-escalation Behaviors in Patrol Officer-Citizen
In progress
Research
S&T
Encounters
Improving the Representation of Women and
2021
Racial/Ethnic Minorities Among U.S. Coast Guard
Research
USCG
Active-Duty Members
Improving Gender Diversity in the U.S. Coast Guard:
2019
Research
USCG
Identifying Barriers to Female Retention
Balancing Quality of Life with Mission Requirements:
An Analysis of Personnel Tempo on U.S. Coast Guard 2019
Research
USCG
Major Cutters
59
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Appendix D. Summary of Individual Survey Data
This appendix provides the full set individual survey Expertise, Supports, and Agency responses
to items reported in the capacity assessment.
Expertise
Respondents were asked to rate how relevant each item is to their job or position from 1 (Not
relevant) to 5 (Central to my job or position) and how knowledgeable the respondent is about
each item from 1 (No knowledge) to 5 (Extensive knowledge/expertise) The questions ask
about respondents' experience with evidence-building activities, by which we mean evaluation,
research, statistics, and analysis as defined above.
Responses
Missing
Percentage of Responses
ID
Item
Scale
(n)
(n)
1
2
3
4
5
Q2
Use logic or system models that
Knowledge
189
2
9%
19%
25%
26%
21%
convey how implemented activities
Q2
are expected to produce outcomes
Relevance
187
4
14%
15%
28%
19%
24%
Q3
Identify contextual factors that
Knowledge
189
2
4%
11%
22%
35%
29%
affect implementation
Q3
Relevance
187
4
5%
10%
19%
32%
33%
Q4
Engage with appropriate
Knowledge
189
2
5%
5%
16%
31%
42%
stakeholders to identify
Q4
researchable questions,
Relevance
187
4
9%
7%
16%
27%
41%
hypotheses, or issues to study
Q5
Formulate research questions to
Knowledge
189
2
10%
11%
22%
28%
29%
Q5
guide evidence-building activities.
Relevance
187
4
16%
18%
20%
19%
27%
Q6
Look for research in peer-reviewed
Knowledge
188
3
9%
14%
26%
22%
29%
journals (that is, by subscription,
Q6
Internet, or library access)
Relevance
186
5
32%
24%
24%
9%
11%
Q7
Look for information on websites
Knowledge
188
3
9%
14%
20%
27%
29%
that collate and assess the level of
evidence for a specific approach
Q7
(e.g., untested, promising, best
Relevance
186
5
23%
20%
23%
19%
15%
practice) presented in research
studies
Q8
Choose the activity to build
Knowledge
188
3
5%
12%
26%
30%
28%
evidence (e.g., descriptive
research, policy analysis,
Q8
performance measurement, or
Relevance
186
5
10%
10%
27%
26%
27%
program evaluation) that is best
suited for a research question
Q9
Choose the study design and
Knowledge
188
3
11%
15%
23%
22%
29%
methods that are best suited for a
research question, considering
factors such as the range of
Q9
designs and methods, resources
Relevance
186
5
15%
21%
23%
17%
25%
available, and the evidence
needed
60
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Responses
Missing
Percentage of Responses
ID
Item
Scale
(n)
(n)
1
2
3
4
5
Q10
Ensure a study plan aligns with
Knowledge
187
4
10%
11%
20%
23%
35%
the intended purpose(s) and
Q10
scope of the study
Relevance
186
5
14%
13%
17%
25%
31%
Q11
Consider and critically assess the
Knowledge
188
3
4%
8%
21%
28%
39%
strengths and limitations of
Q11
different data sources
Relevance
186
5
5%
8%
19%
26%
42%
Q12
Follow Federal regulations on the
Knowledge
189
2
34%
11%
22%
16%
17%
ethical treatment of human
subjects in research and
Q12
evaluation, including seeking
Relevance
188
3
60%
11%
9%
5%
15%
informed consent and protecting
confidentiality
Q13
Analyze quantitative data using
Knowledge
190
1
9%
14%
23%
19%
36%
statistics such as descriptive
statistics, correlations,
Q13
comparison of means, and
Relevance
188
3
16%
13%
15%
22%
34%
regression
Q14
Analyze qualitative data using
Knowledge
190
1
9%
14%
25%
22%
30%
appropriate methods such as
Q14
content analysis or thematic
Relevance
189
2
13%
15%
24%
20%
28%
analysis
Q15
Use data visualization techniques
Knowledge
189
2
5%
5%
26%
32%
31%
to communicate data and support
Q15
interpretation
Relevance
188
3
6%
5%
16%
30%
43%
Q16
Engage stakeholders to
Knowledge
190
1
3%
3%
18%
25%
51%
understand relevant context
Q16
relating to data
Relevance
188
3
2%
5%
14%
26%
54%
Q17
Interpret study results in light of
Knowledge
190
1
10%
11%
16%
28%
35%
limitations of the design and
Q17
methods used
Relevance
188
3
13%
10%
18%
26%
34%
Q18
Assess the reliability of a specific
Knowledge
190
1
13%
13%
25%
22%
27%
study by identifying related
Q18
studies and comparing methods
Relevance
188
3
19%
14%
26%
20%
21%
and results
Q19
Present results concisely using
Knowledge
190
1
3%
3%
12%
26%
56%
language accessible to the
Q19
audience
Relevance
188
3
4%
3%
10%
21%
63%
Q20
Disseminate recommended
Knowledge
190
1
4%
4%
19%
28%
45%
actions to decisionmakers that are
Q20
based on results from evidence-
Relevance
188
3
4%
5%
19%
21%
51%
building activities
Q21
Procure and manage external
Knowledge
190
1
17%
18%
20%
17%
28%
organizations, groups, or experts
Q21
to execute a study
Relevance
188
3
26%
17%
20%
18%
20%
Note: Percentages reporting '4' and '5' may differ from the summed total in Exhibit 15 by 1% due to rounding.
61
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Supports
Respondents were asked to rate how much they agree with each item describing the resources
at their agency for evidence-building activities from 1 (Strongly disagree) to 4 (Strongly agree).
Percentage of Responses
Responses
Missing
ID
Item
1
4
2
3
(n)
(n)
Strongly
Disagree
Agree
Strongly
Disagree
Agree
Access to the support staff I need
Q22
to conduct evidence-building
190
1
15%
26%
45%
14%
activities.
Q23
Adequate time to conduct
190
1
16%
28%
45%
10%
evidence-building activities
Received adequate professional
Q24
learning on how to use data
190
1
9%
26%
46%
19%
effectively to inform what I do on
a daily basis
Dedicated time to participate in a
Q25
data or evidence-related
190
1
17%
38%
36%
9%
professional learning community
Received adequate professional
Q26
learning to find, gather, and
190
1
12%
28%
44%
16%
critically assess data from
different sources
Received adequate professional
Q27
training on how to design and
190
1
14%
37%
36%
13%
conduct evidence-building
activities
Received adequate professional
Q28
training on how to procure
190
1
22%
36%
32%
10%
external organizations, groups, or
experts
Note: Percentages reporting "agree' and 'strongly agree' may differ from the summed total in Exhibits 16 and 17 by
1% due to rounding.
62
U.S. Department of Homeland Security
FY 2021 Capacity Assessment
Agency
Respondents were asked to rate how much they agree with each item below regarding their
operational or support Component from 1 (Strongly disagree) to 4 (Strongly agree).
Percentage of Responses
Responses
Missing
ID
Item
1
4
2
3
(n)
(n)
Strongly
Disagree
Disagree
Agree
Strongly
Agree
Q37
Has articulated how using
190
1
10%
30%
45%
15%
evidence fits with agency goals
Makes it clear to personnel what
Q38
they should be doing with
190
1
12%
45%
32%
11%
evidence
Acts on personnel suggestions for
Q39
using or improving the use of
190
1
10%
29%
51%
11%
evidence
Should be considered a role model
Q40
for evidence use by other Federal
188
3
18%
42%
32%
7%
agencies
Q41
Has prioritized using evidence for
189
2
10%
33%
41%
16%
decision making
Q42
Values and rewards flexibility and
190
1
7%
17%
59%
16%
continuous quality improvement
Modifies its course of action based
Q43
187
4
9%
27%
53%
11%
on evidence
Allows enough time to
create/obtain, analyze, and
Q44
consider research results and
188
3
17%
38%
37%
8%
other evidence when making major
decisions
Uses evidence to support decision
Q45
making on program and policy
189
2
8%
26%
56%
10%
options, operations, and strategies
Involves staff responsible for
Q46
providing evidence in decision-
188
3
12%
27%
49%
13%
making discussions
Appropriately disseminates
Q47
products and findings to internal
188
3
10%
24%
52%
14%
and external stakeholders
Encourages internal
communication to ensure there is
Q48
188
3
6%
22%
57%
15%
information exchanged across the
entire organization
Q49
Has IT systems in place to
189
2
14%
40%
36%
10%
efficiently examine data
Has IT systems and tools to
Q50
generate displays (e.g., reports,
188
3
11%
26%
48%
15%
tables, charts) that are useful to
my work
Note: Percentages reporting "agree' and 'strongly agree' may differ from the summed total in Exhibit 36 by 1% due to
rounding.
63
