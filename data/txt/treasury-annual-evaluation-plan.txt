Treasury FY 2023 Annual Evaluation Plan
Treasury FY 2023 Annual Evaluation Plan
Background.
The Foundations for Evidence-Based Policymaking Act of 2018 ("Evidence Act") requires agencies to develop a
multi-year learning agenda that is used to identify agency priority questions. It further requires agencies to
develop an Annual Evaluation Plan that provides a more detailed description of significant program evaluations
1
that the agency plans to conduct in the following fiscal year. Agencies can define "significant" for this purpose.
Approach.
This document outlines the significant evaluation activities that Treasury plans to conduct in FY 2023. The five
evaluations are projects to evaluate critical programs; the evidence generated by these projects will address
priority questions in the Treasury Learning Agenda. Treasury's Interim Evaluation Policy outlines the guiding
principles and quality standards for all evaluation activities conducted within the Department. Further, Treasury
is working to develop a broader evidence policy that will establish ways for the Evaluation Officer to better track
planned significant evaluations, methodologies, and dissemination of findings. The four types of evidence-
building activities are defined as follows (subject to refinement as policies are further developed):
Evaluation
Collection and analysis of data to assess effectiveness and efficiency of programs, policies, or procedures
Statistics
Collection, compilation, and processing of data for describing or estimating characteristics or insights
concerning groups
Research
Modeling or other systematic use of data to explore emerging issues or potential scenarios to generate
new knowledge
Analysis
Routine and frequent use of data that produces insights for decision making and program management
Dissemination of results for Treasury Evaluation Plan Projects
The agency evaluation policy, when published, will address Treasury's standards for the dissemination of
evaluation results and findings. Over the coming year, with input from the bureaus and offices that regularly
conduct evaluations, the Evaluation Officer, Chief Data Officer, and Statistical Official will determine the
appropriate mechanism for dissemination of findings internally and externally. Future evaluation plans will
include more specific dissemination plans for each evaluation.
Contents
Overview of Planned FY 23 Evaluation Projects
2
Details associated with Planned Evaluation Projects
3
Treasury Strategic Objective 1.1: Tax Administration and Policy
3
Project 1
3
Project 2
4
Project 3
5
1 Treasury defines "significant evaluations" as evaluations that are undertaken to evaluate critical programs, as defined in the Treasury implementation plan
for the Program Management Improvement Accountability Act (PMIAA), and for which the evidence generated by the evaluation is relevant to a priority
research question in the Treasury Learning Agenda. Evaluations required by law are also considered significant. The Treasury PMIAA implementation plan
defines critical programs as programs that are essential to Treasury's successful execution of its mission and for which failure, interruption, or compromise of
these programs could: 1) Undermine Treasury's capability to achieve its objectives; or, 2) Severely impact government operations; or, 3) Result in significant
loss of trust in the Department by key stakeholders/the public.
1
Treasury FY 2023 Annual Evaluation Plan
Overview of Planned FY 23 Evaluation Projects
Treasury plans to conduct three significant evaluation projects aligned to three learning agenda priority research
questions. Some of these activities began in previous years and some will start in FY 2023. All projects will be led
by the IRS and align to Strategic Objective 1.1, Tax Administration and Policy.
How can the IRS address taxpayer needs and preferences to deliver a better taxpayer experience?
1. Customer Experience Analytics
2.
Customer Callback Expansion
3. Customer Experience, Expectations, and Needs Survey
Additionally, Treasury is exploring significant evaluation projects for three other learning agenda questions.
Treasury has not yet identified specific evaluation questions or planned activities, but the Treasury FY 2022-2026
Learning Agenda includes additional background, data, and potential methods or approaches to answering these
questions. Activities may start by the end of FY 2023, dependent on resources.
Strategic
Priority Question
Lead
Objective
organization
1.3 Economically
To what extent are American Rescue Plan (ARP) programs being implemented
Departmental
Resilient
equitably?
Offices
Communities
What are the impacts and/or outcomes of the Emergency Capital Investment Program,
Departmental
Rapid Response Program, and Minority Lending Programs on Community Financial
Offices
Development Institutions (CDFIs) or minority depository institutions (MDIs)?
1.4 Resilient
What strategies deployed in the recovery from COVID-19 best prevented evictions and
Departmental
Housing Market
foreclosures? How can we track evictions nationwide?
Offices
2
Treasury FY 2023 Annual Evaluation Plan
Details associated with Planned Evaluation Projects
Title I of the Evidence Act requires that agencies share specific information for selected significant evaluation
projects to help anticipate the completion of and plan for the appropriate dissemination of the findings for use in
agency decision-making. The following tables (one for each project) provide the required information: the project
title, lead research organization, supporting organization(s) specific evaluation questions for each project,
data(sets) required for the project, the tools and software required for the project, the analytic approaches to be
employed for the project, and anticipated/existing challenges to completing the project.
How can the IRS address taxpayer needs and preferences to deliver a better taxpayer experience?
Customer Experience (CX) Analytics
Lead Organization: IRS Research, Applied Analytics, and Statistic Division / Data Management Division
Supporting Organization(s): IRS W&I Division
Evaluation Questions
Data Sources
Analytic Tools
Analytic Approaches
Question 1: How can
Account
The entire range
Text Mining Capabilities -
Natural Language
Management
of data and tools
Natural Language Processing
Processing techniques
Services (AMS)
available on CDW,
(NLP) models and tools
improve the taxpayer
Individual Master
specifically SAP
developed to analyze, quantify,
experience?
File (IMF)
HANA database,
and categorize unstructured,
Notice Delivery
Sybase IQ, R
textural data.
Question 2: How can
System (NDS)
statistical
Tokenization of AMS and
machine learning be
Individual Returns
software package
National Quality Review System
used to identify
Transaction File
and Python.
(NQRS) narratives
opportunities for
(IRTF)
A visualization
Topic Modeling using LDA
process
Online Services
tool we created
(Latent Dirichlet Allocation)
improvements?
(OLS)
(CX Solution) that
Issue Classification using
Customer Call
leverages a
Support Vector Machine (SVM)
Question 3: How can
Transcripts
variety of data
models
we be sure
A-11 Survey Data
sources and
Sentiment Analysis
implemented
Google Analytics,
organizes events
Predictive Models - Analytical
recommendations
Forsee Survey
and interactions
models developed to predict
have the desired effect
over time to
the likelihood of a customer
on the taxpayer
present the
entering a specific end state
experience?
unique journeys
and prediction of next journey
of the taxpayer.
steps within a tax module.
Models include Logitsistic
Regression, Decision Trees,
Random Forests, and Neural
Networks
Challenges associated with project:
Implementation of recommendations for process improvements requires buy-in from multiple
stakeholders and often requires changes to established processes, procedures and training
Each machine learning technique has pros and cons; need to have them compete against each other
using common objectives and criteria
Aligning data at the TIN level is not always possible
Processing large data volumes in realtime
3
Treasury FY 2023 Annual Evaluation Plan
How can the IRS address taxpayer needs and preferences to deliver a better taxpayer experience?
Customer Callback Expansion
Lead Organization: IRS Wage & Investment (W&I) Division, Customer Account Services Office
Supporting Organization(s): IRS W&I Division, Compliance Office and Operations & Support Office; IRS Tax Exempt
& Government Entities Division
Evaluation Questions
Data Sources
Analytic Tools
Analytic Approaches
Question 1:
Evaluative Data:
Management Information
Measuring progress
How can IRS
CCB Applications
System reports include the
of Customer
maximize the
Currently Configured
following platforms with
Callback
expanded use of
All CCB viable
varying report intervals and
applications
Customer Callback to
applications
stakeholder recipients.
deployed against IRS
improve the taxpayer
CCB Offer Rate
Intelligent Contact
Mod Plan reportable
experience?
CCB Take Rate
Management (ICM)
measures.
CCB Successfully
Contact Analytics
Industry
Registered
Enterprise Telephone Data
benchmarks for KPIs
CCB Reacquisition Rate
(ETD)
Monitoring
CCB Cancellation Rate
3rd Party Tools EyeQueue R
performance trends
CCB Busy/No Answer
Callback Dashboard & Portal
Rate, Transfer Errors
CCB Virtual Port
Utilization
Question 2:
Average Speed of
Management Information
Industry
How can the IRS
Answer
System reports include the
benchmarks for KPIs
evaluate the effect of
Avg Reconnect Time (to
following platforms with
Monitoring
Customer Callback
agent)
varying report intervals and
performance trends
Service to taxpayers?
Hold Time Saved
stakeholder recipients.
Intelligent Contact
Management (ICM)
Contact Analytics
Enterprise Telephone Data
3rd Party Tools EyeQueue R
Callback Dashboard & Portal
Question 3:
Customer Feedback via
Contact Analytics
Evaluating customer
How can the IRS
call recordings
Verint Contact Recording
feedback through
effectively gather
Customer Feedback via
3rd party contracted focus
sampling of calls
feedback from
focus groups
group reports
identified within
taxpayers on the
Customer Feedback via
3rd party or in-house IVR
CCB services.
Customer Callback
surveys
survey reports
designs to best meet
the needs and
preferences of
callers?
Challenges associated with project:
IT Service Provider has limited implemention bandwidth against competing priorities
CCB design applications are evolving with, and in parallel to, infrastructure upgrades
Customer callback offerings are limited by both system and human resources (agents)
Customer callback acceptance varies based on individual customer behavior
4
Treasury FY 2023 Annual Evaluation Plan
How can the IRS address taxpayer needs and preferences to deliver a better taxpayer experience?
Customer Experience, Expectations, and Needs Survey
Lead Organization: IRS Small Business/Self-Employed Division
Supporting Organization(s): none
Evaluation Questions
Data Sources
Analytic Tools
Analytic Approaches
Question 1:
Survey data from two surveys:
SAS, SPSS, R
Data collection
How can the IRS
The 2022 Customer
(statistical
through surveys
effectively measure the
Expectations, Experience and
software)
Focus groups
needs and expectations
Needs (CEEN) survey will engage
Microsoft Excel
Quantitative data
of SB/SE Taxpayers?
small business and self-
analytics (statistical
employed taxpayers to measure
analysis)
expectations for interactions
Behavioral Insights
with the IRS, needs for
compliant tax administration,
Question 2:
and awareness and use of IRS
How can the IRS
products and services.
effectively measure the
The 2023 Tax Professional
needs and expectations
Engagement Survey will be
of Tax professionals?
developed in partnership with
other IRS functions to better
understand the tax professional
community and identify
Question 3:
recommendations to improve
How can the IRS ensure
overall tax compliance by
that future actions are
leveraging the relationship
priorities supported by
between tax professionals and
taxpayers and tax
their clients
professionals?
Challenges associated with project:
Gathering data from a representative sample of SB/SE taxpayers and practitioners
Data gathered may need to be treated as directional and qualitative, rather than quantitative
5
