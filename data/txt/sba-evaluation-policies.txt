Framework and Guidelines for
Program Evaluation
U.S. SMALL
BUSINESS
ADMINISTRATION
*
U.S. Small Business Administration
MARCH 2019
Table of Contents
Table of Contents
2
Letter from the Director
3
Executive Summary
4
SBA Mission and Program Evaluation Integration
4
Chapter 1: Program Evaluation Core Activities
6
Chapter 2: Program Evaluation Framework
10
Chapter 3: Program Evaluation Role in Performance Management
22
Chapter 4: Plan and Prepare
24
4.A. Plan the Evaluation
24
4.B. Identify Key Stakeholders
33
4.C. Develop or Update the Learning Agenda and Program Logic Model
37
4.D Develop Evaluation Questions
45
Chapter 5: Conduct and Monitor
48
5.A. Select an Evaluation Design
55
5.B. Implement the Evaluation
59
Chapter 6: Disseminate and Implement Findings
63
6.A. Communicate Evaluation Results
65
6.B. Implement Recommendations
67
Appendix A: Internal Needs Assessment
68
Appendix B: Federal Benchmarking Study
71
Appendix C: Roles and Responsibilities in the Program Evaluation
73
Appendix D: Glossary
75
Appendix E: Evaluation Resources
80
Appendix F: SBA Program Evaluation Proposal Guidance and Template
81
Bibliography
83
Acknowledgments
85
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
2
Letter from the Director
SBA Colleagues,
I am excited to present the SBA's Framework and Guidelines for Program Evaluation at the U.S. Small
Business Administration. This tool will further support the use of evidence-based decision-making by
providing guidelines and common definitions for small business programs.
In addition, we have made tremendous progress initiating and managing new evaluations to support
program improvement. Most importantly, the SBA published its Enterprise Learning Agenda and
connected key research and evaluation questions to the FY 2018-2022 Strategic Plan.
The development and use of program evaluation as an additional element of performance management
will support the Agency's capacity for continuous improvement and modernization that will, in turn, help
us better meet the needs of America's entrepreneurs and small businesses.
The framework outlines the steps the Agency has, and will continue to prioritize, invest, and use evidence
to inform policy, strategy, and resource decisions. The guidelines serve as a companion to the framework
by providing technical direction to evaluation teams. With the guidelines, the SBA will develop learning
agendas that have transferable applications across programs, which have and will continue to be used to
develop the SBA's Strategic Plan and Annual Performance Plan.
In the spirit of continuous improvement, the framework and guidelines will be periodically reviewed and
updated to best meet the needs of the Agency. In addition, the program evaluation team will continue to
convene the internal Evidence and Evaluation Community of Practice that helped develop this framework
and guidelines to incorporate additional insights as new evaluations are completed.
I appreciate your dedication to build evaluation capacity within the SBA and help further the use of
evidence to build more effective and efficient programs for America's small businesses.
Sincerely,
J this
Jason Bossie
Director
Office Performance Management and the Chief Financial Officer
US Small Business Administration
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
3
Executive Summary
The U.S. Small Business Administration (SBA) has recognized that program evaluation is vital to ensuring an
effective and efficient organization that best supports the needs of the small business community. Evaluations
can help program managers better identify outcomes and impacts of services, reconsider delivery methods
and operations, or support a review of customer service gaps.
Prior to FY 2016, the SBA did not have a coordinated program evaluation function or structured support to
prioritize and conduct evaluations. To address this gap, the SBA established a program evaluation team within
the Office of Program Performance, Analysis and Evaluation (OPPAE) to develop guidance, provide technical
assistance to program managers, oversee an enterprise learning agenda, and monitor and support
independent evaluations.
In FY 2016, the program evaluation team conducted an internal study and best practice review of federal
partners to determine how an evaluation function could be most successfully deployed at the SBA. As a result,
the Agency established a program evaluation framework that governs its program evaluation activities.
SBA Mission and Program Evaluation Integration
Congress created the SBA in 1953 to aid, counsel, assist, and protect the interests of small businesses; to
preserve free competitive enterprise; and to maintain and strengthen the overall economy of our nation. Many
SBA programs have existed since the Agency's formation, but few have been systematically assessed to ensure
optimal program performance.
The SBA manages more than 30 different programs (FY 2014 Federal Program Inventory), which are defined as
"a set of planned activities directed to bring about a specific change to an identified audience." The SBA
administers these programs which support key stakeholders (e.g., entrepreneurs and small businesses) and
outcomes (e.g., job creation and retention, revenue growth).
Program evaluation supports senior leaders in their understanding of whether programs are working effectively
and efficiently; and whether they are serving the interests of small businesses and the economy as intended by
their governing statutes. Moreover, SBA's program evaluation function promotes operational effectiveness,
accountability, and transparency in line with the Agency's mission. Through SBA's program evaluation function,
program managers have support to conduct quality program evaluations that can better inform senior
leadership. If a program is not operating as intended, the SBA can reshape policy and program activities based
on evidence.Â²
Furthermore, program evaluation has been aligned with SBA's performance management functions by
1 Smith, M.F. (1989). Evaluability Assessment: A Practical Approach. Boston: Kluwer Academic Publishers.
2 The term "evidence-based decision making" has grown in usage over the past decade in the federal government with congressional and
presidential aims to improve the effectiveness and efficiency of programs and spending. It has achieved support as a smart policy tool as
witnessed through the establishment of the Evidence Based Policymaking Commission Act of 2016 and Office of Management and Budget
Memos: M-10-01 "Increased Emphasis on Program Evaluations," M-10-32 "Evaluating Programs for Efficacy and Cost-Efficiency,' M-13-17
"Next Steps in the Evidence and Innovation Agenda," M-12-14 "Use of Evidence and Evaluation in the 2014 Budget," and M-14-06
"Guidance for Providing and Using Administrative Data for Statistical Purposes."
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
4
promoting the use of rigorous social science methods and existing Agency planning, decision support, and
performance reporting activities. Program evaluation is a tool of performance management. Although
performance management and program evaluation are both decision-support functions, each area answers
different questions. Performance management supports the ongoing monitoring of programs to describe what
performance a program is achieving. Program evaluation involves systematic methods to understand why
programs are performing at a certain level. These tools complement one another and are both parts of building
a portfolio of evidence.
Through this framework, appropriate evaluation and research approaches will be used to answer questions of
strategic importance to the Agency. These approaches are governed by the methods developed by the U.S.
Government Accountability Office (GAO) and meet the standards on the use of evidence as enacted through
the Government Performance and Results Modernization Act (GPRAMA) of 2010 [Pub. L. 111-352] and the
Evidence Based Policymaking Commission Act (EBPCA) of 2016 [Pub. L. 114-140].
Through SBA's performance management system, program managers set goals, objectives, and measures in
the Strategic Plan and Annual Performance Plan, support decision-making through Quarterly Performance
Reviews (Deep Dives), annual strategic objective reviews, and weekly and monthly dashboards, and report on
performance and evidence in the Annual Performance Report and Agency Financial Report. Although these
tools are useful to support decisions and improve outcomes, SBA leadership has recognized a need to
conduct comprehensive evaluations and to use evaluation results to improve program operations and service
delivery.
This framework outlines the necessary steps the Agency has taken to prioritize, invest, and use the results of
program evaluations. The development and use of program evaluation as an additional element of the
performance management system can better support the Agency's capacity for continuous improvement and
modernization. 3 Along with the development of evidence to inform decision-making and program improvement,
the process of conducting program evaluations creates a mechanism for SBA employees to more fully support
a program's design. Useful evaluation questions and methods that yield valid and reliable data must be
developed in coordination with program managers.
With stakeholder participation built into the program evaluation, evidence can be gathered, and action items
can be established. As a result, SBA employees will be more engaged in their program's operations and
delivery to better support outcomes for small businesses.
In sum, SBA's program evaluation function will help:
Support greater senior leadership decision-making through more robust evidence;
Improve program accountability andeffectiveness;
Create a culture of continuous improvement; and
Engage stakeholders in program operations and service delivery.
3
As an example, imagine if the U.S. postal service continued to be delivered mail by Pony Express today. Letters would take weeks to travel across
the
country when newer forms of transportation would take days. For performance management, the plans, measures, and targets of the postal system
would be based on this outdated delivery method because "that's how it's always been done." To this end, program evaluation provides the SBA with the
tools to ask why programs are not performing as intended and how to improve them through evidence.
4 Stakeholder engagement is observed as an important characteristic of a high performing organization. While employee engagement is not
the intended result of a program evaluation, it is a positive byproduct that can further energize and deliver services more effectively and
efficiently. Government Accountability Office, Comptroller General's Forum: High Performing Organizations. GAO-04-343SP. Washington,
D.C.: February 2004.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
5
Chapter 1: Program Evaluation Core Activities
The SBA program evaluation function follows three core evaluation activities for each new evaluation: planning
and preparation; conducting and monitoring; and disseminating and implementing findings. The SBA
established these core practices through internal interviews and benchmarks of other agencies. See Appendix
A (Internal Needs Assessment) and Appendix B (Federal Benchmarking Study) for further information.
Plan and Prepare
Generally, program evaluation teams are comprised of a lead program evaluator, a project liaison of the
program to be evaluated, and an independent party (i.e., often a contractor; however, in some instances, a
research university or external party may be used) conducting the evaluation. An executive champion (senior
executive) must also be designated to ensure its success. He or she must serve as a program evaluation
advisor to ensure that the evaluation meets the Agency and program office's needs. The executive champion
partners with the SBA program evaluator, project liaison, and independent contractor throughout the evaluation
(each of these roles are further discussed in the sections below).
Before a program evaluation can be successfully launched, trust between the program's stakeholders and the
team conducting the evaluation must be established. The program undertaking the evaluation is referred to as an
evaluand. Stakeholders of the evaluand should be assured that the evaluator will focus on program
improvement and partner with them, as opposed to conducting anaudit. Specifically, program stakeholders
need to trust that evaluation is something that is being done with them and not to them. The evaluation must be
a collaborative and continuous improvement effort of program operations and service delivery.
Establishing trust between the evaluator and the program's stakeholders requires ongoing communication and
collaboration. Trust can be cultivated in the beginning of the evaluation process through the development of
learning agendas and logic models. Learning agendas are continuous improvement tools that create a
structure for a program to consider its evaluation priorities by identifying the questions that will most likely lead
to greater program efficiency and effectiveness when answered. The logic model is another tool that outlines a
program's theory of change (i.e., how a program operates to produce desired outcomes). This process
ensures that the evaluators properly scope the evaluation questions and collaboratively develop them to
promote performance improvement.
Learning agendas support collaborative, self-focused discussion about questions the program stakeholders
would like answered to inform program improvements. To develop a learning agenda, the program evaluator
must gather the stakeholders; review the literature to determine what is known; identify and prioritize the
questions for program effectiveness and efficiency; develop a plan or evaluation design to answer these
questions; conduct studies and analyses; and implement the findings through an implementation plan. The
evaluation questions developed from a learning agenda should ensure that key information can be used by
decision-makers.
Logic modeling helps program managers articulate the "theory of program change.' The program evaluator must
partner with program staff to compile a comprehensive list of the program's resources, activities, customers,
deliverables, and measurable goals. These goals will focus on short-term (knowledge/awareness), intermediate-
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
6
term (behavior), and long-term (change of condition) outcomes.5
During the performance management cycle, there are several opportunities to employ the use of learning
agendas for program improvement. In this framework, the development of learning agendas will be conducted
during the annual strategic objective review (between March and May each year). An SBA program evaluator
will work with his, or her, corresponding performance analyst to engage programs about questions that could
help inform senior leadership decision-making and determine the current state of evidence. Relevant findings
or reports will be added to the SBA evidence registry (a registry of historical reports, evaluations, and other
research).
To ensure that SBA project liaisons and other program evaluation team members have the knowledge and
ability to participate in an evaluation, an SBA program evaluator conducts trainings, as appropriate, throughout
the year. The SBA builds training guides from resources available within the federal evaluation network and
customizes them for SBA programs. To identify program evaluation practices from other agencies and adopt
6
them for the SBA, the Agency incorporated key GAO guidelines into its framework.
Conduct and Monitor
After the planning and preparation phase, the SBA conducts and monitors its program evaluations. The SBA
uses independent contractors and other third-party evaluators (e.g., academic researchers, federal statistical
agencies that match an organization's administrative data, and think tanks) to conduct program evaluations
with active involvement of programstaff, customers, and other stakeholders.
After evaluation teams have been formed, evaluation questions must be scoped with active participation of
program management, including the executive champion. An SBA lead program evaluator serves as a
technical expert to guide the evaluation team. This individual also serves as the Contracting Officer's
Representative (COR), or work assignment manager, of each evaluation study. The SBA's lead program
evaluator provides status updates to the Performance Improvement Officer and the Evidence and Evaluation
Community of Practice.7
The program evaluation team must have active participants throughout the evaluation to ensure its success. If
the program's executive champion, project liaison, or SBA program evaluator departs, the Performance
Improvement Officer and the corresponding Associate/Assistant Administrator should recommend a
replacement for the evaluation team.
Every program evaluation must follow a standard process that will be incorporated in the statement of work for
the contractor or researcher. This standard procedure plays an important role in the evaluation function at the
SBA. The steps of the evaluation process for the evaluation are as follows:
Develop a logic model (theory of change)
5
Examples of learning agendas and logic tables are presented in Appendices XX.
6
Government Accountability Office, Program Evaluation: An Evaluation Culture and Collaborative Partnerships Help Build Agency
Capacity. GAO-03-454. Washington, D.C.: May 2, 2003
7 The SBA has established a Community of Practice to share best practices on program evaluation and evidence-building with program
managers and analysts.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
7
Define the criteria and develop evaluation questions using learning agendas
Assess the questions to determine if the scope is reasonable and whether there is sufficient data
to answer these questions
Design an evaluation methodology with input from program management
Build a quality assurance plan (QAP). A QAP is a formal plan to ensure that the performance data and
the design are appropriate and that evaluation results obtained are defensible. This QAP will specify
limitations of the data and any accompanying restrictions attached to its use.
Conduct the program evaluation (data collection)
Review preliminary results with the evaluation team
Integrate the findings in the performance management system (i.e., ensure evaluation results are
factored into the performance management system, including but not limited to: Annual
Performance Report, Strategic Objective and Portfolio Reviews, Strategic Plan, Quarterly
Performance Reviews (Deep Dives), and Monthly Dashboards)
Prepare a program evaluation report (historical base of evidence) that will be added to an inclusive
evidence base
Present results and recommendations to SBA's senior leadership
Develop an implementation plan in coordination with performance analysts to ensure
recommendations are implemented
Create a summary fact sheet and publish findings internally andexternally, where appropriate
Track and monitor the implementation of the recommendations
Disseminate and Implement Findings
After an evaluation is completed, clearance for public dissemination must be secured from the Performance
Improvement Officer and Associate Administrator of each program's respective office. The evaluation team will
provide the Associate Administrator for the respective program, the Chief Evaluation Officer, the Performance
Improvement Officer, and the Associate Administrator for Congressional and Legislative Affairs a copy of the
report. Actions must also be taken to help the program transform its processes and activities through the
evidence gathered.
To ensure that results are actionable, and recommendations can be implemented, the following principles are
incorporated into each evaluation and considered throughout the process:
Ethics - Conduct the evaluation by adhering to the rules governing human rights, confidentiality,
and privacy. Minimize the burden to research participants and the cost to taxpayers.
Independence - Conduct the evaluation through an outside party that either does not have
vested interest in the outcome or will not interpret the results in ways that are self-serving or
misleading. Eliminate the appearance of bias to ensure results are properlyused.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
8
Rigor - Employ methodological approaches that best support the definitive answers to the
evaluation questions under investigation. The limitations of the methods used and how much the
conclusions drawn can be unequivocally supported should be stated explicitly when describing
the methodology and reporting the findings. Minimize threats to internal validity (the ability to
draw causality inferences) and external validity (generalizing beyond the specific program being
evaluated).
Relevance - Scope and select evaluation questions most closely tied to the goals of the program,
the priorities of the Agency, and the intended use by senior leaders.
Transparency - Ensure that the evaluation scope, design, implementation, and results are available
for internal and public review, assessment, and critique.
After opportunities for improvement are identified, a program manager must have the tools to implement
recommendations. Performance analysts could help support a program as it modernizes.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
9
Chapter 2: Program Evaluation Framework
The SBA Program Evaluation Framework outlines the structure of the program evaluation function, defines
roles and responsibilities, and establishes guiding principles. The Framework is not official policy and is meant
to provide SBA program managers with the tools to initiate a program evaluation. The SBA adheres to an
integrated performance management system that ensures that its programs have the capabilities to fully support
entrepreneurs and small businesses. The Agency's program evaluation function is housed in the Office of
Program Performance, Analysis and Evaluation within the Office of Performance Management and the Chief
Financial Officer. SBA program evaluations are managed through lead program evaluators housed in the
Analysis and Evaluation Division. These evaluators work in coordination with SBA's performance analysts who
support the Agency's strategic planning, decision-support, and performance reporting activities.
Technical Expertise and Management
Program evaluation expertise is cited as one of the most critical elements in the development of capacity. The
SBA will ensure that it assigns a program evaluator who is trained and has experience in the following fields:
research design and methods, data management and statistical analysis, performance measurement and
monitoring, and evaluation communication. SBA's lead program evaluators are skilled in program evaluation,
research methods, and social science.
At the SBA, the responsibilities of a lead program evaluator include the development and refinement of the
Program Evaluation Framework (guidance), establishment of learning agendas and logic models,
development of training materials, delivery of technical advice, coordination with other agency program
evaluations and OMB, and management of program evaluation teams.
In coordination with a lead program evaluator, performance analysts, also housed in the Office of Program
8
Performance, Analysis and Evaluation will provide support through their respective program office accounts.
For performance analysts, employee performance standards include critical elements for program evaluation.
These standards require that the performance analysts support the review of program learning agendas,
participate in the design and review of the program evaluation proposals, and integrate program evaluation
findings into their respective program office performance management products (goals and objectives
identified for a program in the Agency Strategic Plan, Annual Performance Plan, Annual Performance Report)
and processes (Quarterly Performance Reviews (Deep Dives), Strategic Objective and Portfolio Reviews).
Some programs may use their own program funds to conduct an evaluation but may require the assistance of
a program evaluator. The SBA will ensure accountability and transparency of all program evaluation resources
and findings. Moreover, the centralized evaluation team will ensure that evaluation evidence is leveraged for
program improvements and that evidence across evaluations is synthesized to establish best practices.
8
Each performance analyst is assigned to several program offices (i.e., program account).
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
10
Performance and Evaluation Community of Practice
To instill a culture of evidence-based decision making and ensure program evaluation knowledge transfer, the
SBA has established a Performance and Evaluation Community of Practice. The Evidence and Evaluation
Community of Practice is composed of SBA employees representing program offices; and fosters a culture of
learning and performance. It creates and sustains an organizational commitment by employees who support
program evaluation; continuous improvement; and planning and performance management activities.
The community shares ideas and best practices through monthly meetings and helps foster the Agency's
commitment to program improvement. An organization's commitment to a culture of continuous improvement
occurs through the planning and development of program evaluations. Investment in program evaluations is
strategic as the Agency focuses on priorities with senior leadership and uses evidence to inform policy and
program development.
Evidence is valuable to improve program outcomes; a community can help ensure that the evaluations
commissioned are relevant, practical, and achievable. Regular communication between program staff and
senior leadership will be fostered through the community. The members can help promote the use of and
disseminate tools for programevaluation.
Senior Leadership Engagement
For evaluations to be successful, senior leaders must be engaged throughout the process. When a program is
selected for evaluation, senior leadership must be part of the process and approve the following: logic model;
evaluation questions; evaluation design; preliminary results presentation; senior leadership briefings; and the
implementation plan.
To support each program evaluation, an executive champion (senior executive) must be appointed by the
Associate/Assistant Administrator of that program office to ensure its success. The executive champion will
partner with the Performance Improvement Officer and ensure that other senior leaders are informed about its
status. The executive champion will also partner with other senior leaders and present key findings at
Quarterly Performance Reviews (Deep Dives), while making sure that barriers are removed for program staff
to ensure that a successful program evaluation is conducted.
Contract Solicitation
In the federal benchmarking assessment, many agencies mentioned the importance of dedicated funds for
program evaluation. The SBA allocates funds each year for program evaluation that enables the execution of
short-term evaluations. In addition to having internal expertise, a successful evaluation function must have
analytic expertise through internal resources, expert professional evaluators, and external contractors. The
SBA uses independent contractors who have experience conducting program evaluations; developing logic
models and learning agendas; and summarizing actionable recommendations.
The SBA's evaluation contracts will follow Federal rules and regulations. Evaluations are conducted in a
systematic manner that follows the sequence of steps outlined in the winning contractor's approved technical
proposal. The evaluation contracting vehicle will enable the Contracting Officer's Representative (COR) to
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 11
issue technical direction and task orders based on the unique requirements of each evaluation.
Learning Agendas and Logic Models
The SBA will identify two to three program evaluations that each program evaluator will conduct during the
year. As mentioned earlier, learning agendas and logic models will be used to generate a set of evaluation
questions that use quality data, appropriate design, and yield actionable recommendations. As technical
assistance can be time intensive, and require continuous input from offices undergoing evaluations, the SBA will
continue to introduce these tools through the Performance and Evaluation Community of Practice. For each
program evaluation team, training on specific items will be offered at varying points of the evaluation cycle.
Evaluation Integration with Performance Management
Within the SBA, the Office of Program Performance, Analysis and Evaluation is responsible for supporting the
Administrator's priorities; strategically planning a framework around these priorities with goals and objectives;
measuring and assessing progress; using this information to support Agency decisions; and evaluating
programs and strategies. These responsibilities, governed by GPRAMA, must complement and inform the
program evaluation goals of the Agency. As a result, SBA's program evaluation function will be integrated into
the planning, decision-support, and reporting phases of the performance management cycle. The Strategic
Plan and Annual Performance Plan will integrate program evaluations and other evidence. Evaluations will
inform annual strategic objective and portfolio reviews, quarterly deep dives, and other performance analysis.
Findings from these evaluations will be published in performance reports to promote transparency.
During an evaluation, performance analysts will meet quarterly with an SBA program evaluator and project
liaison to both review the current knowledge relevant to a program based on existing evidence and identify
unanswered questions that may be pursued with further evaluation studies. SBA performance analysts are
members of the Community of Practice, which gives them access to program evaluation information and
context. Because the program evaluation function is housed in the SBA's Office of Program Performance,
Analysis and Evaluation, the Agency has a distinct advantage to leverage a plethora of data and other
evidence. Given this unique opportunity, performance analysts in the Performance Management Division can
help identify program activities that could benefit from a program evaluation and help program evaluation
teams identify other sources of data.
The program evaluation team will track the evaluation findings through the SBA's Evidence Registry. The
Evidence Registry stores the results of past program evaluations, research, and other evidence to help ensure
future learning and actions transfer to similar programs. Dependent on the results of the evaluation, metrics or
milestones may be developed, and revisited monthly or quarterly, to promote transparency and ensure
accountability.
Program Evaluation Proposals
The Performance Improvement Officer will sponsor a call for evaluation proposals each year. Proposals will be
gathered each spring in coordination with the annual strategic objective and portfolio reviews and before
development of the OMB Budget and Performance Submission that begins in June. Program offices will be
able to identify potential program evaluations based on their learning agendas and evaluation questions.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 12
Proposals will be collected and reviewed through the Office of Program Performance, Analysis and Evaluation
and delivered to the Chief Evaluation Officer, the Performance Improvement Officer and senior leadership for
selection. Formative evaluations may be rapid and iterative if scoped properly. Such evaluations will
commence starting in June and will be completed within twelve to fifteen months. More complex studies that
measure impact across time may take several years to complete and require relatively greater resources.
Program evaluation proposals must address the following criteria:
How does the evaluation assess program effectiveness andefficiency?
How does the evaluation align with SBA's strategic goals and objectives?
How could the findings enhance the opportunities for better performance measures?
How could the findings transfer to other programs with similar goals, activities, customers, and
intended outcomes?
Evaluation Proposal Selection
The proposals will contribute to SBA's Enterprise Learning Agenda. The SBA program evaluation team will
review proposals based on the criteria and deliver recommendations to the Chief Evaluation Officer and the
Performance Improvement Officer who will then make an announcement to senior leadership on selection.
Multi-Stakeholder Evaluation Teams
A prerequisite for a proposal will be the identification of an SBA executive champion and project liaison, who will
be responsible for working with an SBA program evaluator to plan and oversee the evaluation. Successful
evaluations will require a project liaison to commit four to six hours per week, on average, for twelve to fifteen
months. The project liaison will work with the contractor and the SBA program evaluator who will also serve as
the COR. The program evaluator will advise the evaluation team in planning, designing, and management. The
Performance Improvement Officer, executive champion, and project liaison must guarantee active
representation of the project liaison and staff on the evaluation team before the evaluation begins.
Training and Technical Assistance
Once the evaluations have been selected and a commitment has been made, an SBA program evaluator will
hold customized training sessions with the program evaluation team. The training will include an explanation of
key evaluation principles, steps in the process, and how to work most effectively with the contractor and other
relevant stakeholders, such as those in field operations, subject matter experts, and program staff who are not
directly involved in the evaluation.
Conducting the Evaluation
The evaluation team will be given a clear template that describes the deliverables, timelines, and elements of
the evaluation. This standard procedure will guide the evaluation and will be managed by an SBA program
evaluator with advice and collaboration from the program stakeholders. The evaluation kick-off meeting will
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 13
include a review of the contractual guidelines and allow the team to clarify roles, expectations, and other issues
that have not been explained. The kick-off meeting will take place approximately a week after the contractor has
received the statement of work and about two weeks before they submit a work plan. See Appendix C for
a
description of the roles and responsibilities of key stakeholders in the program evaluation.
Monitoring Ongoing Evaluations
An SBA program evaluator will provide technical advice and assistance on ongoing Agency evaluations and will
engage with the programs at least once a month. He or she will prepare quarterly reports on the progress of
the Agency's evaluation efforts. The implications of findings will be explored for the evaluated programs during
the Quarterly Performance Reviews (Deep Dives) and annual strategic objective review.
Federal Partnerships and Networks
Staying abreast of current directions in the field of evaluation science and in the practice of evaluation requires
involvement with partners in the field. The SBA will ensure that it is represented through organizations that
promote the use of evidence to improve outcomes in the federal government and beyond. The SBA will also
be an active participant alongside the OMB Evidence Team9 and the various federal evaluation communities of
practice. These efforts will ensure that the SBA is represented in the federal community about program
evaluation activities. SBA program evaluators will identify continuing education events with which to benchmark
progress and achievements with the evaluation function at the SBA each year.
Compliance and Ethics
When implementing the evaluation function, the evaluation team will ensure that it complies with the legal
principles of evaluation and social science research methods. Evaluations must adhere to the ethical
requirements set forth by the field and monitored by the oversight bodies. Ethical guidelines are adopted
across the social science disciplines from which evaluation methods are drawn. As appropriate, evaluators
must assure and maintain participant privacy and confidentiality. To ensure ethical guidelines are followed,
institutions of higher education and many evaluation contractors have Institutional Review Boards (IRBs) that
must review and approve all proposed evaluations. IRBs are committees that review research proposals to
ensure ethical guidelines are followed in research involving human subjects.
All evaluations that collect new data from non-Federal entities will follow the requirements of the Paperwork
Reduction Act (PRA). The PRA ensures that respondents are not overburdened by Federal information
collection. The SBA program evaluator will consult with SBA's PRA Officer during the planning phase to
comply with PRA requirements.
Securing an Information Collection Request (ICR) approval as part of OMB clearance can take up to 180 days
to complete from the point of complete submission of responses to required Supporting Statement Part A and
Supporting Statement Part B questions. The evaluation team may explore existing ICRs for data collection.
Data must be available to the public for further analysis and to satisfy the Freedom of Information Act (FOIA).
Thus, SBA evaluation contracts may require public use datasets to be delivered by contractors at the
9 The OMB Evidence Team coordinates program evaluation policy for the Federal Government.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 14
completion of an evaluation. Public-use datasets must remove confidential business information (CBI) and
personally identifiable information (PII).
When opportunities arise to use administrative data to conduct evaluations, a memoranda of understanding
(MOU), and agreements with federal partners who will provide statistical data that can be matched with
program administrative data, must follow the law; and must be constructed to minimize risk to the Agency and
its stakeholders. Similarly, evaluation contracts should contain language referencing the Privacy Act and other
relevant FAR clauses pertaining to data security, data storage, and protection of confidential information.
Data Security Issues: Negotiating a System Security Plan
An evaluation must ensure that participant data are protected from unauthorized access or use. Standards
must be followed to minimize risk of data misuse. These standards will vary depending on the sensitivity of the
data. The Federal Information Security Management Act of 2002 (FISMA) provides protection for federal
Information systems against threats. The National Institute of and Standards Technology (NIST) provides
guidance and standards for FISMA compliance.
An evaluator shall consider the data security standards they will need to require of the contractor early in the
scoping process to ensure they are specified in the contract terms and conditions. To ensure the contractor
complies with the standards, the evaluator shall review the contractor's system security plan that details how
the data will be protected.
In the framework, the SBA must be proactive to ensure that compliance to ethics is invariably achieved. Before
an evaluation begins, an SBA program evaluator will coordinate with staff in the Office of General Counsel
(OGC) on any data collection, sharing, or matching. The SBA will flag all concerns with adherence to these
statutes and regulations so that internal controls can be developed to mitigate potential non-compliance. An
accounting of these concerns will summarize challenges encountered through the evaluation. Each evaluation
team will share any concerns for legal risks in the evaluation with OGC.
Plan and Prepare
Four key considerations influence how you plan for an evaluation:
Choosing the right evaluation approach for your program. The purpose of the evaluation must
inform the evaluation approach, which must also align with the program's maturity. Design evaluations,
sometimes called formative evaluations, are prospective and used in program development or retooling.
Process evaluations are conducted after a program is implemented, and typically serve as a check on
how the program is being managed. Outcome and impact evaluations are retrospective, and they focus
on program results. Impact evaluations also assess the causal links between program activities and
outcomes.
Budgeting for an evaluation. Conducting an evaluation can take considerable time and incur significant
expense. Budgets required for evaluations vary widely, depending on the scope and scale of the
program; the type and complexity of evaluation questions; the evaluation design; and the availability of
existing data. Your COR and agency evaluation practitioners can help you estimate a budget based on
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 15
your program's unique evaluation goals.
Developing an implementation plan. An evaluation implementation plan includes specific evaluation
tasks, the associated deliverables, and a timeline for conducting the evaluation. The plan helps hold the
evaluation team accountable and ensures that an evaluation promptly produces the anticipated outputs.
Anticipating potential data limitations and stakeholder concerns. You should be aware of potential
challenges that SBA programs often face related to program evaluation. These include limitations in
identifying existing data resources, determining if you need to collect new data, and overcoming barriers
to collecting new data. Identify and address common internal and external stakeholder concerns about
outcome evaluations before it starts. These guidelines provide a detailed discussion of common
stakeholder concerns, approaches and responses to consider. These barriers are typical to all program
evaluations; but anticipating them up-front can help you prepare for and overcome them.
Identify Key Stakeholders
A key step in evaluating a program is identifying stakeholders and developing a stakeholder involvement plan.
This plan can be as formal or informal as the situation warrants. These guidelines broadly define a stakeholder as
any person or group who has an interest in the program being evaluated or in the results of the evaluation.
Incorporating a variety of stakeholder perspectives in the planning and implementation stages of your evaluation
will provide many benefits, including:
Fostering a greater commitment to the evaluation process.
Increasing the chances that findings and recommendations are implemented.
To foster cooperation, first identify relevant stakeholder groups and then determine the appropriate level of
involvement for each group (see Appendix C for the key roles in SBA evaluations).
Develop or Update the Learning Agenda and Program Logic Model
Programs considering an evaluation at the SBA must first complete a learning agenda. A learning agenda is a
continuous improvement program tool that creates a structure for a program to consider its evaluation priorities. It
helps program managers address questions using evaluative approaches and evidence to inform decision-
making.
Learning Agenda Elements
Program Office
Briefly describe the primary functions
What are the most important challenges currently facing your team?
Identify goals of the program office for the upcoming year.
A logic model is a diagram and text that shows the relationship between your program's work and its desired
results. Every program has inputs (or resources), activities, outputs, customers (or audiences), and desired
outcomes; a logic model describes the logical (causal) relationships among these program elements, as
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
16
illustrated in the graphic below.
HOW
WHY
Resources/
Activities:
Outputs:
Customer/
Short-term:
Intermediate:
Long-term:
Inputs:
Audience:
Activities you
Product or
Changes in
Changes in
Change in
Investments
plan to
service
User of the
learning,
behavior,
economic
available to
conduct in
delivery/
products/services;
knowledge,
practice, or
conditions
support the
your program
implementation
target audience
attitude, skills,
decisions
program
targets you aim
the program is
understanding
to produce
designed to reach
Program
Results from Program
External Influences:
Factors outside your control (positive or negative) that may influence the
outcome and impact of your program/project
The next section will help you form critical questions that, if answered, would improve the functionality or build
evidence for the impact of your program.
Develop Evaluation Questions
The following four steps should aid evaluators in the process of designing evaluation questions:
1.
Review, update, or develop the learning agenda.
2. Review the logic model and further identify what aspects of your program you wish to evaluate.
3. Consult with stakeholders and conduct a brief literature search for studies on programs ikeyours.
4.
Generate a potential list of the overall evaluation questions.
Group evaluation questions by themes or categories (e.g., resource questions, process questions, outcome
questions). For outcome evaluations, ensure that they will be effective in measuring progress toward program
goals and against identified baselines.
Conduct and Monitor
Assessing evaluation data needs is often the first implementation step of the evaluation and will help to inform
the selection of evaluation design. You will need to determine the extent to which existing data sets, referred to
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 17
as secondary data, can address your evaluation questions. As part of this process, your evaluator will review
the primary data already collected to determine if they are suitable for evaluation purposes. Primary data
collection refers to any new data necessary to be collected. There are many data collection methods used for
program evaluation, and each of these methods has advantages and challenges. Methods include surveys,
interviews, focus groups, and direct monitoring or observation.
You will need to decide if the data that you require to address evaluation questions are qualitative or
quantitative or both. Qualitative data are often in-depth collections of information gathered through
observations, focus groups, interviews, document reviews, and photographs. They are non-numerical in nature
and are often classified into discrete categories for analysis. In contrast, quantitative data are usually
collected through reports, tests, surveys, and existing databases. They are numerical measures of your
program (e.g., the amount of loans administered) that are usually summarized to present general trends that
characterize the sample from which these data are drawn. The decision to use qualitative or quantitative data
is not an either/or proposition. Instead, consider which form of data is most useful (given the evaluation
question and context).
SBA program managers must balance obtaining sufficient high-quality quality data to demonstrate useful
results while not overburdening the partners from whom you would solicit the data. Though you and your
evaluator must gather high-quality data, the requirements cannot be too onerous for partners. Any approach to
primary data collection must consider the "tipping point" where the data collection itself becomes a disincentive
to participation in your program.
Select an Evaluation Design
There are three broad classes of evaluation methodologies: non-experimental, quasi-experimental, and true
experimental.
Non-experimental designs are best suited to answer design and process questions (e.g., What are the
inputs available for this program? Are the activities supporting good customer satisfaction?). Non-experimental
designs do not include comparison groups of individuals or groups not participating in the program.
Quasi-experimental designs are employed to answer questions of program outcome. They often compare
outcomes of program participants with non-participants that have not been randomly selected. Alternately, a
quasi-experiment might measure the results of a program before and after an intervention has taken place to
determine if the time-related changes can be linked to the program's interventions. This type of evaluation
design can be particularly appropriate for evaluating social programs, such as those most often funded by the
SBA, because a true experimental design is often not feasible, practical, or ethical to implement.
True experimental designs (alternately referred to as randomized control trials, or RCTs) involve the random
assignment of potential program participants to either participate in, or be excluded from, the program. These
studies try to assess the causal impact and yield quantitative data that are analyzed for differences in results
between groups based on program participation. True experiments can be used in evaluation when:
Clearly defined interventions can be manipulated and uniformlyadministered;
There is no possibility that treatment will spill over to control groups (those for whom a program's
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 18
intervention is not intended, see textbox); and
It is ethical and feasible to deny a program's services to a group, at least for a long enough time
to support the evaluation.
Implement the Evaluation
After you have settled on your evaluation questions and evaluation design, you are ready to implement the
evaluation. At this juncture, you can generally turn the reigns over to your lead program evaluator. However, a
few aspects of implementation may require your involvement. These include:
Distribute the evaluation design, or a summary of it, to stakeholders, and subsequently
communicate any schedule or other important changes to stakeholders during implementation.
Review and provide feedback on interview guides, surveys, or other data collection instruments, if your
evaluator did not finalize these during evaluation design.
Make first contact with participants whom evaluators need to contact, to inform them about the
evaluation and encourage them to participate in data collection.
Participate in periodic check-ins with your evaluator to ensure implementation is on track, and to help
address any implementation challenges.
Assist in the contextual interpretation of analytical results.
Disseminate and Implement Findings
Although communicating your results is one of the final steps in the evaluation process, start planning for this
important step. Although your evaluator will take primary responsibility for collecting and analyzing evaluation
data, the process of communicating evaluation results requires collaboration between the evaluator and the
SBA program manager. Careful consideration of your program's stakeholders will influence how to best
organize and deliver evaluation results. The results have three basic elements: findings, conclusions, and
recommendations.
Data collected during the implementation of the project will yield findings. Findings refer to the raw data and
summary analyses. Because the findings are a part of the data analysis process, the evaluator should retain
the primary responsibility for communicating findings to program management. Evaluators often deliver
findings to the SBA program in a draft report or preliminary findings briefing.
Conclusions represent the interpretation of the findings, given the context and specific operations of your SBA
program. Your evaluator may independently derive some initial interpretations; however, program managers
should have an opportunity to provide comments based on the draft report and/ or preliminary findings briefing,
to suggest ways to refine or contextualize the interpretation of the findings.
Recommendations are based on the findings and conclusions of your evaluation. A strong evaluator will
understand that framing recommendations is an iterative process that should involve obtaining feedback from
SBA project staff and key stakeholders. Project staff involvement in the development of recommendations is
important, as most recommendations are designed to lead to changes in how programs work.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
19
Communicate Evaluation Results
In addition to a report, you can opt for alternative reporting formats depending on the needs of each
stakeholder group. Common reporting methods include a shortened version of the evaluation report for broad
distribution; briefing(s) that may use slides or other visual aids; and evaluation fact sheet(s). At a minimum, you
and your evaluator's communication of evaluation results should include the following steps:
1.
Present preliminary results and findings to program staff and other relevant stakeholders (e.g.,
SBA's senior management).
2. Prepare a program evaluation report.
3.
Conduct the final recommendations briefing to SBA 's senior management.
4.
Create a summary fact sheet of the evaluation's key findings and recommendations.
5.
Publish findings in consultation with the Office of Congressional and Legislative Affairs; work with
the SBA program evaluation team to disseminate your evaluation findings through the SBA
Program Evaluation & Evidence Registry (PEER).
Tying your findings directly to the evaluation questions strengthens the applicability and relevance of your
results. Organizing your findings and recommendations in a way that clearly makes this link will ensure that
you have collected and are reporting on the key questions that the evaluation was designed to answer.
Implement Recommendations
Implementing evaluation recommendations to your program is one of the greatest sources of value to
programs from the evaluation process. Toward the end of an evaluation, the project liaison should coordinate
with performance analysts in SBA's Office of Program Performance, Analysis and Evaluation to develop a
recommendation implementation plan. This plan should include:
Recommendations for implementation.
Anticipated results based on implementing the recommendations.
Actions planned to implement recommendations.
Action budget.
Timeline for completing actions and implementing recommendations.
Your evaluation plan should receive approval and support from relevant senior management. This approval will
help ensure that resources are sufficient to implement the recommendations. The implementation plan should
also include methods to track and monitor the implementation of recommendations.
Why Evaluate?
Some individuals may say that program evaluation is too time-consuming, onerous, and costly. However, the
history of federal programs suggests that failure to evaluate programs can be costlier in the long term.
Performance management involves several activities that ensure accountability and monitoring of whether
programs are operating as they are intended. The performance management system involves checks to
ensure that that objectives and measures are in place. Although there are costs associated with
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
20
commissioning independent evaluations, program evaluation can empower leaders to better understand what
elements of a program's design work and why. This knowledge can lead to performance improvements that
can lead to cost-savings.
Program evaluation provides the opportunity for a more comprehensive review of a program. Results illustrate
how SBA programs make a difference for entrepreneurs, are effective and efficient, provide customer
satisfaction, offer benefits that outweigh program costs, and merit continued funding. If evaluation results show
that a program needs improvements, information developed from the evaluation can help decision-makers
determine where adjustments should be made to ensure future success. Reasons for evaluating SBA
programs include:
Providing data to stakeholders. Program evaluations provide valuable information to program
managers, senior leadership, program participants, and other external stakeholders.
Improving the program. Program evaluations can help identify when program goals have been
met and whether changes need to be made (in activities or allocation of resources) to meet
program goals.
Informing policy and funding decisions. By helping the SBA understand the role of an individual
program in its broader policy toolbox, program evaluations help SBA' 's senior leadership allocate
resources and set priorities. SBA program managers that can demonstrate a link between program
activities and outcomes through objective evaluation are more likely to receive continued support.
Engaging SBA employees. By involving SBA employees in the development of evaluation
questions, program evaluations create an opportunity for employees to engage more in their
program's operations and delivery.
Stakeholders are increasingly interested in ensuring that programs are adequately evaluated to determine whether
they are well designed and effective. Program evaluation is essential for learning about programs and improving
them. Evaluations can produce data needed to respond to and answer key questions and accountability
demands.
Logic Model
Performance
Program Evaluation
Measurement
Tool/framework that
helps to identify the
Helps track what
Helps explain why you
program resources,
level of performance
are seeing the results
activities, outputs,
the program
target audience, and
achieves
outcomes
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
21
Chapter 3: Program Evaluation Role in Performance Management
Program evaluation is one component of a performance management system. Performance management
systems include logic models, performance measurement, and program evaluation. Together, performance
management activities ensure that SBA programs are meeting their goals effectively and efficiently.
Logic modeling, performance measurement, and program evaluation work in a dynamic system. The logic
model provides a framework that will help you clearly understand and communicate how your program's
activities, outputs, and outcomes connect to your long-term goals. Performance measurement involves
ongoing monitoring and reporting of the program's progress and accomplishments. Program evaluation builds
on these tools as a formal assessment that examines and draws conclusions about the effectiveness of a
program's design, implementation, and impacts. The SBA can then apply the results of program evaluations to
improve program operations and service delivery.
Performance Measurement Vs. Program Evaluation
Imagine you just bought a new car. Both the salesperson and the owner's manual
indicate your car should get 30 miles per gallon of gas. Well, it has been six months,
and you have kept meticulous records. You notice your car has only managed to get 20
miles per gallon. What do you do? You take the car back to the dealership and ask the
mechanic to determine why the car is not meeting the specified performance standard.
The mechanic finds a problem with the engine, fixes it, and you drive off with a better
functioning car.
The gas mileage records are the performance measurement part of the equation, and
the mechanic's diagnosis is the program evaluation. This scenario is an analogy of the
differences and relationships between these two tools as applied to SBA programs.
Program evaluation uses data on program performance to assess why results are occurring. Therefore,
collecting data on program performance is an important component of program evaluation. Your program may
already collect performance data for other purposes (e.g., tracking metrics for annual performance reporting). If
your program has not identified or collected performance data, you must include this task as part of your
evaluation process. The program logic model, described in Chapter 7.C, will help to identify potential
measures. If you have already developed a logic model for your program, you do not need to develop a
different one for the evaluation. Instead, you should regularly review your existing logic model and make any
necessary updates or revisions.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
22
Who Should Use These Guidelines?
Not everyone at the SBA is, or is expected to be, an expert in program evaluation. Many people are
evaluation users; they have limited knowledge of program evaluation but benefit from, see the value of, and
might be called on to participate in the evaluation process occasionally. Others are lead program evaluators
who have an in-depth knowledge of program evaluation and advise, manage, or conduct evaluations.
Although lead program evaluators are capable of planning and managing an evaluation without external aid,
the SBA uses external program evaluators to conduct evaluations to ensure the objectivity and credibility of
evaluation studies. These guidelines primarily support evaluation users. As users:
Program managers are responsible for determining how their programs should be evaluated,
what components of their programs could benefit from an evaluation, and when an evaluation
should take place. Although managers need not have the technical expertise to conduct an
evaluation, knowledge of the basic steps in the evaluation process will help inform decisions that
must be made when commissioning evaluations and using evaluation findings to make
managementdecisions.
Program staff are often part of the program evaluation team and, as such, are responsible for
participating in the program evaluation. They will benefit from having a basic understanding of the
program evaluation concepts and techniques that they may encounter during an evaluation. This
background will allow them to be able to "speak the same language" as the seasoned evaluators
on their team.
For additional information on evaluation stakeholders, Chapter 7.B includes the complete list of evaluation
roles and responsibilities.
How to Use These Guidelines
At its most sophisticated level, program evaluation can be a very complex discipline with practitioners devoting
entire careers to narrow aspects of the field. These guidelines do not assume that you are such an expert, nor
do they aim to make you one. They are intended to introduce the novice to the world of program evaluation
and walk you through a step-by-step framework for how to design and conduct an evaluation of an individual
SBA program. They are designed to enable you to work more effectively with an external program evaluator.
We have included actual examples of SBA programs to help illustrate the concepts described.
Guidelines Roadmap
Before starting a program evaluation, you should become familiar with the key steps in the process. These
guidelines are organized into the next three chapters that reflect each of these steps. While the framework
appears to be linear and sequential, you and your evaluator are likely to revisit one or more of these steps.
Chapter 4: Plan and Prepare
Chapter 5: Conduct and Monitor
Chapter 6: Disseminate and Implement Findings
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 23
Chapter 4: Plan and Prepare
4.A. Plan the Evaluation
4. Plan and Prepare
Four key considerations influence how you plan for an evaluation:
A. Plan the Evaluation
B. Identify Key Stakeholders
Choosing the right evaluation
Budgeting for an evaluation
C. Develop or Update the
Developing an implementation plan
Program Logic Model and
Learning Agenda
Anticipating potential data limitations and stakeholder
concerns
D. Develop Evaluation Questions
If evaluation planning is incorporated into the design of a program,
evaluation costs can be far lower and the quality of the final
5. Conduct and Monitor
evaluation much higher.
A. Set an Evaluation Design
Although evaluations can be designed and conducted once a
B. Implement the Evaluation
program is in operation, doing so may result in higher costs, fewer
options, and decreased capacity to obtain good answers to important
6. Disseminate and Implement Findings
program questions.
A. Communicate Evaluation Results
Choosing the Right Evaluation
B. Implement Recommendations
Program evaluations help assess effectiveness and lead to recommendations for changes at all stages of a
program's development. The type of program evaluation should align with the program's maturity and be
driven by your purpose for conducting the evaluation and the questions that you want to answer. There are
10
several types of evaluations, including, but not limited to:
Design evaluation, sometimes called formative evaluation, seeks to assess whether the program will
operate as planned. It is appropriate to conduct a formative evaluation during a program development
process or during a program redesign process. Evaluating a program's design can be very helpful for
developing an effective SBA program if 1) program goals are less clearly defined, 2) only a few staff
members were charged with developing the program, or 3) uncertainties exist about a program's intended
activities. On the other hand, evaluating a program's design might not be necessary if you have a robust,
inclusive, and clear program development process.
Process evaluation is typically a check todetermine if all essential program elements are in place and
operating successfully. This type of evaluation is typically conducted after a program is running for a
period. Process evaluations can also be used to analyze mature programs undersome circumstances;
such as when you are considering changing the mechanics of the program, or ifyou want to assess
whether the program is operating as effectively as possible. Evaluating a program's process usually is
10
References to evaluation types can vary, but the types and definitions discussed here are quite common in the evaluation field.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
24
not necessary in the early stages of an SBA program if 1) early indicators show that the program is
being implemented according to plan, and 2) program managers and stakeholders are confident that a
program's implementation is on target.
Outcome evaluation looks at programs that have been up and running long enough to show results and
assesses their success in reaching their stated goals. Program outcomes can be demonstrated by
measuring the correlations that exist between program activities and outcomes after you have controlled for
all the other plausible explanations that could influence the results you observe. However, correlation does
not imply causation. Outcome evaluation can tell you that your program likely contributed to the
outcome, but to demonstrate that your program has definitively caused the results you observe you
would need to conduct an impact evaluation. Outcome evaluations are appropriate when baseline, and
post-baseline, data sets are available or could be developed. Baseline data are initial information on a
program or program components collected before receipt of services or participation activities.
Outcome evaluations can also be undertaken if you are interested in determining the role, if any,
context plays; or if your program is producing unintended outcomes. For example, you may discover
your program is achieving distinctive results in different areas or with different populations. However,
outcome evaluations are not appropriate when the program is new.
Impact evaluation is a subset of outcome evaluation that focuses on assessing the causal links
between program activities and outcomes. This evaluation is achieved by comparing the observed
outcomes with an estimate of what would have happened in the absence of the program. While an
outcome evaluation can identify that goals have been met, an impact evaluation identifies the reason
that the goals have been met and that these results would not have been achieved without the
program. This is often referred to as measuring attribution. When attribution cannot be quantified with
any degree of certainty (e.g., if counterfactual data do not exist), evaluators are often able to
characterize a program's contribution to the outcomes realized through analysis of existing or
collected data, and sometimes through triangulation of findings across multiple methods.
In these cases, evaluators clearly describe the limitations of their analysis, including other factors that
could be contributing to the outcomes identified, and why there is any confidence that the program is
contributing to the outcomes. Impact evaluations can be conducted at two phases in a program's
lifecycle. First, they can be conducted as part of the piloting stage to determine if a programmatic
approach should be expanded into a full-scale program. Second, they can be conducted on mature
programs to determine whether a program is having the intended behavior change and/or economic
result. Causal claims can be made when a program is subjected to a randomized control trial (RCT),
where one group receives the program's services and one group does not. Even when an RCT is
completed, the samples may not be large enough to be generalizable to the population. Thus, while it
is rare to be able to ascertain causality in social science research, quasi-experimental designs may
suggest casual relationships and small-scale non-experimental designs can provide valid and reliable
evidence to build the body of evidence needed to inform decision-making. See the methods
discussion in Chapter 5 for more information.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 25
Working with a Program Evaluation Contractor
Program evaluation contractors provide important outside perspective and expertise.
Evaluation contractors are selected by an expert panel based on the panel's review of a
contractor's proposed methods for completing the evaluation, cost, and qualifications as
directed by the FAR. Use these tips for working with a program evaluation contractor:
Work with the contractor to facilitate data collection from internal and external
evaluation stakeholders. This step can cut the cost of an evaluation greatly,
increase the response rate, and reduce the frustration of program participants.
Promote the active involvement of the SBA program staff. Doing so will
lead to a better report that is more likely to meet the needs of the
program with recommendations that are more likely to be implemented.
Have an explicit and documented agreement with the contractor about steps that
will be taken to ensure objectivity (e.g., peer review).
Be clear about who will make final decisions about how the program and
the contractor will share information about the evaluation process, draft
evaluation products, and final evaluation reports or briefings.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
26
Four Types of Program Evaluation
Type
When to Use
What it Shows
Why it's Useful
Design
During
Identifies needs that the program
Informs program
Evaluation
program
should address (e.g., is the program's
design and
development
approach conceptually sound?)
increases the
likelihood of
success
Process
As needed
How all essential program elements
Allows program
Evaluation
after the
are in place and operating (e.g., how
managers to check
program
well are the program's activities being
how program
development
implemented?)
plans are being
stage
implemented
Outcome
After program
The extent to which a program has
Provides evidence
Evaluation
has been
demonstrated success in reaching its
of program
implemented
stated short-term and intermediate
accomplishments
for a
outcomes after you have ruled out other
and short-term
reasonable
plausible rival factors that may have
effects of program
period
produced program results (e.g., to what
activities
extent is the program meeting its short-
and intermediate-term goals?)
Impact
Both during the
Causal relationship between program
Provides evidence
Evaluation
pilot stage and
activities and outcomes (e.g., did the
that the program,
with mature
program's activities cause its long-term
and not outside
programs
goals to occur?)
factors, has led to
the desired effects
SBA Evaluation Proposal Process
SBA's centralized evaluation proposal process follows an annual call for evaluation proposals. This call grows
from discussions with individual programs aimed at identifying questions that, if answered, could lead to
performance improvements. In addition to engaging with the Office of Program Performance, Analysis and
Evaluation throughout the year to discuss evaluation possibilities and to receive technical assistance, programs
interested in program evaluation supportfrom the Analysis and Evaluation Division may submit a proposal to
the Office of Performance Management and the Chief Financial Officer during the annual solicitation period in
spring. Appendix F contains the Program Evaluation Proposal Template. The lead program evaluator will
convene a team of evaluation and program experts to consider the proposals, and the team will make
recommendations to the Chief Evaluation Officer and the Performance Improvement Officer about which
proposals to support in that evaluation cycle. The SBA aims to complete evaluations within 12 to 15 months.
However, evaluations requiring clearance from the Office of Management and Budget (OMB) for primary data
collection may take longer (see Chapter 5 or a discussion of the Paperwork Reduction Act and OMB's role in
overseeing primary data collections).
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 27
Budgeting for an Evaluation
Conducting an evaluation can take considerable time and incur significant expense. Budgets required for
evaluations vary widely depending on the scope and scale of the program, the type and complexity of
evaluation questions, the evaluation design, and the availability of existing data. The Office of Program
Performance, Analysis and Evaluation and other agency evaluation practitioners can help you estimate a
budget based on your program's unique evaluation goals.
The size and scale of your SBA program is likely to drive many of your budgeting considerations. For example,
large programs with multiple partners might require designs that allow for a comparison of data from unique
subgroups involved in the program's efforts. Some programs might be able to take advantage of already
existing administrative data sets, potentially in conjunction with IRS or Census data. Costs of using preexisting
data can vary, but sometimes data can be accessed quickly at low or no cost.
Costs for independent contractors (as described earlier in this chapter) typically dominate the cost of the
evaluation and should be considered when budgeting for an evaluation. If you need to collect new data or
improve the quality of existing data, you should budget additional time and money. The more complicated the
data collection and analysis, the more expensive the evaluation will be. A qualitative analysis based on interview
or focus group data, for example, can be time-consuming and expensive. A smaller budget will limit the
sophistication of any new data collection methods and the statistical analyses. As we point out throughout this
document, however, there are several ways you can answer your evaluation questions. These alternate design
options may fit within your time and fiscal constraints while still providing information useful for your program.
The SBA currently allocates funds for evaluation from a centralized evaluation budget and cost sharing from
individual programs that reserve funds for evaluation activities. The SBA also uses creative options for
implementing evaluation activitiessuch as through relationships with academic institutions, think tanks, and
other non-governmental organizations (NGOs).
Finally, you should ensure that you have management support to authorize the reallocation of internal
resources (i.e., time, funding, staff time) to support the evaluation effort.
Implementation Plan
An evaluation implementation plan includes specific evaluation tasks, the associated deliverables, and a
timeline for conducting the evaluation. The plan helps hold the evaluation team accountable and ensures that
an evaluation promptly produces the anticipated outputs.
Anticipating Potential Data Limitations and Concerns
You should be aware of potential challenges that SBA programs often face related to program evaluation.
These include limitations in identifying existing data resources, barriers to collecting new data, and methods to
address concerns. These barriers are typical to all program evaluations but anticipating them up-front can help
you prepare for and overcome them. In the following sections, we describe these challenges in more detail and
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
28
provide tips for addressing them.
Identifying Existing Data Sources
Ideally, your program should have been collecting performance data since it began, and those data can be
easily used to evaluate the program. The table that follows highlights opportunities to leverage performance
management for program evaluation. As discussed in more detail in Chapter 2, however, you might discover
that you do not have the right type of data needed to conduct the evaluation. If this is the case for your
program, first look to see if the data you need were already collected by another source, such as studies and
reports by other organizations (e.g., Government Accountability Office, SBA's Office of Inspector General). You
and your evaluator can also use information from a readily available source such as a public database or
company reports. A surprising amount of data are collected on thousands of topics, and the key is often in
knowing where to look and remaining persistent. Be aware of how the data are collected, however, and that the
organizations collecting the data might define terms differently than you do. These issues can affect data
quality and validity (or the extent to which a data collection technique accurately measures what it is supposed
to measure), as discussed in more detail in Chapter 2.
Connections Between Performance Management and Program Evaluation
Reporting
Purpose
Leveraging Performance Management for Program
Product
Evaluation
Strategic
Serves as the long-term blueprint
Work with program offices to identify existing evidence to
Plan
for accomplishing the Agency's
justify strategies and programs and develop an Enterprise
mission and priorities. Developed
Learning Agenda.
every four years and connects
Include performance evaluations in Strategic Plan under
SBA's mission with its programs/
program evaluations with an excerpt on how evaluation
activities and defines long-term
efforts further the Strategic Plan.
outcomes through strategic goals,
Evaluation team identifies potential evidence to be used
objectives, and measures. Sets
by performance liaisons as they discuss accomplishments
the framework for annual
and challenges.
planning, budgeting, and
Identify gaps in evidence to find or build.
accountability.
Annual
Serves as the annual plan that
Demonstrate where evidence is used to justify strategies,
Performance
justifies strategies, initiatives,
initiatives, programs, activities, and performance metrics.
Plan/Report
programs, and activities that
Integrate evaluation and evidence.
(APP/APR)
further the Strategic Plan.
Performance analysts describe completed, planned or
ongoing program evaluations with the APP/APR.
Quarterly
Reviews of quarterly progress on
Demonstrate evidence that shows progress or justifies
Deep Dives
performance and promotes senior
strategies. Identify areas for new evaluations.
management accountability to
Programs present on milestones achieved through
drive progress.
program evaluation to help inform performance measures.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
29
Strategic
Annual assessments of strategic
Demonstrate new or existing evidence that connects
Objective and
objectives and program portfolios
inputs, outputs, and outcomes and describes how program
Portfolio
that use performance data and
evaluation is furthering the strategic objectives and
Reviews
other evidence.
contributions to the program portfolios.
Identify programs for potential evaluations.
Use the Strategic Objective Summary Findings and
Portfolio Reviews to inform the Enterprise Learning Agenda
by coordinating with performance analysts.
Performance
Summarize the quality of existing
Explore the value of adding process, outcome, and customer
Measures and
performance measures in terms
satisfaction measures.
Data Quality
of validity, reliability, and
Review logic models and identify useful measures.
Review
utilization.
Collecting New Data
In some cases, existing data sources might be inadequate for your evaluation needs or have quality issues that
cannot be overcome. In this scenario, you will need to develop new data. To collect new data, research SBA
programs (e.g., through the Evidence Registry or the SBA PEER website) that have previously been evaluated
to identify examples of the types of data gathered and to determine how these programs handled similar
challenges. When you are ready to collect new data, the Paperwork Reduction Act might require you to obtain
an Information Collection Request (ICR). Chapter 2 goes into greater depth on navigating the ICR process and
the Paper Reduction Act.
Anticipating and Addressing Concerns
Several consumers of your evaluation may have concerns you will need to address proactively throughout the
evaluation process.
Internal SBA Concerns. First, you must anticipate the concerns of the primary consumers of your evaluation,
those most closely involved in the program, program staff, managers, and SBA senior leadership. Apprehension
about program evaluation is not unique to SBA programs. Program evaluation is often associated with external
accountability demands. The program staff might feel pressured to show results, yet often feel unprepared for
program evaluations. The section that follows presents common concerns and responses to consider.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
30
Evaluation Concerns and Responses to Consider
Target Audience Concerns. The target audience of the program might be apprehensive about evaluation as
well. To address any concerns, discuss the goals and purpose of the evaluation with program participants, and
emphasize that the objective is to improve program function. Provide clear information to participants on:
How the evaluation results will be used.
The level of data transparency (e.g., whether individual participant data will be identified in the
evaluation report or if the data will be aggregated across participants in a way that preserves
confidentiality).
How confidential business information such as firm revenues will be treated (if applicable).
In addition, consider these ideas for involving the program's target audience in the evaluation process:
Involve stakeholders as you develop evaluation questions (discussed later in this chapter).
Continue to involve a subset of program participants and staff throughout the evaluation to help
address concerns and increase the extent and reliability (i.e., the extent to which a measurement
instrument yields consistent, stable, and uniform results over repeated observations or
measurements under the same conditions) of any new information collected (discussed later in
this chapter).
Consider ways to minimize data collection burdens faced by participants and staff throughout the
course of the evaluation by making the best use of existing data and only asking questions that are
relevant to evaluation objectives (discussed in Chapter2)
Provide participants with timely results and feedback (discussed in Chapter 3).
Public Accountability Concerns. Finally, governmental oversight bodies and key public stakeholders often
look to program evaluation as a means of verifying that programs are achieving their intended long-term goals
and thus using taxpayer money effectively. Some parties think that impact evaluations, because they are the
only type of evaluation design capable of making causal links between programs and their long-term goals, are
the only type of evaluations worth conducting. Impact evaluations-which by design, demonstrate a
program's definitive causal effect-should be undertaken whenever possible. However, program staff,
managers, and stakeholders should understand that demonstrating a program's causal effect through a
rigorous impact evaluation often cannot be realistically achieved without a substantial (and often
overwhelming) investment.
As stated earlier, impact evaluations are most easily undertaken when the evaluation approach has been
written into a program's design. Undertaking an impact evaluation after a program's implementation can be
considerably more challenging. Principal barriers to conduct impact evaluations are: 1) fiscal and staffing
limitations, 2) the inability of programs to control or account for the external factors that work in tandem with
programs to produce long-term economic outcomes, and 3) the difficulty of collecting data from non-
participants (necessary to form control groups). Further, questions of impact are not the only questions of value
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 31
to programs. Consult with expert evaluators and program stakeholders to determine what type of evaluation
design is the most viable and useful option for your program.
Planning the Evaluation: The B2B Experience
Boots to Business (B2B) is an entrepreneurial education and training program offered by the SBA
as a training track within the Department of Defense's revised Transition Assistance Program
(TAP). In preparation for an ongoing evaluation, the B2B program developed a comprehensive
evaluation proposal including a learning agenda and logic model table (discussed later in this
chapter). In response, staff from the Office of Performance Management conducted additional pre-
scoping exercises with B2B to help further refine their evaluation questions and identify potential
evaluation designs and methods for answering those questions.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
32
4.B. Identify Key Stakeholders
Who Should Be Involved in Evaluations of SBA Programs?
A key step to evaluation involves identifying stakeholders through
a stakeholder involvement plan. These guidelines broadly define
4. Plan and Prepare
a stakeholder as any person or group who has an interest in the
A. Plan the Evaluation
program being evaluated or in the results of the evaluation.
Incorporating a variety of stakeholder perspectives in the planning
B. Identify Key Stakeholders
and implementation stages of your evaluation will provide many
C. Develop or Update the Program Logic
benefits, including:
Model and Learning Agenda
Fostering a greater commitment to the evaluation
D. Develop Evaluation Questions
process; and
5. Conduct and Monitor
Increasing the chances that findings and
recommendations based on evaluation results are
A. Set an Evaluation Design
implemented.
B. Implement the Evaluation
6. Disseminate and Implement Findings
To foster the desired level of cooperation, you should first identify
relevant stakeholder groups and then determine the appropriate
level of involvement for each group. The remainder of this
A. Communicate Evaluation Results
chapter discusses these steps in more detail.
B. Implement Recommendations
Identifying Relevant Stakeholders
Identify and engage the following principal groups of internal and external stakeholders:
People or organizations involved in program operations are entities designing and
implementing the program and collecting performance information. These entities could also include
field operations staff, sponsors, collaborators, coalition partners, funding officials, and program
managers. This group plays an important role in providing the "boots on the ground" perspective.
People or organizations served or affected by the program might include the program's
target audience, academic institutions, elected and appointed officials, advocacy groups, and
community residents.
Primary intended users of the evaluation results are the individuals able to decide and act
with evaluation results. They include program managers and senior leadership. This group
should not be confused with primary intended users of the program itself, although some overlap
can occur.
Secondary intended users of the evaluation results are also individuals able to decide and act
with evaluation results but for an office or program in the SBA that is not the focus of the evaluation.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
33
SBA's use of enterprise learning agendas (discussed more in Chapter 7.D) emphasizes
opportunities to apply lessons learned from an evaluation across the Agency. Potential secondary
users may be identified from programs that conduct similar activities or programs that contribute
to the same strategic goal and/or objective area outlined in the Strategic Plan.
Agency planners are people, such as key regional and program office liaisons, who support all
aspects of planning and accountability.
Role of Stakeholders
Overall, the SBA favors evaluations where program stakeholders play a role. Involving principal stakeholders in
the evaluation from the beginning is important for fostering their commitment to the evaluation design, and
ultimately the evaluation findings and recommendations. To involve stakeholders, use face-to-face meetings,
conference calls, and/or electronic communications. Choose a method or combination of methods that works
best for the people in the group.
Continued feedback from stakeholders throughout the evaluation process will help to ensure that the
evaluation remains on track to produce useful results. The scope and level of stakeholder involvement will vary
for each program evaluation and stakeholder group, however, and keeping the size of the group manageable
is important. Stakeholder involvement in program evaluation is often iterative. First, your lead program
evaluator should work closely with you on managing stakeholder involvement throughout the program
evaluation process.
Secondly, the level of stakeholder involvement may also be based on the need for external objectivity in the
evaluation. For evaluations where impartiality is paramount, program staff would have less involvement in
evaluation design and implementation. Objectivity might have greater importance in a variety of situations that
are not necessarily unique to programs, such as accountability demands from Congress, the GAO, or OMB.
Furthermore, involvement from program stakeholders may be an especially useful way to alleviate fears when
trust is an issue and is useful for programs that find themselves in a defensive posture due to repeated
criticism and heightened scrutiny.
Planning the Evaluation with Stakeholders
Before designing the evaluation, ensure that all participating stakeholders understand the purpose of the
evaluation and the process: have a conversation with all parties, explaining obligations and expectations of
each party (including informal and implicit expectations). Any conflicts of interest should be addressed openly
at this stage, so as not to compromise the reliability and credibility of the evaluation process and results. When
designing the evaluation, involving as many stakeholders in the initial discussions as possible is good practice.
Continue to consult and negotiate with stakeholders; solicit their reactions to the program logic model (Chapter
7.C) and evaluation questions (Chapter 7.D). Consult and negotiate with stakeholders to establish an
agreement on key data (e.g., including how to select measures, how to measure program impacts, how to set
a baseline and use baseline data, and how to ensure data quality throughout the evaluation process).
From the wider group of stakeholders, select a manageable subset of stakeholder representatives to join the
core evaluation team to help make ongoing decisions about the evaluation. Continued use of this team
throughout the evaluation process will help keep the evaluation focused, alleviate concerns, and increase the
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
34
quantity and quality of information collected. See Appendix C for roles and responsibilities of the program
evaluation team.
Incorporating a Variety of Perspectives
In addition to the principal groups of stakeholders, consider inviting someone to play the role of "devil's
advocate." A skeptic, or someone in the core evaluation team who will challenge assumptions, can strengthen
an evaluation's credibility by ensuring that all decisions and assumptions are thoroughly examined. Try to
identify a program staff person or other individual with knowledge of the program who will ask tough, critical
questions about the program and evaluation process, or someone on the evaluation team can play this role.
Above all, remember that the goal of the evaluation is to produce findings that can be used to improve the
program. Common sense dictates that an evaluationprocess involving the individuals involved in the program
will produce findings that are relevant and useful. Therefore, plan, conduct, and report the evaluation in a way
that incorporates stakeholders' views and encourages their feedback, thereby increasing the likelihood that key
stakeholders will act upon findings.
Participatory Evaluation
Consider implementing a full participatory evaluation, which involves stakeholders in all aspects of the
evaluation, including design, data collection, and analysis. A participatory evaluation will help you and your
evaluator to:
Select appropriate evaluation methodologies;
Develop evaluation questions that are grounded in the perceptions and experiences of clients;
Overcome resistance to evaluation by participants and staff; and
Foster a greater understanding of the evaluation among stakeholders
A participatory evaluation is resource-intensive and not a good fit for every program. You and your evaluator
might choose instead to elicit broad stakeholder input only at key points, consider this input carefully, and be
transparent in decision-making. Key points include developing or reviewing the program logic model,
formulating evaluation questions, developing the evaluation methodology, reviewing the draft evaluation
report, and disseminating findings.
Disseminating and Implementing Findings to Stakeholders
Report findings are considered complete and suitable for public dissemination when they receive approval by
the respective program office Associate Administrator, the Chief Evaluation Officer and the Performance
Improvement Officer. The distribution of the final report with outside parties is prohibited until official clearance
is obtained from the Associate Administrator of the respective program office involved in the evaluation and the
Performance Improvement Officer. Once these clearances are obtained, the SBA will provide a copy to the
Office of Congressional and Legislative Affairs and post the report on its website. At this stage, the findings
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
35
and recommendations should be communicated to the stakeholders identified during the evaluation planning
stages.
SBA evaluation should include a recommendations implementation plan to ensure recommendations are
acted upon. At this stage, the evaluation's executive champion is essential for providing the support to follow
through on evaluation recommendations.
Identifying Key Stakeholders: Women's Business Centers
Women's Business Centers (WBC) provide a variety of services uniquely tailored to meet the
needs of communities they serve to help entrepreneurs explore and achieve economic
independence through business ownership. Over 100 non-profit centers provide free business
advice and no- or low-cost training to established businesses in almost every state and U.S.
territory. The SBA engaged stakeholders to identify the best method to evaluate program
outcomes. Through meetings with SBA program staff, SBA leadership, WBC staff, and the
national Association of Women's Business Centers, the SBA designed an evaluation that
addressed stakeholders' concerns and provided meaningful outcome data. After completing the
evaluation, the SBA held meetings with these stakeholder groups to discuss findings and
recommendations and to foster a collaborative environment.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
36
4.C. Develop or Update the Learning Agenda and Program Logic
Model
What is a Learning Agenda and Why is it Important?
A learning agenda is a continuous improvement program tool
that creates a structure for a program to consider its evaluation
4. Plan and Prepare
priorities. The learning agenda template is comprised of a broad
A. Plan the Evaluation
set of questions related to the work that a program conducts
B. Identify Key Stakeholders
and the challenges it faces. This tool guides program managers
and staff to address the questions using evaluative approaches
C. Develop or Update the Program
and evidence that inform decision-making, ultimately increasing
Logic Model and Learning Agenda
program efficiency and effectiveness. A learning agenda guides
D. Develop Evaluation Questions
program staff through multiple steps to identify program-related
information or data gaps, approaches for filling these data gaps,
5. Conduct and Monitor
and how the findings can be used to improve the program.
A. Set an Evaluation Design
Steps include:
B. Implement the Evaluation
6. Disseminate and Implement Findings
1.
Identify relevant stakeholders.
2.
State the program's goals and objectives.
A. Communicate Evaluation Results
3. Formulate guiding questions that, if answered, would
B. Implement Recommendations
improve the program or build evidence of the
program's impact. These questions may be based on the program's challenges and should
encompass both short-term and long-term program outcomes.
4.
Prioritize the guiding questions.
5.
Develop a plan for answering the guiding questions.
6.
Implement the plan.
7.
Evaluate the findings.
Learning agendas are designed to improve program implementation by using existing evidence about program
performance, generating new knowledge about the program, and fostering innovation. Overall, learning
agendas help support a culture of learning and facilitate rapid iterative program corrections from staff and
partners at the SBA. An example template for developing a learning agenda is below.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 37
Learning Agenda
Elements
Program Office
Briefly describe the primary functions
What are the most important challenges currently facing your team?
Please identify goals of the program office for the upcoming year.
The next section will help you form critical questions that, if answered,
would improve the functionality or build evidence for the impact of your
program. Keep the above challenges and goals in mind when formulating
questions. For each critical question:
Please identifyyou question.
What stakeholders are involved with answering your question?
What results do you expect from the study?
How would you use theresults?
What data collected, if any, can be used to answer your question?
Do you anticipate any limitations for the study? (e.g.,
programmatic implementation, etc.)
SBA's vision for learning agendas involves working with all program offices to determine program-specific
priorities and questions identified across programs. An enterprise learning agenda focuses evaluation activities
by prioritizing evaluation questions that will have the greatest usefulness across the SBA. For example, the
Office of Veterans Business Development may have counseling-related questions about technical assistance
that could inform the Office of Entrepreneurial Development's program implementation. This approach will
produce an enterprise learning agenda that will focus the Agency's evaluation efforts, in alignment with the
strategic plan and other performance management activities.
Using the Learning Agenda in the Evaluation Process
Programs can complete and use learning agendas to facilitate self-examination and self-improvement
processes independently of a formal evaluation. However, if considering an evaluation, you should first
complete a learning agenda. In conjunction with a logic model, this serves as a tool to help generate a set of
evaluation questions and identify potential data sources.
Why Is a Logic Model Important for Program Evaluation?
A logic model is a diagram and text that shows the relationship between your program's work and its desired
results. Every program has inputs (or resources), activities, outputs, customers (or audiences), and desired
outcomes; a logic model describes the logical (causal) relationships among these program elements.
Understanding your program clearly is essential for conducting a quality evaluation, as it helps to ensure that
you are measuring the right indicators and asking the right questions. Whether reviewing an existing logic
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 38
model or creating a new one, accurately characterizing the program through logic modeling is important because
it ensures that program managers, contractors, and other stakeholders involved in designing the evaluation fully
understand the program. These guidelines provide a simple approach to logic modeling, but more complex
logic model approaches could be used (e.g., theory of change diagrams). The logic model terms and
definitions described here provide a basic framework that can be used across the variety of logic model
approaches.
Logic Model Elements
A logic model has seven basic program elements:
1. Inputs/Resources - what you need to run your program (e.g., people and dollars)
2 Activities - what your program does
3. Outputs - the products/services your program produces or delivers
4. Customers/Audiences - those groups whose behavior your program aims to affect
5. Short-Term Outcomes - changes in decision-makers' knowledge, attitude, or skills
6. Intermediate-Term Outcomes - changes in the decision-makers' behavior, practices, or decisions
7. Long-Term Outcomes - changes in the economy because of your program
Also included in logic models are external influences (i.e., factors beyond your control), such as state
programs that mandate or encourage the same behavioral changes as your program and other
circumstances (positive or negative) that can affect how the program operates. Logic models also often
include assumptions about how your program operates. The diagram below shows a high-level logic
model that may be a useful starting point for developing your own. Boxes and arrows represent the logical
connection between the separate program elements. SBA program evaluation training includes additional
information to help you through the process of developing a logic model for your program.
HOW
WHY
Resources/
Activities:
Outputs:
Customer/
Short-term:
Intermediate:
Long-term:
Inputs:
Audience:
Investments
Activities
Product or
User of the
Changes in
Changes in
Change in
available to
you plan to
service
products/service
learning.
behavior,
economic
support the
conduct in
delivery/
S; target
knowledge,
practice, or
conditions
program
audience the
your
implementatio
attitude, skills,
decisions
program
n targets you
program is
understanding
aim to produce
designed to
reach
Program
Results from Program
External Influences:
Factors outside of your control (positive or negative) that may influence
the outcome and impact of your program/project
The next section includes a set of logic models completed at the SBA.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
39
Logic Model for Program Evaluation at the U.S. Small Business Administration (SBA)
Mission: Conduct evaluations, and enable partners to more effectively conduct program evaluations and analyses that inform management
decisions, enhance organizational learning, promote innovation, and foster economic results.
Short-term
Intermediate
Long-term
Customer
Outcomes
Outcomes
Outcomes
Inputs
Activities
Outputs
Reached
(Awareness)
(Behavior)
(Condition)
Lead Program
Develop Program
Evaluator
Evaluation
Budget for program
Framework
evaluations (PE)
Develop a
SBA Program
Managers and staff
Evaluations
Performance
framework to
Evaluation
value PE
reported in
Management Office
conduct program
Framework
Value of innovative
Government
Evaluation culture
staff
evaluation at U.S.
Evaluation
Performance and
established
Performance
SBA
presentations
Program managers
projects/core
programs
Results
Evaluation results
Improvement
Train and mentor
Fact sheets
Headquarters staff
demonstrated
Modernization
used to improve
Officer (PIO) and
Management
Field office
Agency staff on
Deputy
program evaluation
managers and staff
. PE is incorporated
Annual Report
programs and
briefings
increase
further advance
into the
Performance
Brief PIO and DPIO
PE Reports
Performance
Evaluations inform
SBA mission
Improvement
and other senior
Guidelines for
Management
management
Officer (DPIO)
managers
Evaluating an SBA
System
decisions
Presidential
Conduct measures
Program
Management
review incorporate
Fellow (PMF)
PE in performance
GEARS contracting
reporting
team
. Manage
Community of
Practice
Increased
Design and develop
PE/PM training
knowledge of
Evaluations
program evaluation/
courses and
PE/PM
conducted and
.
Quality of
performance
materials
PIO/DPIO
Increased skills to
managed increased
evaluations
management
Trainings delivered
Senior
manage and
PE skills are used
managed and
Community of
(PE/PM) training
to SBA managers
management
conduct evaluations
internally
conducted is
Practice (COP) staff
courses and
and staff
35-40 programs at
Increased skills to
Staff develop useful
improved
materials
Briefings on
SBA
develop
performance
Deliver training
Evaluation
performance
measures
Solicit feedback on
Framework
measures
SBA Program
Evaluation
Framework
Participate in
discussions with
Knowledge sharing
Community of
. Federal partners
Federal partners on
through discussions
Practice
evaluation
External Influences: Strategic Priorities, Evidence Based Policy Making in the Federal Government
40
Developing the Program Logic Model: The All Small Mentor ProtÃ©gÃ© Program
SBA managers and staff in the All Small Mentor ProtÃ©gÃ© Program used the logic model process to develop a clearer picture of the links between the program's
elements and expected results. This process helped the staff prioritize among a wide range of potential evaluation measures.
SBA Office of Performance Management: All Small Mentor-ProtÃ©gÃ© Program (ASMPP) Logic Model
Mission: To provide streamlined access through Mentor-ProtÃ©gÃ© relationships for small businesses to enter into the federal acquisition marketplace
Short-term
Intermediate
Customer
Long-term
Inputs
Activities
Outputs
Outcomes
Outcomes
Outcomes
Reached
(Awareness)
(Behavior)
(Condition)
Develop SOPs, guidance,
SOPs
Increased knowledge on
BOSs and staff at District
Consistent and efficient
and instructions
how to implement the
District Office Staff (BOSs,
Office apply the SOPs
ASMPP implementation
ASMPP
Conduct SBA "in-reach" and
EDSs, DDs, LRSs)
SBA meetings and webinars
customer service
Increased brand awareness
BOSs and DO staff promote
for ASMPP (within SBA)
the program and recruit
potential participants
SBA Headquarters
Promote and market the
ASMPP to external
Conference presentations,
Increased brand awareness
Staff/Time (ASMPP and
audiences
webinars, "industry days'
for ASMPP (outside SBA)
Decision to apply to the
GCBD)
Small and large businesses
ASMPP (or not as
ASMPP attracts, engages,
Introductions made between
not (yet) in the ASMPP
Small and large businesses
appropriate)
and serves the target
Host match-making events
small and large businesses
(directly, or through industry
decide to pursue a
population of businesses
SBA District Office and
& trade groups)
Mentor/ProtÃ©gÃ© relationship
(including veteran-owned,
Field Office Staff/Time
Provide information
E-mails sent and phone
High-quality applications
Improved understanding of
HUBZone, and
calls conducted
from qualified businesses
ASMPP eligibility and
women-owned)
submitted to the ASMPP
application requirements
Review /process
Notification to applicant on
An Approved MP
Budget
applications
approval, decline, or
Program applicants
Applicants learn of their
relationship and formal
approval status
reconsideration request
partnership agreement
Mentor provides Business
Technology (e.g., Certify,
Access to small business
Development Assistance:
office technology)
set-asides
Management/Technical
Financial
Greater awareness of
Achievement of small biz
Contracting
Interactions between Mentor
Large Businesses (Mentors)
ProtÃ©gÃ© skills and
subcontracting goals
Governing Regulations: 13
Trade Education
and ProtÃ©gÃ©
Increased annual revenue
certifications
CFR 125.9 (program)/125.8
-Business Development
Access to diverse suppliers,
(JV); SOPs
.General and/or
skills, and expertise
Administrative
Mentor awards subcontracts
Improved profitability and
Resource Partners
Develop and provide
Program guidance provided
to ProtÃ©gÃ©
longevity
SCORE, SBDC, WBC,
technical assistance to
through e-mails sent and
PTAC, VBOC
Mentors and ProtÃ©gÃ©s
phone calls conducted
Mentor invests in ProtÃ©gÃ©
Increased capacity and
ability to qualify for more
Develop a framework for
Framework/guidance
Small Businesses
Enhanced business skills,
Mentor and ProtÃ©gÃ© create
contracts/requirements
Mentor-ProtÃ©gÃ© relationship
(ProtÃ©gÃ©s)
knowledge, and access to
a Joint Venture
Data collected through
opportunity
applications and contract
Establish and manage a
Community of practice
Mentor-ProtÃ©gÃ© compete
Increase in number of jobs
reports
community of practice
created and maintained
successfully for federal
supported by the companies
contracts
in the program
ProtÃ©gÃ© responds to
Review other Federal
Accept/Decline/Allow to
Other Agencies OSDBUs
additional business
Customer contacts
Continue other Federal
& Small Business Program
Increased diversity and
Agencies MP Programs
Other Federal Agencies
opportunities
competition in federal
Agencies
Staff
strengthen own programs
and/or refer potential
"Doing business differently":
contracting
Provide customer service to
Answer questions and
other Federal Agencies on
provide information about
Other Agencies Acquisitions
participants to SBA program
improved efficiency,
the ASMPP
staff, COs
effectiveness, and longevity
MP programs
Meet agency small business
ASMPP managers, SBA
Improved understanding of
contracting goals and MP
Manage data and
Management, legislature
ASMPP successes and
Data collected, stored,
Program requirements
information collected
and public entities
areas for improvement
through program
analyzed, and reported
Legislature and public
Increased support for what
Program's reputation as
entities
works; continuous learning
ASMPP is effective,
useful and successful
and improvement
efficient, and sustainable
SBA: State Trade Expansion Program (STEP) Grant Initiative Logic Model
Mission: Enable eligible small business concerns (ESBCs) to enter and succeed in the international market place.
Authorization The Trade Facilitation and Trade Enforcement Act of 2015 (HR 644)
Customer
Inputs
Activities
Short-term
Intermediate
Outputs
Long-term
Reached
Outcomes
Outcomes
Outcomes
SBA Office of
State/territory
Increased awareness
Develop funding
Funding opportunity
of STEP Grant
States/Territories
States/Territories
International Trade
governor-designated
opportunity
announcement
lead agency for trade
Initiative and
apply for SBA STEP
receive STEP Notice
(OIT)
announcement
released
and export activities
requirements
Program
of Award (NOA)
States/Territories
OIT budget
Review quarterly
Increased knowledge
receive STEP
of (1) adherence (or
Revise and re-submit
reports for accuracy,
Funding
completeness, and
lack of) to reporting
quarterly report
Feedback on
State/territory STEP
compliance with the
requirements in NOA
quarterly report
Awardee
Terms and Conditions
approved technical
(2) completeness of
proposal
reporting
Revise work plan and
Write-up on record
keeping; regulatory
Increased knowledge
budget estimate to
Conduct site visits
meet milestones
compliance; funds
of status meeting the
utilization; etc.
State governors
performance
State STEP
milestones
Implement
Directors
(expenditure of funds,
improvement efforts
Public
activities, etc.)
Consolidate and
Annual congressional
STEP Grant Initiative
analyze program
report published on
Congressional
Increased knowledge
Recommend changes
involves continuous
information and data
SBA.gov
Members
of STEP performance
to STEP
improvement efforts
Consolidate data on
SBA: U.S. Export
Increased knowledge
ESBCs in STEP
List of ESBCs in
Assistance Centers
of international small
Coordinate with STEP
Program
STEP Program
(USEACs)
businesses in need
ESBCs
State/ territory funds
Develop and promote
and in-kind
STEP program
Program application is
contributions
application
available online
Increased awareness
of program funding
ESBCs apply for
SBA funds
Coordinate with SBA
Contact with known
ESBCs
Grant from STEP
ESBCs receive STEP
and requirements
District Office
ESBCs in region
Awardee
Grant
from STEP Awardee
Ensure STEP ESBC
State/territory STEP
applicant meets
Determination of
Awardee
eligibility requirements
eligibility
Collect program data
Quarterly reports
SBA OIT
See the SBA OIT "Review quarterly reports" activity (above) for subsequent steps.
*Other Export Program
Procure outside
Partners*
International export
Increased knowledge
Strategies and plans
Increased number of
consultation services
of international
for exporting
Both internal to SBA
guidance and best
ESBCs
markets and buyers
product/service in
ESBCs exploring
and external
management
Participate in export
relevant for ESBCs'
relevant markets or
significant new trade
practices
training workshop
for specific buyers
opportunities
product/service
Design marketing
media
Increased number of
jobs retained/created
Participate in foreign
trade mission
Increased number of
ESBCs
Participate in a
ESBCs that export
reverse trade mission
New or additional
Increased awareness
Payment of website
exposure for
Large-scale foreign
of businesses that
Purchase ESBC
Increased value of
fees
product/service
buyers
meet their needs
product/service
ESBC exports
Participate in a trade
Develop partnerships
show exhibition
with ESBC
Other export
Solid arrows indicate the direction of flow in the logic
initiatives
model and reflect the most fundamental connection(s)
between items.
Department of
Subscription to
Dotted arrows indicate a potential flow between items
*Other Export Program Partners* may
External Influences: U.S. and global economic
Commerce (DoC)
services
include: SBA USEACs; SBA District Offices;
conditions; uncertainty of U.S. trade policy; State
based on the results from the previous item.
Small Business Development Centers; SCORE:
governors; State International Development Organization
Mini brackets indicate that an item is potentially
Women's Business Centers; Veterans Business
(SIDO); Congressional appropriations; U.S. House/Senate
applicable to multiple items in the following column and
Outreach Centers: Small Business Investment
Committees on Small Business and Entrepreneurship;
varies by STEP Awardee.
Companies; Certified Development Companies.
support from other SBA programs.
7(a) Loan Program Working Logic Model
Mission: Help small businesses start and grow through guaranteed capital financing
Inputs
Short-term
Intermediate-term
Activities
Long-term
Authorization-Small Business Act
Outputs
Appropriation Credit Subsidyand Loan
Outcomes
Outcomes
Outcomes
Administration
Develop regulations and SOPS
Regulations, SOPs, guidance produced
Greater lender knowledge of
Reducedrisk of ineligibleloans/fewer
loan processing and policy
Reduced oversight costs/increased
improper payments
Train loan center staff
availability of 7(a) loans on
Loan center staff trained
reasonable terms
Policy
Greater loan center knowledge of loan
Train OFO Lender Relation Specialists
processing and policy
Increased opportunities for emerging
OCA/OFA 7(a) Policy Staff and
OFO Lender Relation Specialists trained
markets
OFPO Center Operations Staff
Greater ability of OFO Lenders Relation
Train lenders
Specialists to advise lenders
Reduced (zero) loan subsidy
Lenders trained
Training and Marketing
Train and recruit lenders
More efficient processing of loans
Greater SBA resource partner
SBA resource partners trained
Greater capital access
knowledge of 7(a) loans
OFO Lender Relation Specialists
Train resource partners
More eligible entrepreneurs apply
Entrepreneurs trained and counseled
More businesses soperating/surviving
Greater entrepreneur knowledge of
for 7(a) loans
Train and counsel entrepreneurs
SBA Resource Partners SBDC,
capital/lending
Entrepreneurs in emerging markets
More jobs created and retained
WBC, SCORE, VBOC
Improved understanding of policy
trained and counseled
Greater sales revenue of businesses
More entrepreneurs from emerging
Healthier local economies
Loan Processing
Service existing loans
Lenders and borrowers serviced
More lenders satisfiedwith
markets apply for 7(a) loans
loan processing
Guarantees sold, secondary market
More Lenders financing 7(a) loans
Entrepreneurs are better served
OCA/OFPO Center Operations
Authorize new loans
Staff and Colson Services
Loan applications processed
More entrepreneurs satisfied with
More SBA 7(a) loans approved
Lenders increase 7(a) and
(contractor)
Collect loan servicing fees
loan processing
(dollar and number)
non-7(a) lending
Fees waived (small dollar, veteran)
Pay loan guarantees
Fees assessed/guarantees paid
Systems and Data
Develop loan systems (SBA One, CAFS)
BAOne/CAFSusers
Greater Lenders Relation Specialist
CA/OPSM Systems and Data
Develop lender/loan data and analysis
awareness of 7(a) loan market
Staff
Data and analysis products
More entrepreneurs/lendersaware of
Develop lending tools (Lender Match)
Lender-Matchusers
7(a) loans supply/demand
Risk Management
Reduced burden for the Agency as
Manage delegated authorities
Delegated authorities approved and
lenders require less guidance for
(nominations and renewals)
renewed
routine processing actions
Improved 7(a) and 504 loan portfolio
OCA/OCRM Credit Risk
Lenders more aware of how to apply
Management Staff
Monitor and manage 7(a) and 504 loan
performance
Risk-based reviews conducted
knowledge from training sources
portfolio
More lenders in compliance with
program requirements
External Factors
Lender Behavior (Mergers, Risk-taking)
Business Behavior (Risk-taking, Federal, State, Local Regulations, Entrepreneurial Ecosystem Strength) Trade Association Behavior (NAGGL)
Congress (Appropriations, US Deficit, Oversight)
Economy (Unemployment, Inflation, US Dollar Strength)
SBA Behavior (Employee Recruitment, Training
Resources Contractors)
Note: Arrows and lines are used when connecting different colored boxes (activity types) to outputs or outcomes from one activity type.
SBA 8(a) Business Development Program Logic Model
Program Mission: Help socially and economically disadvantaged owners develop their business
Outputs
Short-term
Activities
Intermediate
Inputs
Long-term
Outcomes
Outcomes
Outcomes
Promote, educate, and advise
SBA meetings and
Increase knowledge of field offices
Budget
internal staff on 8(a) program
webinars held
Increase understanding of 8(a)
Increase 8(a) applicants who
Systems (e.g.
Promote and market 8(a) program
Presentations, training and
program among owners
are eligible and ready for the
Certify.gov)
to businesses
marketing materials
program
Reduce incomplete submissions
SBA HQ
Number of applications
Process applications
Increase applications processed in
Improve quality and quantity
program staff
processed
accordance with requirements
of 8(a) portfolio
Process adverse actions
Actions taken
Eliminate instances of ineligible
Reduce appeals, overturned
SBA Office of
Apply participant change requests
Requests processed
businesses receiving 8(a) benefits
decisions/increase owner
General Counsel
understanding of decision
Eligibility/development
Conduct annual/eligibility reviews
plan reviews conducted
SBA field
operations
Improve owner capacity for
Review joint venture agreements
development
New or continued
Increase non-8(a) federal
agreements approved
Improve owner networks to take
contracts awarded to 8(a)
8(a) advocates
Review mentor/protÃ©gÃ© match
advantage of contract
businesses
(e.g. Resource
opportunities
Partners, PTACs,
Provide business development
OSDBU)
assistance and trainings, non-7(j)
Number of businesses
Improve owner understanding of
Increase 8(a) federal contracts
Provide 1:1 and group training and
assisted or trained
business management/operations
awarded to 8(a) businesses
for contracting
Former 8(a) businesses are more
7(j) line item
TA for 7(j) firms
competitive in the market
funding
Manage/service contracts
Inquiries answered
More jobs are available in the
economy
Promote and educate federal
SBA Office of
acquisition staff on 8(a) program
Information and answers
Increase awareness of 8(a)
Increase use of 8(a) set asides and
8(a) program is sustained for
Government
Contracting
about program and firms
program for agency contracting
sole source/competitive contracts
future socially and economically
Encourage federal acquisition
needs
for 8(a) certified businesses
disadvantaged small businesses
(PCRs)
staff to use 8(a) set asides and
sole source contracts
Improve reputation of 8(a) as a
valuable tool for development
Mini brackets indicate that an input is
Respond to internal and external
Data, analysis conducted
Increase awareness of 8(a)
applicable to all activities; however, HQ
requests (e.g. OIG, FOIA)
Increase support for what works,
program staff is not an input to Review
successes and areas of
Maintain program data for
continuous learning and
joint venture agreements and Encourage
>
Data processed
improvement
reporting
improvement
federal acquisition staff.
Open arrow heads indicate the input is not required in every instance of the activity. Colors are
External factors: Legislative changes that effect 8(a), OMB/Office of Federal Procurement Policy, Federal budget for small business
assigned to inputs with more than 2 arrows to make the connections more visible.
contracts, economic conditions
4.D Develop Evaluation Questions
Evaluation questions are the broad questions that the evaluation is
4. Plan and Prepare
designed to answer. They are often inspired by or build upon existing
performance measures, but they differ from performance measures
A. Plan the Evaluation
in several ways.
B. Identify Key Stakeholders
Performance measures are used to gather data on your program's day-
C. Develop or Update the Program
Logic Model and Learning Agenda
to-day activities and outputs. In contrast, evaluation questions delve
more deeply into the reasons behind program accomplishments and
D. Develop Evaluation Questions
seek to answer whether current operations are sufficient to achieve
5. Conduct and Monitor
long-term goals. Good evaluation questions are important because
they articulate the issues and concerns of stakeholders, examine how
A. Set an Evaluation Design
the program is expected to work and its intended outcomes, and
frame the scope of the evaluation.
B. Implement the Evaluation
While interview, focus group, or survey questions are specific data
6. Disseminate and Implement Findings
collection tools that are used to gather information from participants
that will be used to address the larger evaluation, evaluation
A. Communicate Evaluation Results
questions specify the overall questions the study seeks to answer.
B. Implement Recommendations
The logic model or learning agenda is an excellent place to start the
process of determining what questions to answer in your evaluation. The learning agenda prompts you to
consider high-level critical questions that, if answered, would improve the functionality or build evidence for the
impact of your program. The logic model can further clarify those questions by tying them to the program
activities, outputs, and overall logic. For example, during pre-scoping activities for the HUBZone evaluation,
staff identified a need to understand the factors that prevent the HUBZone program from meeting its goal.
The logic model can help identify the specific program components that may warrant additional examination
based on this question. In the HUBZone logic model below, small businesses are made aware of federal
contracting opportunities through matchmaking events. This step occurs before federal contract dollars are
awarded. A subsequent evaluation question may focus on examining the effectiveness of those matchmaking
events.
Overall, each outcome component in a logic model can be thought of as an evaluation question. Using the first
line of the HUBZone example below, the evaluation might ask, "Are SBA meetings and webinars increasing
the SBA field operations staff knowledge of the HUBZone program?"
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
45
SBA: HUBZone Program Logic Model
Mission: To encourage economic development in historically underutilized business zones "HUBZones" through preferential access to federal procurement opportunities
Short-term
Intermediate
Customer
Long-term
Inputs
Activities
Outputs
Outcomes
Outcomes
Outcomes
Reached
(Awareness)
(Behavior)
(Condition)
Internal program
SBA meetings and
Increased knowledge
Budget
promotion and education
webinars
SBA field operations
of HUBZone Program
y
Regional/Local Economic
Promote and market
Small businesses in a
Development Associations
HUBZone to small
Presentations, webinars,
HUBZone understand the
(EDAs)
businesses
training and marketing
Small businesses
Small business apply
HUBZone opportunity and
materials (e.g. Destination
value of the HUBZone
to HUBZone program
SBA field operations
HUB event)
Provide information and
certification
assistance through
Exchange of e-mails
HZ Help Desk
and phone calls
HUBZone Designation
Data provide by DoD, etc.
Determine HUBZones
Easy to understand,
Federal and local
HUD, BLS (DoL), ACS
accurate up-to-date
(Census), BIA (Dol)
Manage HUBZone
officials
HUBZone maps
Maps
y
Process certifications
New applications
Portfolio of HUBZone
SBA HQ program staff
Recertification
Small Businesses
Material changes in
Increased knowledge
companies' status'
4
of HUBZone certified
Portfolio of sole source
Federal agencies'
businesses and
Ensure compliance
contracts and set asides
acquisition staff
opportunities for HUBZone
Non-compliance
for HUBZone Businesses
Up-to-date list of known,
contracts or set asides
Program examinations
SBA Office of General
non-eligible firms
within an Agency
Adjudicate HUBZone
Counsel (OGC)
status protests
Offices of Small and
Educate and promote
Information and answers
Disadvantage Business
HUBZone program to
about HUBZone program
Utilization (OSDBU)
Federal agencies
.
Small Business
Procurement Advisory
Council (SBPAC)
Encourage federal
Conversations about
SBA Office of Government
acquisition staff to use
opportunities for set asides
Increase jobs and reduce
Contracting (PCRs)
HUBZone set asides and
or sole source contracts for
unemployment in
HUBZones
sole source contracts
HUBZone businesses
HUBZone certified
HUBZone small
businesses receiving
SBA sponsored
businesses introduced to
matchmaking events
Increased knowledge of
contracts in atypical federal
Federal prime contracting
contract locations
Federal agencies with
Federal agencies'
HUBZone businesses
dollars awarded to
potential contracting
acquisition staff
suitable for agencies'
HUBZone certified
Federal agencies'
Reduce Temporary
Other matchmaking events
opportunities (e.g.,
contracting needs
businesses
acquisition staff
Assistance for Needy
Destination HUB events)
Families (TANF)
Use of set asides and
dependency in
sole source contracts
Advocate for HUBZone
Continued legislation
HUBZone areas
HUBZone Council
for HUBZone certified
legislation
supporting HUBZones
businesses
Increase
local revenues
Greater representation of
HUBZone firms and
SBA Office of Policy,
Increase in value of
contracts outside the DMV
Planning, and Liaison
HUBZone certification
(OPPL) and
Goaling Reports
Improved understanding of
Contracting data collected
and Scorecards
SBAHQ program staff
HUBZone successes and
Office of Government
through FPDS
areas for improvement
Increased support for what
Contracting and Business
HUBZone is effective,
works; continuous learning
efficient, and sustainable
Development (GCBD)
Other Federal Programs
Program's reputation as
and improvement
useful and successful
HZ Enacting Statutes
Mini brackets indicate that a box is applicable to all the items in the following column
External Influences: 1) Legislative Changes: 35 percent of employees of a HUBZone firm must reside in a HUBZone, and this is a percentage that may change in the future impact would be positive to increase the number of firms in compliance with the HUBZone
Program and able to receive HUBZone contracts; 20 percent cap from HUD removed in June 2016 to increase the quantity of census tracts able to be qualified for the HUBZone Program and current discussion about whether the 20 percent cap will be reinstated (cap
from IRS LIHTC program) impact will be negative reducing number of firms. 2) Buying Policy Changes: Trends toward category management and how this may impact buying policies of the federal government
Typical SBA program evaluations use three to eight evaluation questions. By working with the learning
agenda, program logic model, and engaging relevant stakeholders, you and your evaluator can develop the
key evaluation questions. The following five steps should aid evaluators in the process of designing evaluation
questions:
1.
Review, update, or develop the learning agenda. As discussed in Chapter 4C, this encompasses
a review of the purpose and goals of the program, identification of the most important challenges
currently facing the program, and development of critical questions that if answered, would
improve the functionality or build evidence for the impact of your program.
2. Review the logic model and further identify what aspects of your program to evaluate.
3. Consult with stakeholders and conduct a brief literature search for studies on programs like yours.
4.
Generate a potential list of the overall evaluation questions.
5. Group questions by themes or categories (e.g., resource questions, process questions, outcome
questions).
When you review the evaluation questions, ensure that they will be effective in measuring progress toward
program goals and against identified baselines. When finalizing evaluation questions consider the following:
Are the questions framed so that the answers are measurable in a high-quality and feasible way?
Are the questions relevant, important, and useful for informing program management or policy
decisions?
Are the primary questions of all the key stakeholdersrepresented?
Defining evaluation questions carefully at the beginning of an evaluation is important, as they will drive the
evaluation design, measurement selection, information collection, and reporting.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
47
Chapter 5: Conduct and Monitor
Once you have finalized your logic model and evaluation questions,
4. Plan and Prepare
consider the following issues to help choose the right design:
A. Plan the Evaluation
What is the overarching question your program needs to
B. Identify Key Stakeholders
answer?
C. Develop or Update the
Where is your SBA program in its life cycle?
Program Logic Model and
What do you hope to show with the results obtained?
Learning Agenda
What additional technical evaluation expertise will you need
D. Develop Evaluation Questions
to carry out the evaluation as designed?
5. Conduct and Monitor
The issues above overlap with those raised in Chapter 4.A because the
A. Set an Evaluation Design
program evaluation process is typically somewhat iterative as it proceeds
B. Implement the Evaluation
through the planning and design steps. At this stage, determine if a
design, process, outcome, or impact evaluation is best, given the
6. Disseminate and Implement
Findings
considerations you discovered in the planning phase (each described in
detail in Chapter 4.A).
A. Communicate Evaluation
Results
The Foundations of Program Evaluation Design
B. Implement Recommendations
When your program communicates with key stakeholders about the
implementation and results of a program evaluation, you and your
evaluator will likely be asked questions related to the rigor and appropriateness of the program evaluation
design. Have thoughtful responses to these types of questions:
Design: Is the evaluation design appropriate to answer the evaluation question(s)? Is a process
evaluation design most desirable? Are outcome and impact evaluation designs more appropriate?
Validity: Are the data you are collecting to represent performance elements measuring what they
are supposed to measure? Are the data valid?
Reliability: Is your measurement of the resources, activities, outputs, and outcomes repeatable
and likely to yield the same results if undertaken by another evaluator? Are the data reliable? How
do youknow?
Feasibility: Do you have the money, staff time, and stakeholder buy-in that you need to answer your
program evaluation question(s)? Is the evaluation design feasible?
Functionality: Can the information collected through your evaluation be acted upon by program
staff? Is the evaluation design functional?
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
48
Core Principles
To ensure that results of evaluations are actionable and that recommendations can
effectively be implemented, the following principles are incorporated into each
evaluation and considered throughout the entire evaluation process:
Ethics - Conduct the evaluation by adhering to the rules governing human rights,
confidentiality, and privacy. Minimize the burden to research participants and cost to
taxpayers.
Independence - Conduct the evaluation through an outside party that does not have
vested interest in the outcome or will not interpret the results in ways that are self-serving or
misleading. Eliminate the appearance of bias to ensure results are properly used.
Rigor - Employ the methodological approaches that best support the definitive
answers to the evaluation questions under investigation. The limitations of the methods
used and how much the conclusions drawn can be unequivocally supported should be
stated explicitly when describing the methodology and reporting the findings. Minimize
threats to internal validity (the ability to draw causality inferences) and external validity
(generalizing beyond the specific program being evaluated).
Relevance - Scope and select evaluation questions most closely tied to the goals of
the program, the priorities of the Agency, and the intended use by senior management.
Transparency - Ensure that the evaluation scope, design, implementation, and
results are available for internal and public review, assessment, and critique.
Clarifying how the program evaluation design handles validity, reliability, feasibility, and functionality will help
you and your evaluator prepare for the scrutiny of external reviewers and yield results that will more accurately
reflect your program's performance. This will ultimately lead to high-quality recommendations on which your
program can act.
To ensure that the program evaluation design addresses validity, reliability, and feasibility, the program
evaluator will consult the relevant technical and program evaluation literature. A technical literature review
involves consulting published information on how the SBA program operates. Additionally, a review of relevant
program evaluation literature will focus on past program evaluations of programs with similarities to your
program. This can also be helpful to identify evaluation strategies and pitfalls of previous evaluations, and
potentially to identify existing data that may be useful for your evaluation. The documentation of this review
can be as simple as a bibliography in the report. Regardless of its length, the literature review should be made
available to internal and external stakeholders to increase the transparency of the program evaluation process
and assist in validating your program evaluation's findings, conclusions, and recommendations.
Much of the discussion surrounding the quality of a program evaluation involves the concept of rigor. Because
well-designed outcome and impact evaluations are better able to determine a direct causal link between a
program's activities and a program's results than other evaluation types, these evaluations are frequently
associated with greater design rigor. Despite this perception, an impact evaluation design is not necessarily
more rigorous than a process evaluation design. The rigor of a program evaluation is not determined solely by
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
49
the type of evaluation that you undertake but instead by the overall evaluation design and implementation (for
more about implementation, please see Chapter 5.B).
The design phase of a program evaluation is a highly iterative process; while this chapter gives a linear
description of the design phase, you and your evaluator are likely to revisit various issues several times.
Decisions about data needs, how those data can be collected, and the feasibility of the evaluation methodology
will all inform the overall design. Your approach to engaging stakeholders (e.g., the members of your core
evaluation team and other interested parties) will influence how iterative this phase becomes.
Assessing the Data Needs for the Evaluation Design
Consider the several classes of data needs when planning your evaluation design.
Type of claims your program is expected to address: attribution or contribution. Attribution involves
making claims about the causal links between program activities and outcomes, achieved by comparing the
observed outcomes with an estimate of what would have happened in the absence of the program. Because
the program itself is often only one of a variety of factors that influence small business decision-making, SBA
programs often have a difficult time demonstrating attribution.
Contribution, in contrast to attribution, involves measuring the correlations that exist between program activities
and outcomes after you have controlled for all the other plausible explanations that might influence the results
you observe. Contribution can tell you that your program likely influenced the outcome but cannot confidently
demonstrate that your program alone has caused the results observed.
Demonstrating attribution should not be thought of as inherently better than demonstrating contribution;
instead, it is simply a matter of what is needed by the program or what data are available. To support
attribution claims, your evaluation will generally need to collect more data, including quantitative data from a
comparison or control group, to be statistically analyzed in comparison to data for program participants.
The use of original primary data or existing secondary data. Primary data are collected by your SBA
program, whereas secondary data are gathered from existing sources that have been collected by others for
reasons independent of your evaluation. The assessment of your data needs should follow three broad steps:
Review the primary data that your program already collects for existing performance reporting to
determine if it can be used to address your evaluation questions.
Search for sources of secondary data that others are collecting and that will appropriately serve
your evaluation needs.
If needed, plan a primary data collection effort specifically for the evaluation.
The form of data you require: qualitative or quantitative data. Data form will shape what types of
analyses are possible, including the types of conclusions you can make. Qualitative data are often in-depth
collections of information gathered through observations, focus groups, interviews, document reviews, and
photographs. They are non-numerical in nature and are often classified into discrete categories for analysis. In
contrast, quantitative data are usually collected through reports, tests, surveys, and existing databases. They
are numerical measures of your program (e.g., the amount of loans administered) that are usually summarized
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
50
to present general trends that characterize the sample from which these data are drawn. The decision to use
qualitative or quantitative data is not an either/or proposition. Instead, consider which form of data is most
useful (given the evaluation question and context). In many cases, collecting both qualitative and quantitative
data in the same evaluation will present the most complete picture of your program.
Measures
When assessing your data needs, consider existing data sources already collected. For example, within the
SBA, the Office of Program Performance, Analysis and Evaluation is responsible for supporting the
Administrator's priorities in part by measuring and assessing progress of SBA programs. These responsibilities
complement and inform the program evaluation goals of the Agency. In other words, all program evaluations
are integrated into the planning, decision-support, and reporting phases of the performance management
cycle. The SBA evaluation team works regularly to integrate reporting and management improvement as
unique functions of its performance measurement responsibilities. During the planning phase of your
evaluation, meet with performance analysts to discuss historical evidence and performance measures that
may inform and support your evaluation (see discussion in the Chapter 2).
Data Collection Methods
Planning can reduce the costs of conducting a program evaluation and increase the quality. If your program
collected data early in its history, this improves the likelihood that you have access to baseline data and
appropriate performance data. Your evaluator should assess your program's existing performance data by
asking you the following questions:
Are the data complete and of high quality? Is data missing due to inconsistent recordkeeping,
systematic omissions in data, or other irregularities?
Are the measurement tools a valid assessment of the program elements you are investigating with
your evaluation questions?
Are the data collection techniques reliable enough to render the same results if they were
independently collected by someone else? Is the data collected according to Standard Operating
Procedures (SOPs)?
Are the data gathered in a way to answer any of the evaluation questions (e.g., are comparable
data available from program non-participants)?
The table that follows describes some data collection methods used for program evaluation and the relative
advantages and challenges associated with each. Weigh the benefits and costs of each before selecting a
data collection method. Using these methods to collect data can be more complex than it appears at first
glance. Poorly collected data can undermine your evaluation's usefulness and credibility.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 51
Method
Overall
Advantages
Key Challenges
Form of Data
Purpose
Direct
Measure program
Can provide evidence of program
Might reveal changes in
Quantitative
Monitoring
outcomes to
impact and yield information useful
indicators only over periods of
assess the
for accountability, and may show
years; might not be sensitive
degree of
whether the program is
changes
accomplishing its goal
Interviews
To understand
Provides a full range and depth of
Are time- consuming/ costly and
Qualitative
someone's
information, allows for
can produce inaccurate results
impressions or
development of relationship with
if respondent recall is inaccurate
experiences
respondent, and can be flexible
Focus Groups
To explore a
Quickly and reliably captures
Can be difficult to analyze and
Qualitative/
topic in depth
common impressions, can be an
can involve a group dynamic
Quantitative
through group
efficient way to gain greater range
that may affect responses
discussion
and depth of information in a short
time, and can convey key
information about programs
Allows events to be witnessed in
Direct
To gather
Can be difficult to reliably code
Qualitative/
real-time and observed within a
Observation of
information about
and interpret what you observe,
Quantitative
Behavior and
how a program
context, and provides possible
and when observers are
Program
insight into personal behavior and
operates
present, can influence
motives
Process
behaviors of participants
Surveys,
To collect
Can be completed anonymously,
Can bias responses, depending
Quantitative
Checklists
answers to pre-
are inexpensive to administer to
on wording; might not provide
determined
many people, are easy to
full story and might not be
questions from
compare and analyze, can
representative due to volunteer
many
produce a lot of data, and most
bias and social desirability
respondents,
conducive to producing results
motivations of respondent
often for
that can be extrapolated to wider
statistical
population
analysis
Document
Gather historical information, don't
To provide an
Might be incomplete if access to
Qualitative/
interrupt program or client's routinesome
documents are restricted,
Reviews
impression of
Quantitative
in program, and collects
and results may not be
program
operations
information that already exists
comparable to your program
through existing
program
documentation
Case Studies
To provide a
Can provide full depiction of
Are usually time- consuming
Qualitative/
comprehensive
program operation and can be a
and focus on one or two
Quantitative
review of one or
powerful means through which to
elements fundamental to
two elements or
portray the program
program and provide a deep,
an entire program
but not broad, view
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 52
Primary Data Collection Challenges
The following challenges for collecting primary data for program evaluation may be present.
Data Needs Versus Data Collection Techniques. SBA program managers must balance obtaining data of
sufficient quality to demonstrate useful results while not overburdening the partners. Any approach to primary
data collection must consider the "tipping point" where the data collection itself becomes a disincentive to
participation in the program. Additionally, obtaining data from non-participants is often difficult, which creates a
major barrier to the design of control groups. Your evaluator can help you brainstorm possible sources for data
on non-participants and evaluation designs without control groups.
Information Collection Requests. Another noteworthy data collection barrier is the Information Collection
Request (ICR). According to the Paperwork Reduction Act, ICRs must be granted by OMB before a federal
agency collects the same or similar information from 10 or more non-federal parties. ICRs describe the
information to be collected, give the reason why the information is needed, and estimate the time and cost to
the public to answer the request. If you and your evaluator need to collect primary data from outside the
federal government, begin this process early in evaluation planning. OMB provides guidance in navigating the
ICR process on its website. Before embarking on the ICR process, consider strategies for collecting new data
that do not require obtaining an ICR:
Identify third-party organizations that might be interested in collecting some of the data that you
need for their own purposes and make it available to the SBA.
IMPORTANT: SBA program managers cannot request third parties to collect data to support an SBA
evaluation without OMB approval through the ICR process; a third party must have an interest
beyond the SBA evaluation for collecting the data.
Evaluate the possibility of collaborating with a related evaluation effort on data collection;
especially other programs that have already received an ICR or plan to file an ICR.
Explore the availability of existing SBA ICRs that might apply to your evaluation questions and
have been ICR-approved for evaluation purposes. SBA Forms to consider include: SBA's
Management Training Report (888), Counseling Information Form (641), and Outreach Event
Survey (20). These forms were written to include multiple programs and activities.
Consider collecting data from federal sources. An ICR is not required if you survey federal
employees as part of their occupation.
Consider all the government agencies, academic institutions, other research organizations,
professional associations, trade associations, and other groups that might share data they have
collected.
Consider teaming with another SBA program that needs to collect data from similar enterprises or
sources that might be willing to share resources.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
53
Tips When Filing Your Own ICR
Start the process early.
Identify examples of similar programs that have received similar data collection
clearance and provide the examples to OMB.
Look for examples of successful and pending, similar ICR and use them as potential
models for your ICR. Reglnfo.gov contains information on ICR packages.
Build future evaluation considerations into any program ICRs filed to avoid the
need to file more than one. For example, new SBA programs can file an ICR early
to cover planned performance measurement and future evaluation needs.
For more information or assistance with the ICR process, see OMB guidance on
navigating the ICR process on their websites.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
54
5.A. Select an Evaluation Design
4. Plan and Prepare
When an SBA program communicates its results of a program
evaluation, an important question will be asked: "What is your
A. Plan the Evaluation
program evaluation methodology?" As a program manager, you do
B. Identify Key Stakeholders
not need to be know the technical aspects of design methodology.
However, you should be able to identify the defining characteristics
C. Develop or Update the
and strengths and limitations of each of three broad classes of
Program Logic Model and
Learning Agenda
evaluation methodologies: non-experimental, quasi-experimental,
and true experimental.
D. Develop Evaluation Questions
Non-experimental designs are generally best suited to answering
5. Conduct and Monitor
design and process questions (e.g., What are the inputs available for
this program? Are the activities leading to customer satisfaction?).
A. Set an Evaluation Design
Non-experimental designs do not include comparison groups of
B. Implement the Evaluation
individuals or groups not participating in the program. In fact, many of
these designs involve no inherent comparison groups. Non-
6. Disseminate and Implement
Findings
experimental designs involve measuring various elements of a logic
model and describing these elements, rather than definitively linking
A. Communicate Evaluation Results
them to other elements in the logic model through causality. These
designs can yield qualitative or quantitative data and are relatively
B. Implement Recommendations
common in evaluations.
Non-Experimental Design
The SBA HUBZone program supports economic development in Historically Underutilized
Business Zones - "HUBZones." The program provides HUBZone businesses access to
federal procurement opportunities and is currently being evaluated to, in part, identify
barriers to achieving federal contracting goals. This evaluation helped to determine ways to
strengthen and increase the effectiveness of the program's outreach strategies.
Quasi-experimental designs are usually employed to answer questions of program outcome; they often
compare outcomes of program participants with non-participants that have not been randomly selected.
Alternately, a quasi-experiment might measure the results of a program before and after an intervention has
occurred to determine if the time-related changes can be linked to the program's interventions. This type of
evaluation design can be particularly appropriate for evaluating social programs, such as those most often
funded by the SBA, because a true experimental design is often not feasible, practical, or ethical to implement.
Achieving the perfect equivalence between the groups being compared is often difficult because of uncontrolled
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
55
factors such as spillover effects (see the text box for more information). Instead, quasi-experimental designs
demonstrate potential causal impact by ruling out other plausible explanations through rigorous measurement
and control. Data generated through quasi-experimental methods are typically quantitative.
Quasi-Experimental Design
The SBA Learning Center is an online database that gives small business owners access to quick,
relevant, accessible, and high-quality content. Once a user selects a course from the catalog, they
view a pop-up registration form. Though not mandatory, upon seeing this form, about half of visitors
abandoned the course without starting the course (45 percent). An updated version of the form was
piloted that decreased the form fields, decreased the number of multiple choice questions, and re-
ordered the questions in an intuitive order. It also indicated that completing the information was
optional. These changes reduced the time required to complete the form but still collected the most
critical information to the SBA. Program staff were able to compare the drop-off rate before and after
this new form was instituted to determine if the new form led to a decrease in drop-off rates. In other
words, did fewer visitors drop off with the shorter form, compared to the original form? Compared to
the average drop-off rate of 45 percent, the shorter form resulted in a drop-off rate of 35 percent.
True experimental designs (alternately referred to as randomized control trials, or RCTs) involve the random
assignment of potential program participants to either participate in or be excluded from the SBA program.
These studies enable measurement of the causal impact and yield quantitative data that are analyzed for
differences in results between groups based on program participation. True experiments can be used in
evaluating SBA programs when:
Clearly defined interventions can be manipulated and uniformlyadministered;
There is no possibility that treatment will spill over to control groups (those for whom a program's
intervention is not intended, see textbox); and
It is ethical and feasible to deny a program's services to a group, at least for a long enough time to
support the evaluation.
The Spillover Effect
The spillover effect occurs when participants of SBA programs share knowledge or strategies gained
through participation in the program with non-participants. This effect is quite common, and it is
desirable because the transfer of knowledge and best practices can lead to performance
improvements from non-participants as well as participants. The spillover effect can pose a challenge
to program evaluators in determining causality when non-participants gain the same knowledge as
program participants, indirectly and not within measurable circles. For SBA programs, technical
assistance is often designed to "spill over" to non-participants; programs are designed to impact small
businesses well beyond those directly participating in the program. In these cases, it can be difficult, if
not impossible, to isolate the effects of the program to measure the true impact in absence of spillover.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 56
RCTs have been labeled as the "gold standard" for program evaluation; however, because of the caveats just
described, true experimental designs are more a theoretical ideal than a practical reality for most SBA and
social policy programs. This makes the demonstration of statistically certain impact difficult for SBA programs.
The manipulation of a program's benefits, which would be central to the design of an RCT on an SBA program,
runs counter to the spirit of spillover, or the sharing of a program's goals and philosophy, that SBA programs
both espouse and encourage. In these cases, quasi-experimental evaluation designs can still provide
meaningful findings to estimate the impact and/or attribution of the program.
Quasi-experimental and experimental designs can be very complex to implement unless the capacity to conduct
them has been a central part of the program's initial design. As the complexity of the evaluation methodology
increases, so too will the resources (money, time, and buy-in) required.
Therefore, regularly check in throughout the design selection phase to ensure that the evaluation methodology
selected can be supported by available resources. You and your evaluator might determine that an evaluation
question cannot be sufficiently answered with the evaluation design options available. In such instances, revisit
the logic model to determine another evaluation question that fits within resource capacity.
Expert Review of the Evaluation Design
A final step that you should consider before implementing your evaluation is an external expert review of the
selected evaluation design. These reviews will help ensure the actual and perceived quality and credibility of
your evaluation. Before commissioning a review of your design, carefully consider the technical expertise of the
intended audience, the availability of resources and time, and the function of the evaluation's results. Not all
evaluations need to undergo an external review before the implementation is underway.
Selecting the Evaluation Design: The SBDC Experience Example
The Small Business Development Center (SBDC) program provides technical assistance to small business
owners, managers, and prospective owners. SBDC service locations offer one-on-one counseling, training, and
technical assistance in small business management.
To determine changes in sales and employment, jobs and sales revenue maintained, and financing obtained by
small businesses that take advantage of these services, the program employed a quasi-experimental evaluation
design. The performance improvements of the responding participants were compared to the weighted average
changes in performance of all similar businesses in the U.S. The incremental improvements in the sample's
performance above the average business (that did not receive the centers' services) were considered evidence
of the centers' positive impact.
The evaluation was supplemented with qualitative and self-reported assessments of the program's impact.
Specifically, respondents were asked to estimate the number of jobs saved and sales revenues maintained
because of the counseling they received from the centers. They were also asked to indicate whether the SBDC
program had assisted them in obtaining financing, and if so, the amount of debt and equity financing they were
able to obtain as a direct result of the counseling received from the SBDC. Finally, respondents were asked a
series of qualitative questions concerning the availability of comparable assistance from private consultants and
the quality of those counselors.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 57
Natural Experiments
You might get lucky and be able to use a quasi-experimental method known as a "natural experiment." You are
best able to capitalize on this scenario if, as a part of your program design, you identify one group that is
receiving a program intervention and another similar group that is not receiving the intervention or is receiving
less of it. A natural experiment is only valid if the two groups are not systematically different on a dimension that
might affect program outcomes, and if differences between the two groups can be reliably assessed.
For example, if the SBA clients of a program systematically receive different levels of counseling, this could
serve as a natural experiment. Evaluators may be able to compare clients who received these differing levels of
counseling to determine if higher levels of counseling positively affect program outcomes.
Building Smarter Data for Evaluating Business Assistance Programs: A Guide for PractitionersÂ¹1 outlines
specific strategies for taking advantage of natural experiments. For example, datasets produced or housed by
federal statistical agencies such as the Census Bureau and the Bureau of Labor Statistics can help identify
comparable non-participants and can provide information about those non-participants.
11 The report is available at https://www.sba.gov/document/report--program-evaluation-evidence-registry-peer.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
58
5.B. Implement the Evaluation
After you have settled on your evaluation questions and evaluation design,
you are ready to implement the evaluation. To support this step, you may
4. Plan and Prepare
need to:
A. Plan the Evaluation
Distribute the evaluation design, or a summary of it, to
B. Identify Key Stakeholders
stakeholders, and subsequently communicate any schedule or
other important changes to stakeholders during evaluation
C. Develop or Update the
implementation.
Program Logic Model and
Learning Agenda
Review and provide feedback on interview guides, surveys, or
other data collection instruments, if your evaluator did not
D. Develop Evaluation Questions
finalize these during evaluation design.
5. Conduct and Monitor
Make first contact with participants whom evaluators need to
A. Set an Evaluation Design
contact to inform them about the evaluation and encourage
them to participate in datacollection.
B. Implement the Evaluation
Participate in periodic check-ins withyour evaluator to ensure
6. Disseminate and Implement
implementation is proceeding and to help address any
Findings
implementation challenges.
A. Communicate Evaluation
Assist in the contextual interpretation of analytical results.
Results
B. Implement Recommendations
Pilot Testing Evaluation Components
Pilot testing should take place before the full implementation of your evaluation. A pilot test involves testing
tools or components of the evaluation, in a limited capacity, with a small number of informed respondents who
can provide feedback on the effectiveness of the approach. For example, test a draft of interview
questions/survey questions with up to nine people who represent (or are like) the people from whom the
evaluation will ultimately be collecting data. Your evaluator might want to pilot test the sampling and data entry
processes, particularly if different people will be collecting and/or entering information. Your evaluator might also
want to revise the data collection instrument or processes based on the comments of the pilot respondents or
trial runs at data collection. Once you and your evaluator are confident about and comfortable with the tools and
processes you have selected, proceed to full implementation of the evaluation.
Protocols for Collecting and Housing Evaluation Data
You and your evaluator should agree to protocols for collecting and housing data during and after the
implementation of your program evaluation. Issues to consider include:
What form will my data take (e.g., text ornumbers)?
How much information will be collected, how often, and for how long?
Do I anticipate that my data collection needs will grow or diminish in the future?
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
59
What capabilities should my data management system have (e.g., a place to input and store data,
software that will enable the analysis of quantitative or qualitative data)?
What data management systems for the program currently exist? Could they fulfill my needs or be
adapted to meet our needs?
Who will need to have access to the data (e.g., SBA staff, the public)? What are the requirements for
protecting the data during collection, transfer, and storage? Do I need to consult with SBA's Privacy
Officer before collecting this data?
These issues can all be tested and potentially adjusted during the pre-test phase. Work with the program
evaluation team to manage your evaluation data and determine if it should be consolidated using existing
platforms and internal data dashboards. Dependent on the results of the evaluation, metrics or milestones may
be developed and revisited monthly or quarterly to promote transparency and ensure accountability.
Data Analysis
Once data collection is complete, your evaluator will analyze and interpret the information collected and develop
analytical findings. The nature of the analysis should adhere to the original methodology and design and will
vary depending on the data collected (quantitative or qualitative; primary or secondary) and the purpose of the
evaluation.
Quantitative Data
Often, quantitative data are collected and organized with the intent of being statistically analyzed; however,
there are limitations on statistical analysis that can affect an SBA program's ability to conduct a valid statistical
analysis. The most common barrier is confounding variables, which make it difficult to assign attribution to
programs in a statistically robust way. Your evaluator can help you brainstorm ways to overcome this barrier that
will enable you to draw inferences about causation or correlation in your sample.
If you are conducting an impact evaluation and have sufficient data, your evaluator can analyze the extent to
which the relationship between your program and a change you have observed is statistically significant. These
tests involve examining the relationship between dependent variables and independent variables.
Dependent variables are aspects of your program that are subjected to performance measurement and are
the central focus of an outcome or impact evaluation. You are examining the degree to which your program
produces a desired outcome, such as increased access to capital. Independent variables are those measured
aspects of your program that you believe might have caused the observed change, such as the activities of the
SBA program. Sometimes, you will collect data that will provide a sense of whether your program can
reasonably (within the rules of statistical probability) conclude that there is a relationship between the
dependent and independent variables. In other words, is your outcome unlikely to have resulted by chance (i.e.,
is this relationship statistically significant)?
In some other cases, you may theorize that a certain element of your program has been produced by your
program's activities based on logic and reasoning that cannot be subjected to formal statistical tests, but that
reasonably follow from other systematic methods. When working with your evaluator, be sure to ask:
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 60
What types of analyses do our data support?
What do the results tell us?
How confident are you in the results? Are the results statistically significant?
What do the results allow us to say about the relationship between thevariables?
Are there any findings that we predicted that the findings do not support?
Are there any findings that run counter to our predictions or expectations?
Even if your quantitative data do not support an analysis of statistical significance, they still may be
systematically analyzed to observe trends or relationships. At a minimum, your evaluator should also provide
descriptive statistics such as means and medians, ranges, and quartiles, as appropriate.
Qualitative Data
Qualitative data include any non-numerical data collected from interviews, surveys, focus groups, and other
means. Essential to the analysis of qualitative data is the concept of coding. Coding is the process of
categorizing information to identify themes, make comparisons, and identify patterns that require further
investigation. Evaluators should categorize and organize their data in a manner that allows for a robust analysis
of all the data they collected. Robust analyses of qualitative data involve multiple layers of coding. For example,
a qualitative analysis may begin with open coding, assigning a brief phrase to represent each new idea in a
response and then proceed to focused coding, where the many open codes are condensed into fewer
categories, from which themes are then created and used to organize the findings and tell the story of the
qualitative data collected.
In qualitative research, the term rigor is used to refer to findings that represent as closely as possible the
experiences of the respondents. Rigor may be enhanced by employing triangulation. In general, triangulation
involves analyzing all the qualitative data to determine if the themes produced from interviews align with the
findings from the quantitative evaluation and provide additional context for the quantitative findings.
Example of Quantitative Analyses to Support Evaluation: The SBDC Experience
In 2011, questionnaires were sent to clients of the 60 SBDCs that participated in the evaluation. Clients were asked
to evaluate the SBDC's services, provide their sales revenues and employment levels, estimate jobs and sales
revenues maintained, and indicate the amount of financing they were able. In addition, clients were asked a series of
qualitative questions concerning the availability of comparable assistance from private consultants and the quality of
those counselors. Therefore, both quantitative and qualitative information was collected in the questionnaire. Overall,
19 percent of clients returned questionnaires. To determine if the number of respondents was sufficient to obtain a
reliable and valid estimation of the average changes in sales revenue and employment, confidence intervals around
the mean were calculated. Evaluators also tested for sample validation, representativeness in the sample, response
bias, and reliability. For quantitative data collected, weighted average values were calculated and then compared to
non-clients' average values. Qualitative data were compiled and categorized for comparability. For example, clients
were asked to evaluate their counselors' knowledge/expertise and working relationship on a five-point scale, from
poor to excellent; these scores were counted and averaged.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
61
Interpretation of Results
Your evaluator should have the technical expertise to undertake a proper content analysis for qualitative data or
a statistical analysis for quantitative data. However, program managers and staff also play an important role in
this analysis. You may be able to answer questions that enable the evaluator to identify and investigate
potential data problems or other anomalies as they arise; give the evaluator feedback on what data analysis will
meet the needs of your audience; and help provide context and insights during interpretation of the findings,
including possible explanations for counterintuitive results.
Based on your expertise and familiarity with the program, you can provide insight into how analytical results
should be interpreted and changes that may be needed to respond to the findings. The mere fact that the
relationship between two variables is shown to be statistically significant does not necessarily mean that the
finding is meaningful. The reverse is also true: if the relationship between two variables is not shown to be
statistically significant, this does not mean that you cannot glean anything meaningful from your findings. You
need to carefully review all results and determine which are meaningful and can guide possible changes in your
program. You and your evaluator should work together to make sure that the data analysis is transparent and
that results areinterpreted appropriately.
Throughout the program evaluation process, your evaluator should share the "evolving story" that is emerging
from the data, when appropriate (i.e., without jeopardizing data validity and the evaluation's objectivity). In turn,
the SBA program must keep the evaluator apprised of cultural and political sensitivities that could influence the
form and format of how the results are presented. There should be no "surprises" when the final report is
delivered.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 62
Chapter 6: Disseminate and Implement Findings
Although communicating your results is one of the final steps in
the evaluation process, you and your evaluator should start
4. Plan and Prepare
planning early for this important step. As discussed in Chapter
A. Plan the Evaluation
5.B, when implementing the evaluation, your evaluator will take
primary responsibility for collecting and analyzing the data;
B. Identify Key Stakeholders
however, the process of communicating evaluation results
C. Develop or Update the Program
requires collaboration between the evaluator and SBA program
Logic Model and Learning Agenda
staff.
Careful consideration of your program's stakeholders will
D. Develop Evaluation Questions
influence how to best organize and deliver evaluation results.
5. Conduct and Monitor
The results have three basic elements: findings, conclusions,
and recommendations.
A. Set an Evaluation Design
Data collected during the implementation of the project will yield
B. Implement the Evaluation
findings. Findings refer to the raw data and summary analyses.
Because the findings are a part of the data analysis process, the
6. Disseminate and Implement
Findings
evaluator should retain the primary responsibility for
communicating findings to the program staff and management.
A. Communicate Evaluation Results
Evaluators often deliver findings to the SBA program in a draft
report or preliminary findings briefing.
B. Implement Recommendations
Conclusions represent the interpretation of the findings, given the context and specific operations of your SBA
program. Your evaluator may independently derive some initial interpretations. However, program managers
and staff should have an opportunity to provide comments based on the draft report and/ or preliminary
findings briefing, to suggest ways to refine or contextualize the interpretation of the findings. A strong evaluator
will want to ensure that the conclusions of the project are sound, relevant, and useful.
Regardless of the design or data collection employed, there will be some limitations to the explanatory power
of any methodology used. Make sure that your evaluator has clearly pointed out the limitations of the findings
based on the design selected when framing and reporting conclusions from the evaluation.
Recommendations are based on the findings and conclusions of your evaluation. The lead program
evaluator will understand that framing recommendations is an iterative process that should involve obtaining
feedback from SBA program managers, staff, and key stakeholders. Executive champion and staff involvement
in the development of recommendations is important, as most recommendations are designed to lead to
changes in how programs work. Implementing the recommendations and the resulting improvements to your
program is one of the greatest sources of value to programs from the evaluation process and is discussed
more in Chapter 6.B.
Preliminary results and draft reports should be shared with core evaluation team members (at a minimum) for
their feedback. Staff members who are directly involved in the program's activities are likely to have a critical
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
63
role in helping to interpret draft findings and make suggestions to the evaluator during the development of
conclusions and recommendations. Your evaluator may also consult published literature and other experts in
the area to make sure recommendations are objective, informed, and appropriate.
Despite the collaboration throughout the evaluation process and the need for active discussion of the findings,
conclusions, and recommendations as discussed above, the evaluation contractor should prepare drafts of
findings, conclusions, recommendations, and final report text. Granting this autonomy to your evaluation
contractor will help ensure that the report is objective and not unduly influenced by the vested interests and
stakeholders who might be affected-directly or indirectly-by the findings. This autonomy will also make the
evaluation less vulnerable to any potential criticism from external reviewers or stakeholders.
Questions to Ask About Your Results
Do the results make sense?
Do the results provide answers to evaluation questions?
Can the evaluation results be attributed to the program?
What are some possible explanations for findings that are surprising?
Have we missed other indicators or confounding variables?
How will the results help you identify actions to improve the program?
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
64
6.A. Communicate Evaluation Results
You and your evaluator should work closely to determine the level
4. Plan and Prepare
of detail and format of the draft report. The evaluator should tailor
presentations of evaluation results to the specific needs of your
A. Plan the Evaluation
stakeholders. Key questions you and your evaluator should ask in
B. Identify Key Stakeholders
presenting results are:
C. Develop or Update the Program
What evaluation questions are most relevant to these
Logic Model and Learning
stakeholders?
Agenda
How do they prefer to receive information?
D. Develop Evaluation Questions
How much detail do they want?
5. Conduct and Monitor
Are they likely to read an entire report?
A. Set an Evaluation Design
Based on the answers to these questions, in addition to a full-length
report, you can opt for alternative reporting formats depending on
B. Implement the Evaluation
the needs of each stakeholder group. Common reporting methods
include a shortened version of the evaluation report for broad
6. Disseminate and Implement
Findings
distribution; briefing(s) that may use slides or other visual aids; and
evaluation fact sheet(s).
A. Communicate Evaluation
Results
At a minimum, you and your evaluator's communication of
B. Implement Recommendations
evaluation results should include the following steps:
Present preliminary results and findings to program staff and other relevant stakeholders (e.g.,
SBA senior leadership).
Prepare a program evaluation report.
Conduct a final recommendation briefing to SBA senior leadership.
Create a summary fact sheet of the evaluation's key findings and recommendations.
Publish findings; work with the SBA program evaluation team to disseminate your evaluation
findings through the SBA Program Evaluation & Evidence Registry (PEER).
Tying your findings directly to the evaluation questions strengthens the applicability and relevance of your
results. Organizing your findings and recommendations in a way that clearly makes this link will ensure that
you have collected and are reporting on the key questions that the evaluation was designed to answer. Here
are some tips to assist you in planning for the application of evaluation results:
Consider whether the results support or challenge the linkages you expected to see in your logic
model. Work with program staff and your evaluator to consider a reasonable set of explanations
for the resultsobtained.
Review the literature to determine if results are consistent with findings published and presented
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
65
on similar programs (ifapplicable).
Work with technical experts and program personnel to developevidence-based explanations to
interpret your results.
If some results were unexpected, develop a set of possible explanations that might explain
counterintuitive findings.
Consult with stakeholders and external experts to develop a list of actionable items that can
inform management decisions; these items might be used later to frame recommendations.
Consider any methodological deficits of your evaluation strategy and consider design shortcomings
when applying the results to your program management directives.
Ensure that your results are transparent and that you share expected as well as counterintuitive
results. Do not suppress findings. Obtaining results inconsistent with your logic model does not
necessarily suggest that the core goals of your program are not worth pursuing and including
such findings will boost the integrity of your report.
Suggest future research or evaluations that should follow from the current evaluation effort.
Build the means for future evaluations into your program infrastructure (e.g., reliable record-
keeping, accessible storage of data, valid measurement of baselines for new program activities),
so that future program evaluations will have the advantage of having useful records to answer
evaluation questions.
Checklist for Reporting Results and Conclusions (Yes or No)
Linkage of results to logic model is clear
Conclusions and results are clearly presented and address key evaluation questions
Clear discussion of next steps is included
Stakeholders have participated in decisions concerning outreach method
Stakeholders are provided with opportunity for comment before evaluation is finalized
Communication Evaluation Results: The Scaleup America Evaluation
SBA's ScaleUp America program completed an evaluation in August of 2016, concluding with a final
report, Office of Entrepreneurial Development ScaleUp America Evaluation Year 1. In accordance with
evaluation reporting best practices, this report presents evaluation findings by key evaluation question
and summarizes conclusions and recommendations by research question in the Lessons and
Conclusions chapter. As part of this evaluation, staff members in the Office of Entrepreneurial
Development provided feedback to the external contractor on early to ensure that the contractor
had
enough time to review and address comments.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 66
6.B. Implement Recommendations
Implementing evaluation recommendations and the resulting improvements to your program is one of the
greatest sources of value to programs from the evaluation process.
Toward the end of an evaluation, coordinate with performance
4. Plan and Prepare
analysts in SBA's Office of Program Performance, Analysis and
A. Plan the Evaluation
Evaluation to develop an implementation plan. This plan should
include:
B. Identify Key Stakeholders
Recommendations for implementation.
C. Develop or Update the
Program Logic Model and
Anticipated results based on implementing the
Learning Agenda
recommendations.
Actions planned to implement recommendations.
D. Develop Evaluation Questions
Action budget.
5. Conduct and Monitor
Timeline for completing actionsano implementing
recommendations.
A. Set an Evaluation Design
B. Implement the Evaluation
Your evaluation plan should receive approval and support from
6. Disseminate and Implement Findings
relevant senior leadership. This approval will help ensure that
programmatic resources are sufficient to implement the
A. Communicate Evaluation Results
recommendations. Before dissemination to the public, the
respective program office senior leader, the Chief Evaluation
B. Implement Recommendations
Officer, and the Performance Improvement Officer must provide
clearance. Reports should be made available to the Office of Congressional and Legislative Affairs before
publication online.
The implementation plan should also include methods to track and monitor the implementation of
recommendations.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
67
Appendix A: Internal Needs Assessment
To ensure that this program evaluation framework meets the Agency's needs, the SBA documented its
program evaluation history and surveyed SBA employees. In addition, the SBA examined GAO reports (GAO-
15-347 and GAO-12-819) on the Agency's development and use of program evaluations. These reports stated
that the SBA had not conducted enough program evaluations, and of the evaluations conducted, few actions
had been taken to improve program operations and service delivery for entrepreneurs and small businesses.
As of June 2018, SBA program evaluations have taken place in the following offices: Office of Entrepreneurial
Development, Office of Veterans Business Development, Office of Government Contracting and Business
Development, Office of Investment and Innovation, Office of Disaster Assistance, and Office of Capital Access.
To learn more about SBA program evaluations, please refer to the SBA Strategic Plan and Enterprise Learning
Agenda, available atwww.sba.gov/about-sba/sba-performance/strategic-planning/sba-strategic-plan-fiscal-
years-2018-2022.
To better understand the context of SBA program evaluation history, more than 60 SBA employees (senior
leaders, program managers, analysts, and field staff) provided information on their efforts to build, design, or
conduct program evaluations. These employees understood the importance of program evaluation, its
methods and tools, and its connection to performance management. A benchmarking study of federal partners
and a review of literature on building evaluation capacity 12 was also undertaken to inform the creation of
a
framework that would meet the unique program evaluation needs of the Agency.
The following sections explore the opportunities and challenges identified during the interviews. Key themes
mirrored the GAO guidance on how to foster program evaluation capacity. SBA respondents identified several
areas that could bolster the Agency's program evaluation capacity: a desire to conduct more program
evaluations and identify process efficiencies, and the ability to better engage employees in program
design/redesign. In addition, program managers stated that program evaluation and performance
management should be built into the design of a program before its launch. Respondents also cited challenges
to initiating program evaluations, including lack of resources, lack of technical assistance, change
management issues, and data quality issues. The proposed framework incorporates these identified
opportunities and challenges.
Opportunities
Desire for More Formative Evaluations and Process Improvements: As a result of Office of Management
and Budget memoranda on administrative data, the Agency had promoted impact evaluations with tools
developed in economics and statistics. While impact evaluations can be useful, they require substantial
funding and time for program managers to conduct controlled experiments, collect and analyze extensive
datasets, and form partnerships with statistical agencies to match administrative data (e.g., Census Bureau,
12
Government Accountability Office, Program Evaluation: An Evaluation Culture and Collaborative Partnerships Help Build Agency Capacity.
GAO-03-454. Washington, D.C.: May 2, 2003
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
68
Internal Revenue Service, Bureau of Labor Statistics).
Fewer than three employees interviewed knew the difference between impact evaluations and other types of
program evaluations (needs assessments, process evaluations, design evaluations, customer satisfaction
evaluations, and summative evaluations). When other types of evaluations were defined, respondents
indicated a desire and need to conduct formative (process) evaluations and cited the pressing need for the
SBA to improve program operations and service delivery.
SBA employees identified many inefficient processes and ineffective approaches. They noted that where other
agencies have modernized their programs, many Agency programs continue to operate outdated systems or
use older delivery methods that may not be as useful to current entrepreneurs and the small business
community. Interviewees also noted that where other agencies have streamlined program functions and
reduced processing times, the SBA continued to allocate resources to inefficient activities that were designed
decades ago. With additional cost and time savings, SBA program managers could better support
entrepreneurs and small business owners as they grow the economy.
Desire to Engage Staff on Program Design/Redesign: Program evaluation is a stakeholder-driver activity.
SBA employees noted that they appreciate the need to design or redesign programs based on changing
entrepreneurial environments. For example, technology is rapidly modifying the way businesses operate, and
the American consumer has changing desires and needs. SBA employees noted that they have ideas that
could enhance service delivery and impact but do not currently have the tools, knowledge, or resources to
transform them. A program evaluation will include data gathered from employee experience and practice in
order to draw conclusions that may lead to performance improvements.
Challenges
Resources: A lack of funding and tools to conduct program evaluations has hindered progress. A short-term
formative or narrowly scoped summative program evaluation can cost as little as $75,000 and up to $500,000
depending on the evaluation's questions and design. Multi-year studies measuring outcome and impact can
cost considerably more. Many SBA program managers find these costs to be outside their allocated budgets.
In addition, to deploy a successful program evaluation, senior leadership must dedicate staff time to its design,
management, and implementation of findings because key program staff and personnel must be active
members of any contractor-led evaluation team from start to finish.
Technical Assistance: Employees noted that they often did not understand the context or have the tools to
develop a program evaluation. To deploy a meaningful evaluation, a program evaluator with training in social
science research methods should be assigned as the project lead. Employees have limited time to research
best practices, design, develop, and conduct a program evaluation.
Change Management: Respondents reported that the development of an evaluation within the current system
might be met with resistance. As employees are tasked with completing more work with fewer resources each
year and changing priorities, program improvements require dedicated attention by senior leadership and
program managers.
Data Quality and Evidence: Evaluators must have access to quality data from many sources to understand
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 69
how the program operates and its impact. Some programs have limited, inconsistent, and/or incomplete data
due to ineffective reporting. In addition, program managers that have available data may question its reliability
(e.g., self-reporting by recipients without audits, summary level data submitted by resource partners).
Quality data collection requires that program managers dedicate time to develop valid and reliable performance
measures, collect and review data from non-federal entities that may not be incentivized to report quality data,
and develop and maintain data storage systems. Although some resources are available for these activities, the
necessary requirements are often time- and resource-intensive and become secondary or tertiary
responsibilities of program managers.
Also, historical evaluations and research had been scattered throughout the Agency. Some information had
been available on SBA program websites or housed in internal folders. This evidence hasd not been centrally
housed for future use. The synthesis of data for building knowledge across similar programs is a critical aspect
of using evidence to inform program decisions and to acquire a better understanding of whether and how
programs are working as intended. Thus, the SBA has created and is committed to continuously updating both
an internal evidence registry and a public-facing evidence registry known as SBA Program Evaluation &
Evidence Registry (PEER). PEER is available atwww.sba.gov/about-sba/sba-performance/performance
budget-finances/program-evaluation-evidence-registry-peer
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
70
Appendix B: Federal Benchmarking Study
In addition to the internal survey, the SBA reviewed practices and capacity at other federal agencies in the
design of this framework. The team interviewed nearly 50 federal employees at 20 agencies and 27 separate
program evaluation units between August and November 2016. Respondents were identified through the OMB
Evidence Deputies Workgroup and the FedEval listserve. Each agency had varying levels of maturity in its
program evaluation units. The interview questions focused on the structure of program evaluation units, the
types of program evaluations conducted, the tools used for training and communication, successes to
13
replicate, and challenges to overcome. The findings are organized into broad, overlapping categories. The
benchmarking study documented resources and practices that are essential for the SBA program evaluation
function.
Of the agencies interviewed, three to 50 employees were housed in each program evaluation unit; newer units
maintained fewer employees. Many of the newer units (formed within the past five years) anticipated
expanding their roles and further enhancing their program evaluation function given their success and a
trending emphasis on evidence-based decision making in the Federal Government. In cabinet-level
departments, program evaluation units maintained $10-100 million budgets and managed separate program
evaluation units in their respective agencies or bureaus. At smaller agencies, program evaluation unit budgets
ranged from $750,000 to $5.7 million. Agencies managed program evaluation funds through set-asides or a
pool of funds collected from program offices each year.
Agencies conducted between six to 300 program evaluations each year. The numbers are predicated on the
types of program evaluations (formative, summative, impact assessment), and the scope of the questions
(broad questions versus focused questions). Agencies used external contractors to conduct objective and
independent program evaluations. The contractors work with their agency program evaluators to understand
the needs of the programs and the context for conducting the evaluation. Program evaluations conducted by
internal employees may not be objective or independent; for instance, results could be skewed to favor
behaviors or practices of some employees or political viewpoints that may not be in the best interest of
effective or efficient program delivery.
The type of program evaluations conducted also varied. Many agencies focused most of their resources on
summative, outcome evaluations. Outcome evaluations employ evaluation designs that focus on identifying
whether changes in knowledge (short-term outcomes) and behavior (intermediate outcomes) have resulted
from a program's resources, activities, and outputs.
Agencies with mature program evaluation units often partnered with their performance management units to
establish metrics for new programs with the goal of undertaking future, impact, evaluations. Impact evaluations
demonstrate whether programmatic activities definitively cause the long-term outcomes the program intends to
affect. Respondents noted that impact evaluations can be expensive and can only be undertaken when high
quality data is available. Many program evaluators cited challenges related to resources, technical expertise,
13 The interview guide for these discussions is included in Appendix B. The summary and analysis of the findings is included in a best
practices report. Korn, R.L. (2017) Program Evaluation in the Federal Government: A Review of Best Practices (unpublished).
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
71
and data quality. In a joint paper developed by the SBA and U.S. Department of Commerce, best practices are
outlined for using data collected by an agency's program and matching it with administrative data collected by
statistical agencies to enable the evaluation of a program's impacts. As a result, businesses that received the
benefits of a program could be compared to groups that did not receive the program's benefits to determine
impacts.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
72
Appendix C: Roles and Responsibilities in the Program
Evaluation
Executive Champion - a member of senior management who has championed an evaluation study and
will stay apprised of the program evaluation's progress. The executive champion will ensure that
programmatic resources (i.e., staff time, access to relevant data, introductions to external stakeholders)
are sufficient. The champion will be informed of the study design and potential limitations, receive regular
study progress updates and provide feedback and guidance to ensure that the results and recommendations
generated from the evaluation are leveraged by management for performance improvements.
Performance Improvement Officer - a senior executive of the agency who advises and assists the
head of the agency and the Chief Operating Officer to ensure that the mission and goals of the agency
are achieved through strategic and performance planning, measurement, analysis, and reviews to improve
the results achieved. At the SBA, the Performance Improvement Officer is the Chief Financial Officer &
Associate Administrator for Performance Management and the Deputy Performance Improvement Officer
is the Director of the Office of Program Performance, Analysis and Evaluation.
Chief Evaluation Officer - a senior executive of the agency who provides strategic direction, policy
oversight and advice on goals, objectives, strategy, metrics, and evidence to guide the Agency's mission,
improve program effectiveness, and ensure operational efficiency. At the SBA, the Chief Evaluation
Officer is the Director of Program Performance, Analysis and Evaluation.
Lead Program Evaluator - a staff member with technical expertise in program evaluation, research
methods (e.g., survey design and qualitative research), and data analysis. This individual possesses the
skillset to manage, conduct, and serve as a Contracting Officer's Representative for external contractors
commissioned to conduct program evaluations. During the pre-award stage of an evaluation, the Lead
Program Evaluator prepare the requirements for evaluations and will convene a team with relevant subject-
matter expertise to review proposals and make recommendations about which proposals to support.
Program Evaluation Team - all stakeholders of a program who are actively engaged in the evaluation
process. This team will typically include staff members working with the program, program partners
internal and external to the organization, a lead program evaluator (Contracting Officer's Representative),
an executive champion, and members of a contracting team conducting the program evaluation.
Performance Analyst - a team member of the Performance Management Division who coordinates
and manages performance management activity. The performance analyst works with program offices on
performance planning, measurement analysis, regular assessment of progress towards goals, and the
use of performance information to ensure that programs are operating efficiently andeffectively.
Project Liaison - a program staff member who represents the program in evaluation meetings
arranged by the Lead Program Evaluator/COR, and continually provides subject matter expertise and
reviews of deliverables. The project liaison reports to their executive champion to inform them of
evaluation-related achievements, challenges, and needs. The project liaison is involved throughout the
evaluation to ensure that project deadlines are met and that evaluation-relevant communications
intended for the contractor are sent to the COR for review and further dissemination.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 73
Evaluation Contractor - the external evaluator hired to independently conduct a program evaluation for
an agency. The contractor will be a professional program evaluator who ensures objectivity of the
evaluation study. The contractor will have expertise in program evaluation, data analytics, and
organizational change management, and will scope the evaluation questions to ensure that the selected
research design will allow for a comprehensive investigation of each question.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
74
Appendix D: Glossary
Activities: The actions taken to implement a program. Examples of SBA program activities include developing
and maintaining a program website, offering trainings, issuing grants, processing loans, approving
certifications, developing policy, and establishing relationships with partners.
Attribution: The assertion that certain events or conditions were, to some extent, caused or influenced by
other events or conditions. In program evaluation, attribution means a causal link can be made between a
specific outcome and the actions and outputs of a program.
Baseline: Initial information on a program or program components collected before receipt of services or
participation activities. Baseline data provide a frame of reference for the change that you want the SBA
program to initiate. These data represent the current state of the economy, community, or sector before your
program begins. Baseline data can also approximate what results might have been in the absence of the
program.
Coding: The process of categorizing information to identify themes, make comparisons, and identify patterns
that require further investigation.
Conclusions: The interpretation of the evaluation findings, given the context and specific operations of a
program.
Confounding Variable: A variable that when combined with a program's activities or inputs may mask the
results.
Contribution: The assertion that a program is statistically correlated with subsequent events or conditions,
even after you have accounted for non-program factors also associated with the same events and conditions.
Control Group: A group whose characteristics are similar to those of the program, but which did not receive
the program services, products, or activities being evaluated. Collecting and comparing the same information
for program participants and non-participants enables evaluators to assess the effect of program activities.
Customers: See "Target Decision-Makers."
Dependent Variable: The variable that represents what you are trying to influence with a program. It answers
the question "what do I observe" (e.g., economic results).
Enterprise Learning Agenda: (see "Learning Agenda") Focuses evaluation activities by prioritizing evaluation
questions that will have the greatest usefulness across the Agency.
Evaluand: The subject of an evaluation; typically, the program undertaking the evaluation.
Evaluation Methodology: The methods, procedures, and techniques used to collect and analyze information
for the evaluation.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 75
Evaluation Questions: The broad questions the evaluation is designed to answer and the bridge between the
description of how a program is intended to operate and the data necessary to support claims about program
success.
Evaluation Users: Most SBA program managers and staff, who often have limited knowledge of program
evaluation but benefit from and see the value of evaluations. From time to time, evaluation users might be
called upon to participate in the evaluation process.
Evidence Registry: A registry of evaluations, research, and other evidence that supports decision-making.
The public-facing SBA Program Evaluation & Evidence Registry (PEER) features research conducted by SBA
programs and others to help answer what works in small business assistance programs and to make those
answers broadly available so that senior management, program managers, policymakers, researchers, and
the public can make evidence-based decisions.
Expert Review: An impartial assessment of the evaluation methodology by experts who are not otherwise
involved with the program or the evaluation; a form of peer review. The Peer Review Handbook outlines
requirements for the peer review of major scientific and technical work products and provides useful tips for
managing expert reviews.
External Influences: Positive or negative factors beyond your control that can affect the ability of your
program to reach its desired outcomes.
Feasibility: The extent to which an evaluation design is practical, including having an adequate budget, data
collection and analysis capacity, staff time, and stakeholder buy-in required to answer evaluation questions.
Findings: The raw data and summary analyses obtained from the respondents in a program evaluation.
Functionality: The extent to which information collected through the evaluation process can be acted upon by
program staff.
Impact Evaluation: An evaluation that focuses on questions of program causality; it allows claims to be made
with some degree of certainty about the link between the program and outcomes; assesses the net effect of a
program by comparing program outcomes with an estimate of what would have happened in the absence of
the program.
Independent Variable: The variable that represents the hypothesized cause (e.g., SBA program activities) of
the observations during the evaluation.
Indicator: A measure, usually quantitative, that provides information on program performance and evidence of
a change in the "state or condition" of the system.
Information Collection Request (ICR): A set of documents that describe reporting, recordkeeping, survey, or
other information collection requests of the public by federal agencies. The ICR provides an overview of the
collection and an estimate of the cost and time for the public to respond.
Intermediate-Tern Outcomes: Changes in behavior that are broader in scope than short-term outcomes;
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 76
often build upon the progress achieved in the short-term.
Learning Agenda: A continuous improvement program tool that creates a structure for a program to consider
its evaluation priorities. The learning agenda template is comprised of a broad set of questions related to the
work that a program conducts and its challenges. This tool assists program managers address questions using
evaluative approaches and evidence to inform decision-making, ultimately increasing program efficiency and
effectiveness.
Logic Model: A diagram with text that describes and illustrates the components of a program and the causal
relationships among program components and the problems they are intended to solve, thus defining the
measurement of success. Essentially, a logic model visually represents what a program does and how it
intends to accomplish its goals.
Long-Term Outcomes: The overarching goals of the program, such as changes in economic conditions.
Mean: A measure of central tendency sometimes referred to as the average; the sum of the values divided by
the number of values.
Median: A measure of central tendency; the number separating the upper and lower halves of a sample. The
median can be found by ordering the numbers from lowest to highest and finding the middle number.
Natural Experiment: Situations that approximate a controlled experiment; that is, they have "natural"
comparison and treatment groups. This scenario provides evaluators with the opportunity to compare program
participants with a group that is not receiving the program offered. Natural experiments are not randomized,
and therefore, strong causal claims of direct impact cannot be made. Evidence is required to show that the
comparison group is a reasonable approximation of an experimental control group.
Non-Experimental Design: A research design in which the evaluator can describe what has occurred but
cannot control or manipulate the provision of the treatment to participants as in a true experimental design or
approximate control using strong quasi-experimental methods.
Outcome Evaluation: An evaluation that assesses a mature program's success in reaching its stated goals. It
focuses on outputs and outcomes (including unintended effects) to assess program effectiveness or process
to understand connection to outcomes. Often, outcome evaluations are appropriate only when at least
baseline and post-baseline data sets are available or could be developed.
Outputs: The immediate products that result from activities, often used to measure short-term progress.
Participatory Evaluation: An evaluation that involves stakeholders in all aspects of the evaluation, including
design, data collection, analysis, and communication of findings.
Program Manager: The person responsible for determining what programs should be evaluated and when
these evaluations should take place. Managers do not necessarily need to have the technical expertise to
conduct an evaluation but should be aware of the basic structure of the evaluation process, so they can make
informed decisions when commissioning evaluations and using evaluation findings to make management
decisions.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 77
Performance Measure: An objective metric used to gauge program performance in achieving objectives and
goals. Performance measures can address the type or level of program activities conducted (process), the
direct products and services delivered (outputs), or the results of those products and services (outcomes).
Primary Data: Data collected "firsthand" by your SBA program specifically for the evaluation.
Process Evaluation: An evaluation that assesses the extent to which a program is operating as it was
intended. Process evaluations typically determine if all essential program elements are in place and operating
efficiently and effectively. Process evaluations can also be used to analyze mature programs under some
circumstances, such as when you are considering changing the mechanics of the program.
Program Design Evaluation: An evaluation most appropriately conducted during program development; it
can be helpful when staff have been charged with developing a new program. Program design evaluations
provide a means for programs to evaluate the strategies and approaches that are most useful for a program to
achieve its goals.
Program Evaluation: A systematic study that uses objective measurement and analysis to answer specific
questions about how well a program is working to achieve its outcomes and why. Evaluation has several
distinguishing characteristics relating to focus, methodology, and function. Evaluation 1) assesses the
effectiveness of an ongoing program in achieving its objectives, 2) relies on the standards of project design to
distinguish a program's effects from those of other forces, and 3) aims to improve programs by modifying
current operations.
Qualitative Data: Describes the attributes or properties of a program's activities, outputs, or outcomes.
Qualitative data can be difficult to measure, count, or express in numerical terms; therefore, data are
sometimes converted into a form that enables summarization through a systematic process (e.g., content
analysis, behavioral coding). This data may be unstructured and contain a high degree of subjectivity, such as
free responses to open-ended questions.
Quantitative Data: Can be expressed in numerical terms, counted, or compared on a scale. Measurement
units (e.g., feet and inches) are associated with quantitative data.
Quartile: The three data points that divide a data set into four equal parts.
Quasi-Experimental Design: A research design with some, but not all, characteristics of an experimental
design. Like randomized control trials (see below), these evaluations assess the differences that result from
participation in program activities and the result that would have occurred without participation. The control
activity (comparison group) is not randomly assigned. Instead, a comparison group is developed or identified
through non-random means, and systematic methods are used to rule out confounding factors other than the
program that could produce or mask differences between the program and non-program groups.
Randomized Control Trial (RCT): An experimental study that is characterized by random assignment to
program treatments (at least one group receives the goods or services offered by a program, and at least one
group-a control group-does not). Both groups are measured post-treatment. The random assignment
enables the evaluator to assert with confidence that no other factors other than the program produced the
outcomes achieved.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
78
Range: The difference between the highest and lowest value in a sample.
Recommendations: Suggestions for the SBA program based on the evaluation's findings and conclusions.
Reliability: The extent to which a measurement instrument yields consistent, stable, and uniform results over
repeated observations or measurements under the same conditions.
Resources: The basic inputs of funds, staffing, and knowledge dedicated to the program.
Secondary Data: Data taken from existing sources and re-analyzed for a different purpose.
Short-Term Outcomes: The changes in awareness, attitudes, understanding, knowledge, or skills resulting
from program outputs.
Spillover Effects: Improvements by non-participants due to transfers of attitudes, beliefs, knowledge, or
technology from program participants.
Stakeholder: Any person or group that has an interest in the program being evaluated or in the results of the
evaluation.
Stakeholder Involvement Plan: A plan to identify relevant stakeholder groups to determine the appropriate
level of involvement for each group and engage each group in the evaluation accordingly.
Targets: Improved level of performance needed to achieve stated goals.
Target Decision-Makers: The groups and individuals targeted by program activities and outputs, also known
as the target audience or program participants.
Validity: The extent to which a data collection technique accurately measures what it is supposed to measure.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
79
Appendix E: Evaluation Resources
SBA Program Evaluation Resources
SBA's SharePoint site on program evaluation and performance management
SBA's Enterprise Learning Agenda:
www.sba.gov/sites/default/files/aboutsbaarticle/FY 2018 Enterprise Learning Agenda OMB SBA Final 2 08 2018-Final 1.pdf.
Other Online Evaluation Resources
Logic Modeling
University of Wisconsin Extension LogicModels
Program Evaluation
W.K. Kellogg Foundation's Evaluation Handbook. Contains resources on developing evaluation
questions, plans, budgeting for evaluation, managing a contractor, and checklists. Includes the
Evaluation Handbook and Logic Model Development Guide.
U.S. Government Accountability Office, Designing Evaluations. Policy and guidance materials on
evaluations, evaluation design, case study evaluation, and prospective evaluation methods.
The Evaluation Center at Western Michigan University. Excellent resource for evaluation checklists,
instructional materials, publications, andreports.
Online Evaluation Resource Library. Contains evaluation instruments, plans, reports, and instructional
materials on project evaluation design and methods of collecting data.
Web Center for Social Research Methods: Site provides resources and links to other locations on the
web that deal in applied program evaluation methods, including an online hypertext textbook on
applied methods, an online statistical advisor, and a collection of manual and computer simulation
exercises of common evaluation designs for evaluators to learn how to do simple simulations.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
80
Appendix F: SBA Program Evaluation Proposal Guidance and
Template
Each year, the SBA updates its Enterprise Learning Agenda (ELA) to support a set of new Agency program
evaluations based on questions identified by senior leadership during the development of the FY current
Strategic Plan. The ELA includes historical evaluations, research, audits, and other data that supported the
development of SBA's strategic goals and objectives along with future questions, that when answered, can
help develop policy and refine processes to better serve small businesses. The program evaluation call for
proposals supports the current year's Enterprise Learning Agenda and SBA's progress toward its strategic
goals and objectives.
During the inaugural year of a centralized evaluation function, the SBA launched a newly developed framework
and managed its first set of coordinated program evaluations. The evaluations have helped SBA program
managers identify and improve actions for program operations and outcomes, support efficiency gains, and
enhance service delivery.
Through this guidance, the SBA will support program evaluations in coordination with your offices. In line with
Administrator McMahon's vision, these proposals should demonstrate how they will help the SBA become a
more effective and efficient organization. The SBA plans to fund between four to five new evaluations each
year that will be managed by one of SBA's lead program evaluators and a team of independent contractors
with expertise in program evaluation.
Approach and Timeline: The SBA will adhere to the following schedule to develop an annual cohort of
program evaluations. The evaluations will be managed by an SBA lead program evaluator and a project
manager within the program office selected. Proposals must be developed in coordination between the
program evaluation team and the program offices and finalized by the proposal submission deadline for each
year's call for proposals.
Upon review of the proposal, a meeting will be held with the program office Executive Champion and the
Performance Improvement Officer/Chief Financial Officer to discuss the proposals merits and needs. Selected
program evaluations will be awarded each year when the review and selection process have concluded.
Contacts: Terell P. Lasane and Brittany Borg will serve as lead program evaluators and will work with the
following program office accounts:
Instructions: Please complete one template per program evaluation proposal in coordination with SBA's
program evaluation team in the Office of Performance Management. The program evaluation team will provide
technical assistance to your office to help scope the proposals and identify key resources to define your
evaluations, including the most current Enterprise Learning Agenda. Please submit this proposal to your lead
program evaluator by the date specified.
1.
Program: Identify the name of the program to be evaluated.
2.
Executive Champion: Identify a senior executive in your office who will champion the program
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
81
evaluation. The Executive Champion will ensure progress on the evaluation in coordination with the
Performance Improvement Officer/Chief Financial Officer. The champion must be available for
monthly discussions on the evaluations progress and help the team make course corrections,
where appropriate.
3.
Project Manager: Identify a project manager in your office who will support the Executive
Champion and SBA lead program evaluator to design the evaluation, monitor its progress, and
implement the recommendations that result from the findings. The project manager must dedicate
between 5-10 hours per pay period to help scope the evaluation, identify data sources, provide
technical assistance, and review deliverables in coordination with the lead program evaluator.
4.
Program Activities: List the activities of your program that will be evaluated (resources, outputs,
customers reached, and outcomes desired). As an example, the SBA evaluated the recruitment of
veterans to the Boots to Business program to assess veteran recruitment strategies and the factors
that helped retain them in the training sessions. If you have a logic model developed for your
program, please include it with your proposal.
5.
Evaluation Questions: Identify the most important evaluation questions that will help improve the
program and its functions. Develop 3-6 questions that relate to the operations of the program or its
outcomes. The questions should be specific and define the opportunity, identify potential actions
that could be taken upon completion of the evaluation to improve the program, and have key steps
that can be completed (e.g., analysis of existing data, collection of data through surveys,
administrative data matching). List the questions in order of priority to the program. The program
evaluation staff will help scope these questions with your office.
6.
Benefits: In two-three paragraphs, explain how this evaluation will support recommendations that
could improve program processes, enhance service delivery, or provide a better understanding of
outcome(s)/impact(s and how the evaluation supports SBA strategic goals. For example, the
Community Advantage pilot program completed its evaluation to gain a better understanding of the
impacts of technical assistance on lending outcomes. The SBA sought evidence to support a
decision to determine program permanency.
7.
Evidence: List the available evidence that will be used to support the evaluation. Evidence comes
from a variety of sources, including historical program evaluations, performance data, audits, and
traditional research. The SBA has analyzed in-house data collected from the program offices,
developed and disseminated surveys for data collection, used information from the Government
Accountability Office, and integrated traditional research from the SBA Office of Advocacy, nonprofit
research, and the Congressional Research Service.
8.
Available Data: If the program office is aware of existing datasets that may be useful for the
proposed evaluation, please note which datasets they are, which office maintains them (include
agencies external to the SBA), and if a public use version exists that does not contain CBI and PII.
The program evaluation team will work with you to determine data security needs, so please list all
potentially useful datasets, even if they are sensitive or have restricted access.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page 82
Bibliography
Agency Strategy to Advance the Use of Evidence in Decision-Making: A Proposal for Collaboration Between
the SBA IEWG and the SBA PMO. 14 06 02 SBA Impact Evaluation Methodology- Scope, Critical Elements
and Logistics.doc, Internal SBA document.
Barriers to Using Administrative Data for Evidence Building. OMB Communications: July 15, 2016.
Budget Fiscal Year 2016 - Chapter 7: Building Evidence with Administrative Data. Budget of the United States
Government. Analytical Perspectives. February 2, 2015.
Budget Fiscal Year 2017 - Chapter 7: Building the Capacity to Produce and Use Evidence. Analytical
Perspectives, Budget of the United States Government, Fiscal Year 2017. February 9, 2016.
Budget of the United States Government Fiscal Year 2017 Chapter 5: Social Indicators. Analytical
Perspectives Issued: February 9, 2016.
Building Smarter Data for Evaluating Business Assistance Programs: A Guide for Practitioners and
Evaluators. Subcommittee of the Evaluating Business Technical Assistance Programs Working Group: May 9,
2016.
Building Smarter Data for Evaluating Business Assistance Programs: A Guide for Practitioners U.S.
Department of Commerce and U.S. Small Business Administration. January 2017.
Burwell, Sylvia. M-14-06 Memorandum for the Heads of Executive Departments and Agencies. Executive
Office of the President Office of Management and Budget. Subject: Guidance for Providing and Using
Administrative Data for Statistical Purposes. February 14, 2014.
Burwell, S., MuÃ±oz, C., Holdren, J., & Krueger, A. M-13-17 Memorandum to the Heads of Departments and
Agencies. Executive Office of the President Office of Management and Budget. Subject: Next Steps in the
Evidence and Innovation Agenda. July 26, 2013.
Commission on Evidence-Based Policymaking. Commission on Evidence-Based Policymaking Final Report.
September 7, 2017.
Economic Report 2014 - Chapter 7: Evaluation as a Tool for Improving Federal Programs. U.S. Government
Printing Office Published: March 2014.
Entrepreneurial Assistance: Opportunities Exist to Improve Collaboration and Performance Management for
Financial Assistance Programs GAO-14-335T. February 6, 2014.
Fact Sheet: Middle Class Economics: The President's Fiscal Year 2016 Budget. OMB Communications
February 2, 2015.
Fragmentation, Overlap, and Duplication: An Evaluation and Management Guide. GAO-15-49SP. April 14,
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
83
2015.
Kalil, T., Graves, D., & James, K. The White House Washington: Memorandum for Deputies. RE: Deputies
Committee on Administration's Evidence Agenda. July 6, 2015.
Korn, R.L. (2017). Program Evaluation in the Federal Government: A Review of Best Practices. Unpublished
Paper.
Orszag, Peter R. M-10-01 Memorandum for the Heads of Executive Departments and Agencies. Executive
Office of the President Office of Management and Budget. Subject: Increased Emphasis on Program
Evaluations. July 29, 2010.
Orszag, Peter R. M-10-32 Memorandum for the Heads of Executive Departments and Agencies. Executive
Office of the President Office of Management and Budget. Subject: Evaluating Programs for Efficacy and
Cost-Efficiency. July 29, 2010.
Overview of Federal Evidence-Building Efforts. OMB Communications July 15, 2016.
Privacy and Confidentiality in the Use of Administrative and Survey Data. OMB Communications July 15,
2016.
Program Evaluation: Annual Agency-Wide Plans Could Enhance Leadership Support for Program Evaluations
GAO-17-743. September 29, 2017.
Small Business Administration Agency Financial Report Fiscal Year 2016.
Small Business Administration: Leadership Attention Needed to Overcome Management Challenges GAO-15-
347. October 28, 2015.
Some Agencies Reported that Networking, Hiring, and Involving Program Staff Help Build Capacity. GAO-15-
25. November 13, 2014.
Using Administrative and Survey Data to Build Evidence. OMB Communications July 15, 2016.
Zients, Jeffrey. M-12-14 Memorandum to the Heads of Executive Departments and Agencies. Executive Office
of the President Office of Management and Budget. May 18, 2012.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
84
Acknowledgments
This Framework for Evaluation and Guidelines for Evaluating a Small Business Administration Program was
produced with the energies and talents of the SBA staff. To these individuals, the Office of Program
Performance, Analysis and Evaluation and the Chief Financial Officer offer sincere thanks and
acknowledgment.
We would also like to specifically acknowledge the SBA's Evidence and Evaluation Community of Practice, the
SBA's Office of General Counsel, the Office of Management and Budget's Evidence Team, Industrial
Economics Inc, and TSC Enterprises for contributions to the production of this framework and guidelines.
Framework and Guidelines for Program Evaluation at the US Small Business Administration - Page
85
