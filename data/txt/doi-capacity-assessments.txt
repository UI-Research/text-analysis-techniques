FY 2022 - 2026 Capacity Assessment
U.S. Department of the Interior
FY 2022 - 2026 Capacity Assessment
OF
NARCH 3, 1849
FY 2022 - 2026 Capacity Assessment
Maturity for Building Evidence
The Foundations for Evidence-Based Policymaking Act of 2018 (Evidence Act) sets expectations that
Federal agencies will improve decision-making by using the best available evidence. The Department of
the Interior (DOI or the Department) continues to develop its evidence building capabilities, including
assessing evidence, building maturity, and identifying current capabilities from which improvements in
the upcoming years can be based. DOI is challenged to consistently assess the use of evidence across a
widely distributed and diverse organization in which entities operate independently, differ in their
missions and vocabularies around evidence, and vary in approaches, resources, capabilities, and
perceptions of the purpose and use of evaluation and evidence.
To help characterize an agency's capacity for using evidence in their operations and decision-making,
maturity models are described and used in assessments. DOI's maturity model for evidence defines
capability levels with a scale consisting of five levels of increasing maturity from "Initiating" to
"Innovating." Table 1 (below) describes characteristics of organizations that are operating at each of the
following defined levels for a given outcome.
Level 1 - Initiating: The activity may or may not achieve its purpose.
Level 2 - Managed: The previously described activity is now implemented in a managed
fashion (planned, monitored, and adjusted) and its work products are appropriately
established, controlled, and maintained.
Level 3 - Established: The previously described Managed activity is now implemented using a
defined process that can achieve its outcomes.
Level 4 - Predictable: The previously described Established activity now operates within
defined limits to achieve its outcomes. Quantitative management needs are identified,
measurement data are collected and analyzed, and corrective action is taken to address root
causes of variation.
Level 5 - Innovating: The previously described Predictable activity is now continually improved
to respond to change aligned with organizational goals and requirements.
Page 2 of 9
FY 2022 - 2026 Capacity Assessment
Table 1. Characteristics of the maturity levels of evidence, evaluation, and learning.
Level 1 -
Level 2 -
Level 3 -
Level 4 -
Level 5 -
Initiating
Managed
Established
Predictable
Innovating
Absence of
Partial
Institutionalizati
Support for
Evaluation
evidence
leadership
on of capacity
building
activities directly
building
support
building
knowledge
support Learning
activities
Engaged
activities
about/capacity
Agenda and
Lack of
stakeholders
Inclusive
to use
Annual
agreement
Limited
stakeholder
evaluation
Evaluation Plan
regarding
strategies for
engagement
Cross cutting
Evidence
learning
conducting and
Range of
activities with
informs daily
priorities
using
evidence
result shared
operations to
No current/
evaluations
building
externally
support
centralized list
Common
activities
Consistency
continuous
of evidence
lexicon
implemented
across
improvement
building
Results shared/
evaluation
Budget requests
activities
disseminated
activities
are built on
Uncertainty
Prioritization of
sound evidence
regarding
studies focused
Forward looking
stakeholders
on
evaluation plans
effectiveness of
that reflect
key programs
evaluation
needs, capacity,
activity, and
results
As an essential component of implementing the Evidence Act and building maturity across the
Department, DOI completed an initial capacity assessment over a two-month period in 2021. DOI's
capacity assessment consisted of:
1. A survey of evidence leads in the DOI bureaus and offices; and
2. An independent assessment by DOI's Evaluation Officer team in the Office of Planning and
Performance Management.
These two approaches were intended to establish a baseline of evidence capability and capacity. From
this, the Office of Planning and Performance Management developed an evidence capacity maturity
model and a plan that could improve DOI's evidence capacity over the next four years in alignment with
the FY 2022-2026 DOI Strategic Plan.
Page 3 of 9
FY 2022 - 2026 Capacity Assessment
Existing Knowledge of Current Evidence Capacity
Methods and Results
The Department assessed evidence and evaluation capacity from two perspectives. First, the Office of
Planning and Performance Management asked the evidence leads within bureaus and offices to
complete an organizational self-assessment. The evidence leads responded to a survey questionnaire to
provide their perspective on their organization's evidence-building capacity. The survey included a Likert
scale rating and as well as an option to provide a qualitative response to each question. This allowed
each responding organization to include additional context on what would be needed to improve their
evidence capacity in a specific area.
Each responding organization's evidence lead rated their organization's capacity by their level of
agreement with the statements provided. The survey used a standard Likert numerical scale from one to
five, where one signifies low to no agreement with a capability statement and five signifies strong
agreement with a capability statement. Decimal responses were accepted. The below scores reflect
cross-DOI ratings from 14 DOI entities on seven capability areas. Not all organizations responded to each
capability statement. In those cases, aggregate ratings reflect fewer number of respondents. The
average, self-reported "evidence capacity" score across the seven areas (with no weighting of one area
over another) was 3.11. This rating is limited as it only indicates the evidence leads' perception of their
own bureau or office capacity and not a holistic independent assessment of DOI's evidence capacity.
Table 2: Bureau evidence and evaluation capacity ratings
Capacity Survey Question*
Avg.
1. Our organization uses evaluations to support better management and improve decision
making.
3.21
2. Our organization has the right resources (people, processes, technologies) in place to benefit
from the use of evidence building activities.
3.07
3. Our organization uses a variety of evaluation types.
3.26
4. Our organization values evidence and is making progress towards becoming a data-driven
organization.
3.31
5. Our organization is able to assist staff and program offices in using evaluations and data in day-
to-day operations.
2.96
6. Our organization has an effective communication and reporting capability to review and
disseminate findings.
3.50
7. Our organization measures outcomes, not just outputs.
2.83
Overall DOI Capability
3.11
*DOI developed the capacity survey statements in consideration of the criteria factors for capacity specified in
the Evidence Act.
The second method of capacity assessment was the Evaluation Officer's judgment of evidence capacity.
The Evaluation Officer tasked a third-party contractor to conduct deep dive research and analysis of the
organization, asking open ended questions to learn more about processes relating to programming
decisions, annual budget formulation, and communication from leadership. The analysts worked with
the bureaus to review materials, including evaluations, and sought examples of how information was
used in planning, decision-making, and the culture of evidence building in each organization. The
Evaluation Officer's analyst assessment considered the information gleaned in these interviews, used
the same scoring criteria, applied a consistent definition and standard for activities (i.e., evaluation), and
Page 4 of 9
FY 2022 - 2026 Capacity Assessment
provided an independent score of DOI's coverage, quality, methods, effectiveness, and independence in
the use of statistics, evaluation, research, and analysis as required by the Evidence Act. The Evaluation
Officer and analyst reviewed the findings and determined a rating assessment from the observations,
notes, and materials.
Table 3 summarizes the Evaluation Officer's assessment. A white box indicates the observation that
either the area was low or nascent at the DOI or that the Evaluation Officer and analyst were unable to
make an accurate determination. The light blue to dark blue boxes indicate the observation that there is
a higher level of maturity or adoption on that characteristic. Many of the DOI organizations have robust
capacity for research and analysis but are weaker in the use of statistics and evaluation.
Table 3. The Evaluation Officer's assessment of DOI's baseline capacities.
Coverage
Quality
Methods
Effectiveness
Independence
Use of Statistics
Use of Evaluation
Use of Research
Use of Analysis
Increasing maturity and adoption
Low/nascent
Routine/normal
Practice/expectations
Synthesis
The results of the assessment provide key insights into the evidence capacities - and gaps - for the
Department of the Interior.
First, the self-assessment responses indicate that DOI organizations self-assess their capacity to use
evidence and conduct evidence-building as variable, but moderate overall. The strongest average
response was on the Department's value of evidence and desire to become better at using it. This is an
excellent result that is foundational for progress. The lowest average score was for the focus on outputs
rather than outcomes. Recognition of this challenge is a critical first step in developing a strategy to
rectify it.
Second, the assessment by the Evaluation Officer team indicates that the maturity level is highly variable
across categories. In conducting the deep dive interviews, it became clear that indeed, organizations
were using information to make decisions and doing so across the breadth of the Department's mission
and operational activities. However, there was a challenge in demonstrating, at the highest program
levels, how evidence is used to affect strategic and programming decisions, resource allocations, or
program or project improvements. The concept of evaluation, particularly "significant" or program-level
evaluation, and hallmarks of quality evaluation, fell short of the Evidence Act standards. Some
organizations viewed evaluation being annual management program review for cost and schedule needs
for the coming year. Similarly, there was little evidence of using performance information to make
decisions about program or project implementation, realignment, expansion, or potential rescoping or
termination. Other organizations appeared to align the idea of evaluation with scientific study and
publishing results (e.g., species disease in a specific area) as evaluation. Still others inspected facilities,
updated policies, released surveys, held focus groups, and conducted other like operational activities. All
Page 5 of 9
FY 2022 - 2026 Capacity Assessment
this work is important, and all are forms of evidence needed to make day to day decisions, demonstrate
performance, or enable compliance with Federal requirements. However, the Office of Planning and
Performance Management's Analyst found little evidence of evaluation and statistics, as a systematic
and holistic review of evidence to assess effectiveness, equity, alignment to goals, coverage, and impact.
Few used independent third parties, though there were some instances of program reviews, issues
analysis, or process evaluations (often mandated) by external Federal organizations, the National
Academies, or cross-cutting issues identified and assessed (with accompanying recommendations) by
the Office of the Inspector General or Government Accountability Office.
Across the board, the Evaluation Officer determined that the use of evidence in decision making
appeared strong at the day-to-day activity level, but DOI is less mature in overall capacity for use of
information at higher project or program level and was especially weak in evaluation. This is not
unexpected, as the emphasis on evaluation of programs and their associated products, services, delivery
mechanisms, and such, is a relatively new requirement. In addition, evaluation has traditionally been
associated with a high cost, long lead times to results, and not directly aligned to service delivery-
attributes that conflict with the need to increase mission services despite flat budgets and decreased
purchasing power. These findings are now driving DOI efforts at building awareness of the benefits of
evaluation as a tool for enabling effective and equitable DOI mission delivery.
Combining the two streams of information, with a few caveats described below, we conclude that DOI's
primary evidence capacity maturity gaps include:
1. Confusion over vocabulary around evidence, especially the term and use of program evaluation,
confusion of the distinction between program audits and evaluations, and in different types of
evidence.
2. Lack of skill in identifying mission or operational outcomes and evidence needs, building
evidence, and how to use evidence to make decisions and drive change to meet those
outcomes.
3. Lack of a broad, Department-wide capability for conducting or enabling evaluations.
Independent, external evaluation is conducted only within some programs, some by engaged
managers, or when mandated.
4. Lower awareness and capacity in methodologies, assessing the quality and utility of data,
analyzing, and applying information, and in presenting or interpreting information. This gap
exists even though knowledge of advanced analytics, statistics, fundamentals of research, and
evaluation are evident in organizations, especially in scientific or analytical divisions within
organizations. When conducted, the quality of evaluation and analysis appears to be high, but
coverage across organizations is low.
5.
Lack of strategic use of program evaluation to improve DOI programs.
The available evidence allows us to draw the preceding conclusions but are bounded by a few caveats.
First, the sample size of self-evaluation was, due to capacity constraints, relatively small and focused on
a select - though well-informed - sample of DOI. A broader survey of not just bureau evidence leads,
but other users of evidence, across grades, occupational series, geographies, and other variables across
DOI would provide additional insights that could alter our current interpretation. Second, to balance the
depth of information gathered with capacity (time and resource) constraints, a relatively small set of
questions were used in this initial assessment. Future assessments can broaden the set of questions and
use a diversity of approaches (e.g., surveys, structured focus groups, and others) to get into details that
will build the broad suite of evidence that DOI needs. Regardless of these caveats, the data provided
critical information to begin charting the course to mature evidence capacity at DOI.
Page 6 of 9
FY 2022 - 2026 Capacity Assessment
Maturing Evidence Capacity
Given the results and conclusions of this initial assessment, DOI can begin to chart a path forward for
maturing our evidence capacity across the Department. This initial path includes three core steps:
refined data, capacity building, and relationship building.
Refined Data
The baseline capacity self-assessments described above provided insights into overall strengths and
weaknesses in conducting and benefitting from evidence building activities across DOI. However, as
noted above, DOI could benefit from a more formal and strategically designed survey to benchmark
DOI's progress against a standard Federal evidence and evaluation maturity model. A more robust and
methodologically sound survey by informed respondents across the Department could be used to
identify strengths, weaknesses, opportunities, and threats associated with the use of evidence and
evaluation. This assessment would establish a baseline for measuring future improvements and assist
DOI in understanding the state of current evidence building practices and in discovering, describing, and
assessing the DOI evidence-building infrastructure.
Through a future, Department-wide survey, DOI could assess statistics, evaluation, research, and
analysis work to better:
1.
determine if evidence-building processes are supporting the decision-making needs of DOI and
how they contribute to achievement of DOI strategic goals;
2. determine to what degree DOI policy and strategy guidelines are being met;
3. determine to what degree evidence-building activities conform to recognized standards, when
applicable;
4. benchmark evidence-building process improvements against a DOI maturity model; and
5. provide a roadmap for future improvements in evaluation capacity to support DOI's Learning
Agenda and Annual Evaluation Plan.
Capacity Building
To strengthen evidence building and evaluation capacity with a heavy emphasis on enabling program
evaluation of the Department's programs and projects, DOI will require a dedicated evaluation
specialist. The FY23 Budget includes funding for both an evaluation specialist and evaluation fund, to be
overseen by the independent (to the implementing bureaus and offices) evaluator. The evaluator who
will assist organizations with establishing an evaluation methodology, data and evidence needs for
evaluation, and acquiring and overseeing qualified contracted evaluation services. By establishing a
distinct fund, leadership can work with program offices to propose candidate activities for evaluation
without diverting fiscal resources from ongoing operations, thus removing potential cost and skills
barriers.
The Evaluation Officer continues to collaborate with the Chief Data Officer and Statistical Official to seek
other means of strengthening a culture of evidence-based decision making. In addition, in coordination
with the Statistical Official and Chief Data Officer, the new evaluator will work to ensure that evaluation
data are transparent, open, and meets a high bar for statistical integrity and that evaluations are
conducted in compliance with DOI's Evaluation Policy.
Page 7 of 9
FY 2022 - 2026 Capacity Assessment
An evaluator will help assess evidence across DOI mission areas, including cross-cutting, cross-
organizational outcomes and impacts such as climate change mitigation and response, economic
development, diversity, equity, inclusion, and accessibility efforts, environmental justice, and balancing
current land and water resource needs with stewardship and conservation. The evaluation specialist
will:
Identify evaluation requirements and opportunities for key programs and initiatives in the FY
2022-2026 DOI Learning Agenda;
Assess and mature evaluation capability across the Department;
Develop, coordinate, and conduct Department-wide evidence and evaluation trainings;
Assist bureaus and offices in establishing their own evaluation policies, programs, and budget
requirements for evaluation;
Assist in developing requirements and solicitations and contracts for external evaluation;
Help DOI organizations apply evaluation recommendations and findings to program
management, project management, and risk management; and
Work with the Statistical Official and Chief Data Officer to ensure data are available and can be
organized to support evaluation and other statistical analyses.
Relationship Building
DOI's Evaluation Officer will continue close collaboration and relationship-building with staff in bureaus
and offices to increase working knowledge of evidence and the value placed on evidence in decision
making, and to support a culture where evidence-based decision making is a routine and standard
practice. The Evaluation Officer will also continue efforts to broaden staff and leadership understanding
of terminology in the evaluation field, knowledge building about forms of evidence, how each form of
evidence can be applied across a project or program life cycle, and how evidence use benefits program
managers. The Evaluation Officer will assist in developing bureau, office, and special action teams to
develop coordinated evidence-building plans. The Office of Planning and Performance Management will
further build on the established relationships with leaders of cross-cutting initiatives and Administration
priorities and initiate conversations with organizations that have not been traditionally involved in
performance and evidence-building conversations. DOI recognizes that leadership support for
evaluation work is a crucial component to successfully building an evidence culture, and the Evaluation
Officer will continue to engage in conversations with decision makers across the organization to
advocate for evaluation work as tool that will help DOI perform.
Page 8 of 9
FY 2022 - 2026 Capacity Assessment
DOI Current Evaluations and Analysis
The FY 2022-2026 DOI Learning Agenda identifies current evidence building activities. Evaluation work
will follow the standards outlined in DOI's Evaluation Policy and OMB guidance M-20-12 for relevance
and utility, rigor, independence and objectivity, transparency, and ethics, and the Evaluation Officer and
Departmental offices may play a role in leading or supporting evaluations, particularly when they
require cross-bureau coordination. The following is a list of Administration priority areas with planned
evidence building and planned evaluations for FY 2022-2026.
Diversity, Equity, Inclusion, and Accessibility: DOI is conducting evidence building activities on visitors to
public lands and barriers to accessing public lands; grants programs across DOI and specifically grants to
tribes, tribes accessing grants, tools/mechanisms to distribute, educate, and share awareness of grants;
and contracts to businesses and individuals from underserved communities. These address the
Executive Order 13985 on DEIA, and results will inform DOI programs, policies, and regulations to better
achieve DEIA outcomes. DOI has some demographic and population data that could be used in the
evidence building, but with the demographic limitations for tribes, visitors, grantees, and contractors
DOI will need to conduct extensive stakeholder outreach, surveys, and other data gathering.
Invasive Species, Wildland Fire, and International Species Conservation: DOI will evaluate the
effectiveness of measuring outcomes of invasive species control, wildland fire recruitment, training, and
retention strategy effectiveness, and the impact of DOI financial assistance on international species
conservation. Congress and the public have shown great interest in achieving better outcomes in these
areas and these evaluations will build on other evidence-building activities to address these issues. The
evaluations will provide insight into where outcomes are best met and how to better allocate and direct
resources.
Climate and Sustainability: The DOI Climate Action Plan and Sustainability Plan lay out a path to support
people, communities, and cultural resources, build healthy watersheds and water supplies, conserve
biodiversity and ecosystems, ensure the health of coastal and marine resources, and maintain the DOI
infrastructure and facilities while reducing DOI's negative impact on the climate and planet. DOI will
evaluate the effectiveness in select areas to look at the outcomes being achieved, leading practices, and
where there are limitations. While many of these areas are currently being stood up and planned,
evaluations will look at pilot programs and other new efforts to determine effectiveness. Data will come
from these efforts and future data and evaluations will be planned.
Bipartisan Infrastructure Law: The Department is also assessing the need for establishing evaluation
requirements and resources in assessing outcomes, effectiveness, alignment, outcomes, and impacts of
the Bipartisan Infrastructure Legislation (BIL). The Evaluation Officer and team continues to coordinate
with the BIL program manager and leadership team on the topic.
Page 9 of 9
